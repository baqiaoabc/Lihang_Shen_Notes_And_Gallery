{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unstructured data\n",
    "\n",
    "## EXERCISE: SMS spam filtering with naive Bayes\n",
    "\n",
    "\n",
    "\n",
    "References:\n",
    "* http://sebastianraschka.com/Articles/2014_naive_bayes_1.html\n",
    "* http://zacstewart.com/2015/04/28/document-classification-with-scikit-learn.html\n",
    "* https://gist.github.com/zacstewart/5978000\n",
    "\n",
    "Other options:\n",
    "* https://scikit-learn.org/0.19/datasets/twenty_newsgroups.html\n",
    "* https://scikit-learn.org/0.18/datasets/rcv1.html\n",
    "\n",
    "### Download data from UCI ML data repo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import urllib.request\n",
    "import zipfile\n",
    "\n",
    "DATA_URI = 'https://archive.ics.uci.edu/ml/machine-learning-databases/00228/smsspamcollection.zip'\n",
    "DATA_DIR = 'data'\n",
    "ARCHIVE_NAME = 'smsspamcollection.zip'\n",
    "FILE_NAME = 'SMSSpamCollection'\n",
    "\n",
    "# set up paths (in portable, OS-agnostic way)\n",
    "local_archive_path = os.path.join(DATA_DIR, ARCHIVE_NAME)\n",
    "local_file_path = os.path.join(DATA_DIR, FILE_NAME)\n",
    "\n",
    "## set up local data directory\n",
    "os.makedirs(DATA_DIR, exist_ok=True)\n",
    "\n",
    "## save file from DATA_URI to local_path\n",
    "urllib.request.urlretrieve(DATA_URI, local_archive_path)\n",
    "\n",
    "## extract content from archive\n",
    "z = zipfile.ZipFile(local_archive_path, 'r')\n",
    "z.extractall(DATA_DIR)\n",
    "z.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read and profile data using Pandas\n",
    "\n",
    "Pandas provides tools that streamline some of the data analysis and visualisation work we've done in previous exercises. \n",
    "\n",
    "[DataFrame](http://pandas.pydata.org/pandas-docs/stable/dsintro.html#dataframe) is the most commonly used data structure in Pandas. It is a 2-dimensional labeled data structure with columns of potentially different types. You can think of it like a spreadsheet or SQL table.\n",
    "\n",
    "Let's use it now to read and profile our spam data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     label                                            message\n",
      "0      ham  Go until jurong point, crazy.. Available only ...\n",
      "1      ham                      Ok lar... Joking wif u oni...\n",
      "2     spam  Free entry in 2 a wkly comp to win FA Cup fina...\n",
      "3      ham  U dun say so early hor... U c already then say...\n",
      "4      ham  Nah I don't think he goes to usf, he lives aro...\n",
      "...    ...                                                ...\n",
      "5569  spam  This is the 2nd time we have tried 2 contact u...\n",
      "5570   ham               Will Ã¼ b going to esplanade fr home?\n",
      "5571   ham  Pity, * was in mood for that. So...any other s...\n",
      "5572   ham  The guy did some bitching but I acted like i'd...\n",
      "5573   ham                         Rofl. Its true to its name\n",
      "\n",
      "[5574 rows x 2 columns]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr:last-of-type th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th colspan=\"4\" halign=\"left\">message</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "      <th>unique</th>\n",
       "      <th>top</th>\n",
       "      <th>freq</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>label</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>ham</th>\n",
       "      <td>4827</td>\n",
       "      <td>4518</td>\n",
       "      <td>Sorry, I'll call later</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>spam</th>\n",
       "      <td>747</td>\n",
       "      <td>653</td>\n",
       "      <td>Please call our customer service representativ...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      message                                                               \n",
       "        count unique                                                top freq\n",
       "label                                                                       \n",
       "ham      4827   4518                             Sorry, I'll call later   30\n",
       "spam      747    653  Please call our customer service representativ...    4"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import csv\n",
    "import pandas\n",
    "\n",
    "messages = pandas.read_csv(local_file_path, sep='\\t', quoting=csv.QUOTE_NONE,\n",
    "                           names=[\"label\", \"message\"])\n",
    "print(messages)\n",
    "\n",
    "# view aggregate statistics\n",
    "messages.groupby('label').describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text feature extraction\n",
    "\n",
    "- It's easy for humans to see the patterns that distinguish ham and spam messages. What kind of features could we feed to a machine learning algorithm?\n",
    "\n",
    "Feeding raw text data (e.g., \"Sorry, I'll call later\") to a machine learning algorithm would not be very useful. As a first step let's assume that there is a systematic difference between the words used for spam and ham.\n",
    "\n",
    "`scikit-learn` includes [several functions for creating feature vectors from text](http://scikit-learn.org/stable/modules/feature_extraction.html#text-feature-extraction). These first tokenise -- split strings into words (e.g., \"Sorry\", \"I'll\", \"call\", \"later\").\n",
    "\n",
    "Then they create feature vectors where indices correspond to a specific word and values correspond to the frequency of the corresponding word. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3734, 8745)\n",
      "Feature names:\n",
      " ['00' '000' '000pes' '008704050406' '0089' '0121' '01223585236'\n",
      " '01223585334' '0125698789' '02']\n",
      "\n",
      "Feature words for row 1:\n",
      " ['Do', 'you', 'hide', 'anythiing', 'or', 'keeping', 'distance', 'from', 'me']\n",
      "\n",
      "Original string for row 1:\n",
      " Do you hide anythiing or keeping distance from me\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "\n",
    "# First split in to train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(messages.message, messages.label, test_size=0.33,\n",
    "                                                    random_state=5) # so we get the same results\n",
    "\n",
    "# Fit and transform training to feature vectors of words\n",
    "v = CountVectorizer(binary=True, lowercase=False)\n",
    "X_train_bin = v.fit_transform(X_train)\n",
    "print(X_train_bin.shape)\n",
    "\n",
    "# scikit-learn vectorisers produce sparse array representations.\n",
    "# These store only observed features for each instance instead of a complete vector.\n",
    "# They're great for working with text data, but require a bit of work to inspect.\n",
    "\n",
    "# View feature names\n",
    "print('Feature names:\\n', v.get_feature_names_out()[:10])\n",
    "\n",
    "# View complete feature vector for a row as a list of words\n",
    "def iter_features(X, row, names):\n",
    "    for i in X[row].indices:\n",
    "        yield names[i]\n",
    "print('\\nFeature words for row 1:\\n', list(iter_features(X_train_bin, 1, v.get_feature_names_out())))\n",
    "\n",
    "# Compare to original data\n",
    "print('\\nOriginal string for row 1:\\n', X_train.iloc[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build pipelines and choose parameters\n",
    "\n",
    "`scikit-learn` includes pipeline functionality that makes it possible to specify and optimise a sequence of actions.\n",
    "\n",
    "Let's see whether the results in [Wang and Manning (2012)](http://nlp.stanford.edu/pubs/sidaw12_simple_sentiment.pdf) hold for the SMS spam data we're using here.\n",
    "\n",
    "Specifically we will compare multinomial naive Bayes to support vector machine with a degree-2 polynomial kernel. We will choose the best feature representation for both:\n",
    "* binary vs term frequency vs tfidf\n",
    "* unigrams vs bigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "MNB best params:\n",
      " {'tfidf__use_idf': True, 'vect__binary': True, 'vect__ngram_range': (1, 1)}\n",
      "\n",
      "SVM best params:\n",
      " {'tfidf__use_idf': False, 'vect__binary': False, 'vect__ngram_range': (1, 1)}\n",
      "\n",
      "MNB test result:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "         ham       0.95      1.00      0.98      1586\n",
      "        spam       1.00      0.69      0.82       254\n",
      "\n",
      "    accuracy                           0.96      1840\n",
      "   macro avg       0.98      0.85      0.90      1840\n",
      "weighted avg       0.96      0.96      0.95      1840\n",
      "\n",
      "\n",
      "SVM test result:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "         ham       0.96      1.00      0.98      1586\n",
      "        spam       0.98      0.75      0.85       254\n",
      "\n",
      "    accuracy                           0.96      1840\n",
      "   macro avg       0.97      0.88      0.92      1840\n",
      "weighted avg       0.96      0.96      0.96      1840\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "# Pipeline for multinomial naive Bayes\n",
    "mnb = Pipeline([('vect', CountVectorizer(lowercase=False)),\n",
    "                ('tfidf', TfidfTransformer()),\n",
    "                ('clf', MultinomialNB())\n",
    "               ])\n",
    "\n",
    "# Pipeline for polynomial support vector machine\n",
    "svm = Pipeline([('vect', CountVectorizer(lowercase=False)),\n",
    "                ('tfidf', TfidfTransformer()),\n",
    "                ('clf', LinearSVC(C=0.1, penalty='l2', dual='auto'))\n",
    "               ])\n",
    "\n",
    "# Grid search parameters\n",
    "param_grid = [{'vect__binary': [True, False],\n",
    "               'vect__ngram_range': [(1, 1), (1, 2)],\n",
    "               'tfidf__use_idf': [True, False]\n",
    "              }]\n",
    "\n",
    "# Find best parameters for MNB and SVM\n",
    "gs_mnb = GridSearchCV(mnb, param_grid)\n",
    "gs_mnb.fit(X_train, y_train)\n",
    "print('\\nMNB best params:\\n', gs_mnb.best_params_)\n",
    "\n",
    "gs_svm = GridSearchCV(svm, param_grid)\n",
    "gs_svm.fit(X_train, y_train)\n",
    "print('\\nSVM best params:\\n', gs_svm.best_params_)\n",
    "\n",
    "# Print accuracy\n",
    "print('\\nMNB test result:\\n', classification_report(y_test, gs_mnb.predict(X_test)))\n",
    "print('\\nSVM test result:\\n', classification_report(y_test, gs_svm.predict(X_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TODO Handling class imbalance with SVM\n",
    "\n",
    "- Which is better? Do we care about overall performance or just one of our classes? How does this compare to Wang and Manning's result?\n",
    "- The `fit()` method for `LinearSVC` includes the `class_weight` parameter with can help deal with imbalanced data. The \"balanced\" mode uses the values of y to automatically adjust weights inversely proportional to class frequencies in the input data as `n_samples / (n_classes * np.bincount(y))`. Train a new SVM model with the best grid search parameters and `class_weight='balanced'`. Is this result better?\n",
    "- Is there a more appropriate scoring function we could pass to `GridSearchCV` (using `sklearn.metrics.make_scorer`)? Does this give different MNB or SVM results for the parameter grid above?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "SVM2 test result:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "         ham       0.99      0.98      0.99      1586\n",
      "        spam       0.90      0.94      0.92       254\n",
      "\n",
      "    accuracy                           0.98      1840\n",
      "   macro avg       0.95      0.96      0.95      1840\n",
      "weighted avg       0.98      0.98      0.98      1840\n",
      "\n",
      "\n",
      "MNB GS2 best params:\n",
      " {'tfidf__use_idf': True, 'vect__binary': True, 'vect__ngram_range': (1, 1)}\n",
      "\n",
      "SVM GS2 best params:\n",
      " {'tfidf__use_idf': False, 'vect__binary': False, 'vect__ngram_range': (1, 1)}\n",
      "\n",
      "MNB GS2 test result:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "         ham       0.95      1.00      0.98      1586\n",
      "        spam       1.00      0.69      0.82       254\n",
      "\n",
      "    accuracy                           0.96      1840\n",
      "   macro avg       0.98      0.85      0.90      1840\n",
      "weighted avg       0.96      0.96      0.95      1840\n",
      "\n",
      "\n",
      "SVM GS2 test result:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "         ham       0.96      1.00      0.98      1586\n",
      "        spam       0.98      0.75      0.85       254\n",
      "\n",
      "    accuracy                           0.96      1840\n",
      "   macro avg       0.97      0.88      0.92      1840\n",
      "weighted avg       0.96      0.96      0.96      1840\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# TODO: replace the content of this cell with your Python solution\n",
    "# raise NotImplementedError\n",
    "\n",
    "# 1 - SVM gives a better result for spam, which is the category of interest.\n",
    "#     Since it's the minority category in a binary problem,\n",
    "#     we should be looking at precision and recall for spam only.\n",
    "\n",
    "# 2 - Spam recall and f-score are better while precision is worse.\n",
    "#     Which to deploy depends on the appropriate precision/recall tradeoff for users.\n",
    "svm2 = Pipeline([('vect', CountVectorizer(lowercase=False, binary=False, ngram_range=(1,1))),\n",
    "                 ('tfidf', TfidfTransformer(use_idf=False)),\n",
    "                 ('clf', LinearSVC(C=0.1, penalty='l2', class_weight='balanced', dual='auto'))\n",
    "                ])\n",
    "svm2.fit(X_train, y_train)\n",
    "print('\\nSVM2 test result:\\n', classification_report(y_test, svm2.predict(X_test)))\n",
    "\n",
    "# 3 - We can use `f1_score` with `pos_label='spam'` and `average='binary'`.\n",
    "#     This gives the same results for this parameter grid, but could be used to choose the best `class_weight`.\n",
    "#     Note with f1_score, we could also specify whether we prefer precision or recall.\n",
    "from sklearn.metrics import make_scorer, f1_score\n",
    "f1_scorer = make_scorer(f1_score, pos_label='spam', average='binary')\n",
    "gs_mnb2 = GridSearchCV(mnb, param_grid, scoring=f1_scorer)\n",
    "gs_mnb2.fit(X_train, y_train)\n",
    "print('\\nMNB GS2 best params:\\n', gs_mnb2.best_params_)\n",
    "\n",
    "gs_svm2 = GridSearchCV(svm, param_grid, scoring=f1_scorer)\n",
    "gs_svm2.fit(X_train, y_train)\n",
    "print('\\nSVM GS2 best params:\\n', gs_svm2.best_params_)\n",
    "\n",
    "print('\\nMNB GS2 test result:\\n', classification_report(y_test, gs_mnb2.predict(X_test)))\n",
    "print('\\nSVM GS2 test result:\\n', classification_report(y_test, gs_svm2.predict(X_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### OPTIONAL Generalisation, data size, features\n",
    "\n",
    "- Plot training vs generalisation error by number of features (using the `max_features` option to CountVectorizer). Should we use fewer features?\n",
    "- Plot training vs generalisation error by amount of training data. Would it be worth obtaining more training data?\n",
    "- What other features could we use?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# TODO: replace the content of this cell with your Python solution\n",
    "# raise NotImplementedError\n",
    "\n",
    "# 1 - Refer to week 10 exercise.\n",
    "\n",
    "# 2 - Refer to week 10 exercise.\n",
    "\n",
    "# 3 - We haven't done anything to normalise our word features.\n",
    "#     We could convert everything to lowercase and stem or lemmatise.\n",
    "#     Character n-gram features might pick up common spam strings (e.g., \"XXX\")\n",
    "#     Capitalisation patterns might help distiguish, e.g., spam \n",
    "#     might use upper case more.\n",
    "#     Message length would help distinguish for this data set (see plot), but\n",
    "#     may not generalise.\n",
    "#     It's worth noting as well that this sample may not be particularly\n",
    "#     representative of the population of SMS messages since it\n",
    "#     was collected from several specific subpopulations\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "%matplotlib inline\n",
    "messages['length'] = messages['message'].map(lambda text: len(text))\n",
    "_ = messages[messages.length<=250].hist(column='length', by='label', bins=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EXERCISE: Forecasting movie gross with support vector regression\n",
    "\n",
    "Imagine you have an opportunity to invest in films after seeing descriptions. Perhaps you run a cinema and need to decide which films to show. If we could predict box office gross based on descriptions, then we'd have a good basis for investment decisions.\n",
    "\n",
    "In this exercise we'll predict movie gross with support vector regression. \n",
    "\n",
    "### Download reviews and movie gross data\n",
    "\n",
    "DBPedia converts Wikipedia pages into structured semantic web data. Let's use it to grab the data we need. DBpedia has a public SPARQL endpoint at http://dbpedia.org/sparql. Enter the following query and save as `CSV` under `Results Format` and click `Run Query`.\n",
    "\n",
    "#### Query\n",
    "```\n",
    "PREFIX category: <http://dbpedia.org/resource/Category:>\n",
    "PREFIX dbtype: <http://dbpedia.org/datatype/>\n",
    "PREFIX dcterms: <http://purl.org/dc/terms/>\n",
    "\n",
    "SELECT ?dburl ?title ?budget ?gross ?abstract\n",
    "WHERE { \n",
    " ?dburl dcterms:subject category:2013_films .\n",
    "\n",
    " ?dburl foaf:name ?title .\n",
    " FILTER(LANG(?title) = \"en\") .\n",
    "\n",
    " ?dburl dbo:budget ?budget .\n",
    " FILTER(xsd:float(?budget) > xsd:float(\"1.0E7\")) .\n",
    " FILTER(datatype(?budget) = dbtype:usDollar) .\n",
    "\n",
    " ?dburl dbo:gross ?gross .\n",
    " FILTER(datatype(?gross) = dbtype:usDollar) .\n",
    "\n",
    " ?dburl dbo:abstract ?abstract .\n",
    " FILTER(LANG(?abstract) = \"en\") .\n",
    " FILTER(fn:string-length(?abstract) >= xsd:int(140)) .\n",
    "} \n",
    "```\n",
    "\n",
    "This will return the box office gross and the Wikipedia abstract for English films from 2013 that had a USD budget of more than 10 million. This will be our training data. Upload to your `data` directory on Jupyter Hub and rename to `movies.2013.csv`.\n",
    "\n",
    "Run the same query with `category:2014_films` to get test data. Upload to your `data` directory on Jupyter Hub and rename to `movies.2014.csv`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "162 162\n",
      "144 144\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import os\n",
    "\n",
    "TRAIN_NAME = 'movies.2013.csv'\n",
    "TEST_NAME = 'movies.2014.csv'\n",
    "\n",
    "def read_movies(path):\n",
    "    data = []\n",
    "    target = []\n",
    "    for d in csv.DictReader(open(path,encoding=\"utf8\")):\n",
    "        data.append(d['abstract'])\n",
    "        target.append(float(d['gross']))\n",
    "    return data, target\n",
    "\n",
    "train_path = os.path.join('data', TRAIN_NAME)\n",
    "X_train, y_train = read_movies(train_path)\n",
    "print(len(X_train), len(y_train))\n",
    "\n",
    "test_path = os.path.join('data', TEST_NAME)\n",
    "X_test, y_test = read_movies(test_path)\n",
    "print(len(X_test), len(y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Select parameters for support vector regression\n",
    "\n",
    "Let's see whether the results in [Kogan et al. (2009)](http://www.cs.cmu.edu/~nasmith/papers/kogan+levin+routledge+sagi+smith.naacl09.pdf) hold for the movie gross data we're using here.\n",
    "\n",
    "Specifically we will choose the best feature representation for both:\n",
    "* binary vs term frequency vs tfidf\n",
    "* unigrams vs bigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.svm import LinearSVR\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "\n",
    "svr = Pipeline([('vect', CountVectorizer(lowercase=True)),\n",
    "                ('tfidf', TfidfTransformer()),\n",
    "                ('clf', LinearSVR(epsilon=0.1))\n",
    "               ])\n",
    "\n",
    "# Grid search parameters\n",
    "param_grid = {'vect__ngram_range': [(1, 1), (1, 2)],\n",
    "              'vect__binary': [True, False],\n",
    "              'tfidf__use_idf': [True, False],\n",
    "              'tfidf__sublinear_tf': [True, False],\n",
    "              'clf__C': [1e8, 1e9],\n",
    "             }\n",
    "\n",
    "# Find best parameters\n",
    "gs_svr = GridSearchCV(svr, param_grid)\n",
    "gs_svr.fit(X_train, y_train)\n",
    "print('Grid search mean and stdev:\\n')\n",
    "\n",
    "scoring = gs_svr.cv_results_\n",
    "for mean_score, std, params in zip(scoring['mean_test_score'],scoring['std_test_score'],scoring['params']):\n",
    "    print(\"{:0.3f} (+/-{:0.03f}) for {}\".format(\n",
    "            mean_score, std * 2, params))\n",
    "\n",
    "print('\\nSVR best params:\\n', gs_svr.best_params_)\n",
    "print('\\nSVR r-squared on training data:\\n', gs_svr.score(X_train, y_train))\n",
    "print('\\nSVR r-squared on test data:\\n', gs_svr.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TODO Evaluation and discussion\n",
    "\n",
    "- Draw a residual plot with training data in blue and test data in green (like we did in week 9). Is a linear model appropriate for our data? Is prediction performance comparable to training performance?\n",
    "- How could we improve this experimental setup? We can use `gs_svr.steps` to access feature names (`vect.get_feature_names_out()`) and weights (`clf.coef_`) from the pipeline components respectively. Use Python `zip` function to combine and sort these. What do the highest-weighted features tell us about our experimental setup?\n",
    "- Would you use this model to pick investments?\n",
    "- Maybe we're not predicting the right thing. What derived values could we predict? If you have time to try them, are they better?\n",
    "- Is there other text data that might be a better predictor of box office returns?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: replace the content of this cell with your Python solution\n",
    "# raise NotImplementedError\n",
    "\n",
    "# 1 - Training residuals are nearly zero for most movies with a few exceptions.\n",
    "#     This may indicate overfitting. The residual plot for test data is OK.\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "print('Residual plot for training data (blue) and test data (green) below')\n",
    "plt.scatter(gs_svr.predict(X_train), y_train-gs_svr.predict(X_train), c='blue', s=40, alpha=0.5, edgecolor='white')\n",
    "plt.scatter(gs_svr.predict(X_test), y_test-gs_svr.predict(X_test), c='green', s=40, alpha=0.5, edgecolor='white')\n",
    "plt.plot([-10,50], [0,0], c='black')\n",
    "plt.ylabel('Residuals ($y - \\hat{y}$)')\n",
    "plt.xlabel('Predicted values ($\\hat{y}$)')\n",
    "\n",
    "# 2 - More data!!!\n",
    "#     It would be better to have another chunk of data, e.g., from 2012.\n",
    "#     Then we could train on 2012 and tune our parameters on 2013.\n",
    "#     2014 could then serve as a final held-out test set to evaluate generalisation.\n",
    "#     It may be better to predict return / budget instead of the raw return value.\n",
    "#     We should also compare to a baseline model, e.g., using budget to predict gross return.\n",
    "#     Wikipedia is a bad source of descriptions for our scenario, since it is constantly\n",
    "#     edited and so we have data leakage. We could use the version of the Wikipedia\n",
    "#     abstract from when the movie was released.\n",
    "#     Printing the highest-weighted features highlights issues of data leakage..\n",
    "steps = dict(gs_svr.best_estimator_.steps) # access pipeline components directly\n",
    "import pprint\n",
    "vect = steps['vect'] # vectorizer component\n",
    "clf = steps['clf'] # classifier component\n",
    "feature_weights = clf.coef_\n",
    "feature_names = vect.get_feature_names_out()\n",
    "# print top 20 features\n",
    "print('\\nTop 20 features:')\n",
    "pprint.pprint(sorted(zip(feature_weights, feature_names), reverse=True)[:20])\n",
    "\n",
    "# 3 - No way would I deploy. The data leakage here means we can't trust this experiment setup.\n",
    "\n",
    "# 4 - We could try the ratio of gross to budget, then multiply a prediction by\n",
    "#     budget to get predicted gross.\n",
    "\n",
    "# 5 - Reviews, e.g., from Metacritic or Rotten Tomatoes.\n",
    "#     Here it would be good to compare to a baseline that predicts gross based on star ratings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# End of Tutorial. Many Thanks."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
