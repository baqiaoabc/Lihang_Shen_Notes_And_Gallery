{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Exploration with Python\n",
    "\n",
    "## EXERCISE 1: Reading and accessing data\n",
    "\n",
    "### Reading the WFH survey responses data using Pandas\n",
    "\n",
    "Download the _WFH-Survey-Responses-NSW.csv_ file from \"Week 3 Data Exploration with Python\" module on Canvas. This is a clean version of the file we worked on last week. Note that we have changed the format of the file to .csv (comma-separated values), so you can get familiar with another very common file type. Make sure that you save this file in the same folder you have this jupyter notebook.\n",
    "\n",
    "To read the file and store the data, we will use `pandas`, an external Python module which contains useful functionality for processing and transforming data. First we will read our file with the `read_csv` function, and then we will print the first 3 rows of our data using the `head` function, to see how it looks like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "pd.options.mode.chained_assignment = None\n",
    "\n",
    "df = pd.read_csv('WFH-Survey-Responses-NSW.csv')\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, there are four columns at the end of the file that contain no information. We can easily remove them by using the pandas `drop` function. You can use this function to remove columns or rows, or even single cells."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(columns=['Unnamed: 19','Unnamed: 20','Unnamed: 21','Unnamed: 22']).head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's define column header names (define constants for dictionary keys)\n",
    "\n",
    "In pandas, we can access the information of a column using the _header_ as an input, as `df['column_header']`. You can even select multiple columns, separating each column header by a comma, e.g: `df[['column1_header','column2_header']]`.\n",
    "\n",
    "Given that the headers in our file are very long questions, we can create a variable with a shorter name to store the original header. That way we can use this shorter version as an input instead of the original header, making it much easier to work with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RESPONSE = 'Response'\n",
    "YEAR_BORN = 'What year were you born?'\n",
    "GENDER = 'What is your gender?'\n",
    "INDUSTRY = 'Which of the following best describes your industry?'\n",
    "INDUSTRY_DETAILED = 'Which of the following best describes your industry? (Detailed)'\n",
    "OCCUPATION = 'Which of the following best describes your current occupation?'\n",
    "OCCUPATION_DETAILED = 'Which of the following best describes your current occupation? (Detailed)'\n",
    "ORGANISATION_EMPLOYEE_NUMBER = 'How many people are currently employed by your organisation?'\n",
    "MANAGE_PEOPLE = 'Do you manage people as part of your current occupation?'\n",
    "HOUSEHOLD = 'Which of the following best describes your household?'\n",
    "EMPLOYMENT_TIME = 'How long have you been in your current job?'\n",
    "METRO_REGIONAL = 'Metro / Regional'\n",
    "PERCENTAGE_WFH_LAST_YEAR ='Thinking about your current job, how much of your time did you spend remote working last year?'\n",
    "ORGANISATION_WFH_ENCOURAGEMENT = 'My organisation encouraged people to work remotely'\n",
    "ORGANISATION_WFH_PREPARATION = 'My organisation was well prepared for me to work remotely'\n",
    "ORGANISATION_WFH_COMMON = 'It was common for people in my organisation to work remotely'\n",
    "ORGANISATION_WFH_PERMISSION = 'It was easy to get permission to work remotely'\n",
    "WFH_COLLABORATION = 'I could easily collaborate with colleagues when working remotely'\n",
    "WFH_RECOMMEND = 'I would recommend remote working to others'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accessing columns\n",
    "\n",
    "Now that we have created an easier way to access a column, let's see how it works.\n",
    "\n",
    "Let's select the column that contains the answers to the question _What year were you born?_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df[YEAR_BORN]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TODO: Select the columns having answers to the questions: _Which of the following best describes your industry?_ and _Which of the following best describes your industry? (Detailed)_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# TODO: replace the content of this cell with your Python solution\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## *STOP PLEASE. THE FOLLOWING IS FOR THE NEXT EXERCISE. THANKS.*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EXERCISE 2: Frequency distribution\n",
    "\n",
    "Obtaining the frequency distribution or mode of a column is quite simple when using pandas. We first need to select the column we want to use, and then by using the `value_counts()` function. This function will count the number of times the same value appears in that column and return the frequency distribution.\n",
    "\n",
    "Let's obtain the frequency distribution for the question _What year were you born?_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df[YEAR_BORN].value_counts())\n",
    "print(df[YEAR_BORN].value_counts().max())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TODO: Calculate frequency distribution for the question: _Which of the following best describes your industry?_ and _Which of the following best describes your industry? (Detailed)_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# TODO: replace the content of this cell with your Python solution\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import datetime64\n",
    "from datetime import datetime\n",
    "# Reference https://numpy.org/doc/1.18/reference/arrays.datetime.html\n",
    "df[YEAR_BORN] = df[YEAR_BORN].apply(str)\n",
    "df[YEAR_BORN] = pd.Series([datetime.strptime(year,'%Y') for year in df[YEAR_BORN]])\n",
    "# If you need a datetime type (note pandas does not support times coarser than nanosecond.)\n",
    "df.astype({YEAR_BORN: 'datetime64[ns]'})\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode values as NaNs (not a number) or NaTs (not a time)\n",
    "import numpy as np\n",
    "before = df[YEAR_BORN].min()\n",
    "df[YEAR_BORN] = df[YEAR_BORN].replace(np.datetime64('1900-01-01'), np.datetime64('NaT'))\n",
    "after = df[YEAR_BORN].min()\n",
    "print('before:', before)\n",
    "print('after:', after)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TODO: Update a function that cleans a Pandas Series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gender_series = pd.Series(['M', 'Male', 'NB', 'Female', 'F', 'NonBinary', 'Undisclosed'])\n",
    "\n",
    "# Define the set of allowed values for the Series\n",
    "from enum import Enum\n",
    "class Gender(Enum):\n",
    "    UNKNOWN = 1\n",
    "    FEMALE = 2\n",
    "    MALE = 3\n",
    "    NONBINARY = 4\n",
    "\n",
    "# A function that applies a transformation to the data in a series\n",
    "def my_function(value):\n",
    "    \"\"\"Example: manually map string values to an Enum\"\"\"\n",
    "    if value in {'Female', 'F'}:\n",
    "        return Gender.FEMALE\n",
    "    # TODO: handle other values\n",
    "    else:\n",
    "        raise NotImplementedError(f'TODO: Handle {value}.')\n",
    "\n",
    "gender_series.apply(my_function)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## *STOP PLEASE. THE FOLLOWING IS FOR THE NEXT EXERCISE. THANKS.*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EXERCISE 3: Calculating descriptive statistics\n",
    "\n",
    "### Statistics with Pandas\n",
    "\n",
    "Pandas includes multiple statistic functions, such as `min()`, `max()`, `mean()` and `median()`. Additionally, it includes the function `describe()`, which provides descriptive statistics.\n",
    "\n",
    "Let's have a look at the statistics for the question _What year were you born?_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[YEAR_BORN].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's have a look at the statistics we get when dealing with nominal data. To do this, we will obtain the descriptive statistics for the question _Which of the following best describes your industry?_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[INDUSTRY].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TODO: Obtain the descriptive statistics for the questions: _Which of the following best describes your current occupation?_ and _How many people are currently employed by your organisation?_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: replace the content of this cell with your Python solution\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## *STOP PLEASE. THE FOLLOWING IS FOR THE NEXT EXERCISE. THANKS.*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## EXERCISE 4: Visualisation with matplotlib\n",
    "\n",
    "### Making a histogram\n",
    "\n",
    "`matplotlib` provides functionality for creating various plots.\n",
    "\n",
    "Let's make a histogram for the question _What year were you born?_ To create a histogram, we use the `hist(x,bins=n)` function from matplotlib, where we need to specify the values (`x`) we want to plot and the number of bins (`n`) we want in our histogram. Additionally, we can specify the space between bars using the `rwidth` option."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.hist(df[YEAR_BORN], bins = 5, rwidth=0.8)\n",
    "plt.ylabel('Number of responses')\n",
    "plt.xlabel('Year')\n",
    "plt.title('What year were you born?')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try changing the number of bins and observe how the plot changes. The higher the number of bins, the smaller the bars will become, as it will divide the data in more segments. If you know the _min_ and _max_ of your data values, you can calculate an appropiate number of bins depending of what you want to observe. For example, if your data values go from 1 to 100, if you select 10 bins, it will divide your data in segments of every 10: 1-10, 11-20, ... , 91-100. If you select 20 bins, it will divide your data in segments of every 5: 1-5, 6-10, ... , 96-100. Always choose a number of bins that allows you to observe the tendency of the data.\n",
    "\n",
    "Now, let's make a histogram with some nominal data. To do this, we first need to obtain the frequency distribution for the data we want to plot (See Exercise 2), and then use a bar plot to visualise the distribution. In this case, we don't need to use the histogram function, and there's no bin size because we're not dealing with numerical data.\n",
    "\n",
    "Let's make the bar plot for the question _Which of the following best describes your industry?_ Given that our data is nominal data, it's best to make a horizontal bar plot. Additionally, we can use the pandas function `plot.barh()` to plot the data. This way, we only need to obtain the frequency distribution of the data and then plot. We can set the title of the plot as an option and then we can specify the labels of the axis using the `set_xlabel()` and `set_ylabel` functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "industry_freq = df[INDUSTRY].value_counts()\n",
    "ax = industry_freq.plot.barh(title='Which of the following best describes your industry?')\n",
    "ax.set_xlabel('Frequency')\n",
    "ax.set_ylabel('Industry')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TODO: Make a histogram for the question: _Which of the following best describes your current occupation?_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# TODO: replace the content of this cell with your Python solution\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Making a scatterplot\n",
    "\n",
    "Finally, let's make a scatterplot to compare the year born with the percentage of time WFH."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data = df[[YEAR_BORN,PERCENTAGE_WFH_LAST_YEAR]]\n",
    "data[PERCENTAGE_WFH_LAST_YEAR] = data[PERCENTAGE_WFH_LAST_YEAR].str.rstrip('%').astype('float')\n",
    "data_sorted = data.sort_values(by=YEAR_BORN)\n",
    "\n",
    "plt.scatter( data_sorted[YEAR_BORN], data_sorted[PERCENTAGE_WFH_LAST_YEAR], s=5)\n",
    "plt.title('Year born vs Percentage WFH')\n",
    "plt.xlabel('Year born')\n",
    "plt.ylabel('Percentage')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extra: Exploring other plotting options and customisation\n",
    "\n",
    "If you would like to explore more plotting options, we recommend you visit the `seaborn` tutorial here https://seaborn.pydata.org/tutorial.html\n",
    "\n",
    "You will find multiple plotting options and will learn how to edit your plot to your liking. Here, we show you some examples of what you can achieve by using the `seaborn` library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "sns.set_theme(color_codes=True)\n",
    "tips = sns.load_dataset(\"tips\")\n",
    "sns.lmplot(x=\"total_bill\", y=\"tip\", hue=\"smoker\", data=tips, markers=[\"o\", \"x\"], palette=\"Set1\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## EXERCISE 5: Boxplot and Correlation\n",
    "\n",
    "### Draw a boxplot for year born\n",
    "\n",
    "Mean and standard deviation are not informative for skewed data. `boxplot` is a good visualisation for viewing and comparing distributions. It also shows outliers, e.g., values greater than `Q3+1.5*IQR` or less than `Q1-1.5*IQR`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from datetime import datetime, date\n",
    "\n",
    "def birthdate_to_age(born):\n",
    "    #get today's date\n",
    "    today = date.today()\n",
    "    return int(today.year - born.year - ((today.month, today.day) < (born.month, born.day)))\n",
    "\n",
    "data = df[YEAR_BORN].dropna().to_list()\n",
    "age_list = [birthdate_to_age(b) for b in data]\n",
    "\n",
    "fig = plt.figure(figsize =(10, 7))\n",
    "plt.boxplot(age_list)\n",
    "plt.title('Distribution of Age')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate correlation between two variables\n",
    "\n",
    "Scipy includes various correlation statistics​\n",
    "\n",
    "- Pearson’s r for two normally distributed variables​: stats.pearsonr()\n",
    "\n",
    "- Spearman’s rho for ratio data, ordinal data, etc (rank-order correlation): stats.spearmanr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import stats\n",
    "\n",
    "# only keep rows where both year born and percentage wfh last year are defined\n",
    "data = df[[YEAR_BORN,PERCENTAGE_WFH_LAST_YEAR]].dropna()\n",
    "\n",
    "year_born = data[YEAR_BORN]\n",
    "precent_wfh = data[PERCENTAGE_WFH_LAST_YEAR]\n",
    "\n",
    "print(stats.spearmanr(year_born, precent_wfh)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## EXERCISE 6: Text data\n",
    "\n",
    "### Simple tokenisation and word counts\n",
    "\n",
    "Tokenisation is the process of breaking text into it's component parts, e.g., sentences, words. Below is a simple whitespace tokeniser that also removes some leading/trailing punctuation. We can use this to count the frequency of terms across our data analytics definitions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenise(text):\n",
    "    for word in text.lower().split():\n",
    "        yield word.strip('.,')\n",
    "\n",
    "def is_valid_word(w):\n",
    "    if w == '':\n",
    "        return False\n",
    "    else:\n",
    "        return True\n",
    "\n",
    "def get_words(d):\n",
    "    words = []\n",
    "    for word in tokenise(d):\n",
    "        if is_valid_word(word):\n",
    "            words.append(word)\n",
    "    return words\n",
    "\n",
    "text = df[OCCUPATION_DETAILED].to_string()\n",
    "data = get_words(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removing stop words\n",
    "\n",
    "Very common function words can be removed to focus our analysis on content words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "STOP_WORDS = frozenset([ # http://www.nltk.org/book/ch02.html#stopwords_index_term\n",
    "    'i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', 'your', 'yours',\n",
    "    'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', 'her', 'hers',\n",
    "    'herself', 'it', 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves',\n",
    "    'what', 'which', 'who', 'whom', 'this', 'that', 'these', 'those', 'am', 'is', 'are',\n",
    "    'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does',\n",
    "    'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until',\n",
    "    'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into',\n",
    "    'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down',\n",
    "    'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here',\n",
    "    'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more',\n",
    "    'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so',\n",
    "    'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', 'should', 'now'\n",
    "    ])\n",
    "\n",
    "def is_valid(w):\n",
    "    if w.lower() in STOP_WORDS: \n",
    "        return False\n",
    "    elif w.isdigit(): # if all characters in the string are digits\n",
    "        return False\n",
    "    \n",
    "    return True\n",
    "\n",
    "data = [w for w in data if is_valid(w)]\n",
    "data[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting term frequency\n",
    "\n",
    "Now we can build a simple horizontal bar chart that displays the most common terms across data analytics definitions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "\n",
    "def iter_word_freq(data, min_freq = 50):\n",
    "    c = Counter(data)\n",
    "    for term, freq in c.items():\n",
    "        if freq >= min_freq: \n",
    "            yield term, freq\n",
    "\n",
    "d = {k: v for k, v in sorted(iter_word_freq(data))}\n",
    "\n",
    "ys = [i+0.5 for i,_ in enumerate(d)]\n",
    "plt.barh(ys, d.values(), align='center')\n",
    "plt.yticks(ys, list(d.keys()))\n",
    "plt.axis([0,600,0-0.1,len(d)+0.1])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# End of Tutorial. Many Thanks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  },
  "vscode": {
   "interpreter": {
    "hash": "4396f389b93e7269692bd3bea4c62813bbe379469bde939b058805f538feec11"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
