{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "215ff320",
   "metadata": {},
   "source": [
    "# Workshop 6: spaCy\n",
    "\n",
    "Today, you will learn about spaCy, a robust and widely used library for NLP. This lab is based on [a tutorial](https://course.spacy.io/en/)  by Ines Montani, one of the creators of spaCy. One fun note - the other founder of spaCy completed his PhD at USyd!\n",
    "\n",
    "First, we need to install the library:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "52fc35ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.2 -> 25.0.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install spacy --quiet"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "6eeb3a97",
   "metadata": {},
   "source": [
    "# Core parts of spaCy\n",
    "\n",
    "At the center of spaCy is the object containing the processing pipeline. We usually call this variable \"nlp\".\n",
    "\n",
    "For example, to create an English `nlp` object, you can `import spacy` and use the `spacy.blank` method to create a blank English pipeline. You can use the `nlp` object like a function to analyze text.\n",
    "\n",
    "It contains all the different components in the pipeline.\n",
    "\n",
    "It also includes language-specific rules used for tokenizing the text into words and punctuation. spaCy supports a variety of languages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b1e0ab56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import spaCy\n",
    "import spacy\n",
    "\n",
    "# Create a blank English nlp object\n",
    "nlp = spacy.blank(\"en\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e716aca4",
   "metadata": {},
   "source": [
    "When you process a text with the `nlp` object, spaCy creates a `Doc` object – short for \"document\". The Doc lets you access information about the text in a structured way, and no information is lost.\n",
    "\n",
    "Each document contains a series of tokens\n",
    "\n",
    "![image.png](https://course.spacy.io/doc.png)\n",
    "\n",
    "`Token` objects represent the tokens in a document – for example, a word or a punctuation character.\n",
    "\n",
    "To get a token at a specific position, you can index into the doc.\n",
    "\n",
    "`Token` objects also provide various attributes that let you access more information about the tokens. For example, the `.text` attribute returns the verbatim token text.\n",
    "\n",
    "The Doc behaves like a normal Python sequence by the way and lets you iterate over its tokens, or get a token by its index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bad59536",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "world\n",
      "Hello\n",
      "world\n",
      "!\n"
     ]
    }
   ],
   "source": [
    "# Created by processing a string of text with the nlp object\n",
    "doc = nlp(\"Hello world!\")\n",
    "\n",
    "# Index into the Doc to get a single Token\n",
    "token = doc[1]\n",
    "\n",
    "# Get the token text via the .text attribute\n",
    "print(token.text)\n",
    "\n",
    "# Iterate over tokens in a Doc\n",
    "for token in doc:\n",
    "    print(token.text)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ce853975",
   "metadata": {},
   "source": [
    "A `Span` object is a slice of the document consisting of one or more tokens. It's only a view of the `Doc` and doesn't contain any data itself.\n",
    "\n",
    "![image.png](https://course.spacy.io/doc_span.png)\n",
    "\n",
    "To create a span, you can use Python's slice notation. For example, `1:3` will create a slice starting from the token at position 1, up to – but not including! – the token at position 3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c2f2a7fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NLP class\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(\"Hello NLP class!\")\n",
    "\n",
    "# A slice from the Doc is a Span object\n",
    "span = doc[1:3]\n",
    "\n",
    "# Get the span text via the .text attribute\n",
    "print(span.text)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "66c49b41",
   "metadata": {},
   "source": [
    "Here you can see some of the available token attributes:\n",
    "\n",
    "`i` is the index of the token within the parent document.\n",
    "\n",
    "`text` returns the token text.\n",
    "\n",
    "`is_alpha`, `is_punct` and `like_num` return boolean values indicating whether the token consists of alphabetic characters, whether it's punctuation or whether it resembles a number. For example, a token \"10\" – one, zero – or the word \"ten\" – T, E, N.\n",
    "\n",
    "These attributes are also called lexical attributes: they refer to the entry in the vocabulary and don't depend on the token's context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9963e2d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index:    [0, 1, 2, 3, 4]\n",
      "Text:     ['It', 'costs', '$', '5', '.']\n",
      "is_alpha: [True, True, False, False, False]\n",
      "is_punct: [False, False, False, False, True]\n",
      "like_num: [False, False, False, True, False]\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(\"It costs $5.\")\n",
    "print(\"Index:   \", [token.i for token in doc])\n",
    "print(\"Text:    \", [token.text for token in doc])\n",
    "\n",
    "print(\"is_alpha:\", [token.is_alpha for token in doc])\n",
    "print(\"is_punct:\", [token.is_punct for token in doc])\n",
    "print(\"like_num:\", [token.like_num for token in doc])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "1062c70f",
   "metadata": {},
   "source": [
    "Now let's try applying what you learned above.\n",
    "\n",
    "- Use `spacy.blank` to create a blank English (`\"en\"`) `nlp` object.\n",
    "- Create a `doc` for the sentence \"This is a sentence.\" and print its text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1df6e6f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is a sentence.\n"
     ]
    }
   ],
   "source": [
    "# TODO\n",
    "\n",
    "# Solution\n",
    "# Import spaCy\n",
    "import spacy\n",
    "\n",
    "# Create the English nlp object\n",
    "nlp = spacy.blank(\"en\")\n",
    "\n",
    "# Process a text\n",
    "doc = nlp(\"This is a sentence.\")\n",
    "\n",
    "# Print the document text\n",
    "print(doc.text)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "93c1b4c0",
   "metadata": {},
   "source": [
    "spaCy supports many different languages.\n",
    "\n",
    "Try doing the same thing, but this time in German (`\"de\"`) with \"Das ist ein Satz.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "daec3d22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Das ist ein Satz.\n"
     ]
    }
   ],
   "source": [
    "# TODO\n",
    "\n",
    "# Solution\n",
    "# Import spaCy\n",
    "import spacy\n",
    "\n",
    "# Create the German nlp object\n",
    "nlp = spacy.blank(\"de\")\n",
    "\n",
    "# Process a text (this is German for: \"This is a sentence.\")\n",
    "doc = nlp(\"Das ist ein Satz.\")\n",
    "\n",
    "# Print the document text\n",
    "print(doc.text)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "8373db1c",
   "metadata": {},
   "source": [
    "And again, in Spanish (`\"es\"`) with \"Esta es una frase.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ccda9e64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Esta es una frase.\n"
     ]
    }
   ],
   "source": [
    "# TODO\n",
    "\n",
    "# Solution\n",
    "# Import spaCy\n",
    "import spacy\n",
    "\n",
    "# Create the Spanish nlp object\n",
    "nlp = spacy.blank(\"es\")\n",
    "\n",
    "# Process a text (this is Spanish for: \"This is a sentence.\")\n",
    "doc = nlp(\"Esta es una frase.\")\n",
    "\n",
    "# Print the document text\n",
    "print(doc.text)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "2f9eafca",
   "metadata": {},
   "source": [
    "When you call `nlp` on a string, spaCy first tokenizes the text and creates a document object. In this exercise, you’ll learn more about the `Doc`, as well as its views `Token` and `Span`.\n",
    "\n",
    "Step 1\n",
    "- Use `spacy.blank` to create the English `nlp` object for the sentence \"I like tree kangaroos and narwhals.\".\n",
    "- Process the text and instantiate a `Doc` object.\n",
    "- Select the first token of the `Doc` and print its `text`.\n",
    "- Create a slice of the `Doc` for the tokens “tree kangaroos” and “tree kangaroos and narwhals”."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9c057d84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I\n",
      "tree kangaroos\n",
      "tree kangaroos and narwhals\n"
     ]
    }
   ],
   "source": [
    "# TODO\n",
    "\n",
    "# Solution\n",
    "# Import spaCy and create the English nlp object\n",
    "import spacy\n",
    "\n",
    "nlp = spacy.blank(\"en\")\n",
    "\n",
    "# Process the text\n",
    "doc = nlp(\"I like tree kangaroos and narwhals.\")\n",
    "\n",
    "# Select the first token\n",
    "first_token = doc[0]\n",
    "\n",
    "# Print the first token's text\n",
    "print(first_token.text)\n",
    "\n",
    "# A slice of the Doc for \"tree kangaroos\"\n",
    "tree_kangaroos = doc[2:4]\n",
    "print(tree_kangaroos.text)\n",
    "\n",
    "# A slice of the Doc for \"tree kangaroos and narwhals\" (without the \".\")\n",
    "tree_kangaroos_and_narwhals = doc[2:6]\n",
    "print(tree_kangaroos_and_narwhals.text)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "2d48dd61",
   "metadata": {},
   "source": [
    "Now, let's use spaCy’s Doc and Token objects, and lexical attributes to find things in text.\n",
    "\n",
    "Try to find every time a percentage is mentioned. You’ll be looking for a token that is a number, followed by a token that is a percent sign.\n",
    "\n",
    "- Create a `Doc` for the text \"In 1990, more than 60% of people in East Asia were in extreme poverty. Now less than 4% are.\"\n",
    "- Use the `like_num` token attribute to check whether a token in the `doc` resembles a number.\n",
    "- Get the token following the current token in the document. Remember that `token.i` gives you the position of a token in a document.\n",
    "- Check whether the next token’s `text` attribute is a percent sign ”%“."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5a6226e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percentage found: 60\n",
      "Percentage found: 4\n"
     ]
    }
   ],
   "source": [
    "# TODO\n",
    "\n",
    "# Solution\n",
    "import spacy\n",
    "\n",
    "nlp = spacy.blank(\"en\")\n",
    "\n",
    "# Process the text\n",
    "doc = nlp(\n",
    "    \"In 1990, more than 60% of people in East Asia were in extreme poverty. Now less than 4% are.\"\n",
    ")\n",
    "\n",
    "# Iterate over the tokens in the doc\n",
    "for token in doc:\n",
    "    # Check if the token resembles a number\n",
    "    if token.like_num:\n",
    "        # Get the next token in the document\n",
    "        next_token = doc[token.i + 1]\n",
    "        # Check if the next token's text equals \"%\"\n",
    "        if next_token.text == \"%\":\n",
    "            print(\"Percentage found:\", token.text)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "9102a883",
   "metadata": {},
   "source": [
    "So far, we've used the `nlp` module on its own, with some built-in capabilities (e.g., breaking text up into a series of tokens).\n",
    "\n",
    "To do more, we use *trained pipelines*. These are models that enable spaCy to predict linguistic attributes in context, e.g.,\n",
    "- Part-of-speech tags\n",
    "- Syntactic dependencies\n",
    "- Named entities\n",
    "\n",
    "Trained pipeline components have statistical models that enable spaCy to make predictions in context.  Pipelines are trained on large datasets of labeled examples. They can be updated with more examples to fine-tune their predictions – for example, to perform better on your specific data.\n",
    "\n",
    "spaCy provides a number of trained pipeline packages you can download using the `spacy download` command. For example, the \"en_core_web_sm\" package is a small English pipeline that supports all core capabilities and is trained on web text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "16e66c91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting en-core-web-sm==3.8.0\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.8.0/en_core_web_sm-3.8.0-py3-none-any.whl (12.8 MB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m12.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mm eta \u001b[36m0:00:01\u001b[0m0:01\u001b[0m:01\u001b[0m\n",
      "\u001b[?25h\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.0.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('en_core_web_sm')\n"
     ]
    }
   ],
   "source": [
    "!python -m spacy download en_core_web_sm"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "59182ee9",
   "metadata": {},
   "source": [
    "The `spacy.load` method loads a pipeline package by name and returns an nlp object. The package provides the binary weights that enable spaCy to make predictions. It also includes the vocabulary, meta information about the pipeline and the configuration file used to train it. It tells spaCy which language class to use and how to configure the processing pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c5cfebee",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "edd0c65e",
   "metadata": {},
   "source": [
    "## Part of Speech tags\n",
    "\n",
    "Let's take a look at the model's predictions. In this example, we're using spaCy to predict part-of-speech tags, the word types in context.\n",
    "\n",
    "The code prints both `token.pos_` and `token.pos`. Attributes that return strings usually end with an underscore. Attributes without the underscore return an integer ID value.\n",
    "\n",
    "Here, the model correctly predicts all of the POS tags."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a0a90cf3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "She PRON 95\n",
      "ate VERB 100\n",
      "the DET 90\n",
      "pizza NOUN 92\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "# Load the small English pipeline\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Process a text\n",
    "doc = nlp(\"She ate the pizza\")\n",
    "\n",
    "# Iterate over the tokens\n",
    "for token in doc:\n",
    "    # Print the text and the predicted part-of-speech tag\n",
    "    print(token.text, token.pos_, token.pos)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "610c0f63",
   "metadata": {},
   "source": [
    "## Syntax\n",
    "\n",
    "In addition to the part-of-speech tags, we can also predict syntactic structures. As a reminder, syntax captures how words in text are related. For example, whether a word is the subject or object of the main verb in the sentence.\n",
    "\n",
    "The `.dep_` attribute returns the predicted dependency label.\n",
    "\n",
    "The `.head` attribute returns the syntactic head token. You can also think of it as the parent token this word is modifying / attached to."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "470c7d5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "She PRON nsubj ate\n",
      "ate VERB ROOT ate\n",
      "the DET det pizza\n",
      "pizza NOUN dobj ate\n"
     ]
    }
   ],
   "source": [
    "for token in doc:\n",
    "    print(token.text, token.pos_, token.dep_, token.head)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "9456115a",
   "metadata": {},
   "source": [
    "To describe syntactic dependencies, spaCy uses a standardized label scheme. Here's an example of some common labels:\n",
    "\n",
    "The pronoun \"She\" is a nominal subject attached to the verb – in this case, to \"ate\".\n",
    "\n",
    "The noun \"pizza\" is a direct object attached to the verb \"ate\". It is eaten by the subject, \"she\".\n",
    "\n",
    "The determiner \"the\", also known as an article, is attached to the noun \"pizza\".\n",
    "\n",
    "We can draw the structure like this (as generated by displaCy, another part of the library):\n",
    "\n",
    "![image.png](https://course.spacy.io/dep_example.png)\n",
    "\n",
    "## Named Entities\n",
    "\n",
    "Named entities are \"real world objects\" that are assigned a name – for example, a person, an organization or a country.\n",
    "\n",
    "The `doc.ents` property lets you access the named entities predicted by the named entity recognition model.\n",
    "\n",
    "![image.png](https://course.spacy.io/ner_example.png)\n",
    "\n",
    "It returns an iterator of `Span` objects, so we can print the entity text and the entity label using the `.label_` attribute.\n",
    "\n",
    "In this case, the model is correctly predicting \"Apple\" as an organization, \"U.K.\" as a geopolitical entity and \"$1 billion\" as money."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "71804248",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Apple ORG\n",
      "U.K. GPE\n",
      "$1 billion MONEY\n"
     ]
    }
   ],
   "source": [
    "# Process a text\n",
    "doc = nlp(\"Apple is looking at buying U.K. startup for $1 billion\")\n",
    "\n",
    "# Iterate over the predicted entities\n",
    "for ent in doc.ents:\n",
    "    # Print the entity text and its label\n",
    "    print(ent.text, ent.label_)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "2c6477e0",
   "metadata": {},
   "source": [
    "A quick tip: To get definitions for the most common tags and labels, you can use the `spacy.explain` helper function.\n",
    "\n",
    "For example, \"GPE\" for geopolitical entity isn't exactly intuitive – but `spacy.explain` can tell you that it refers to countries, cities and states.\n",
    "\n",
    "The same works for part-of-speech tags and dependency labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "13ddefaf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Named entity label GPE means:\n",
      " Countries, cities, states\n",
      "POS tag NNP means:\n",
      " noun, proper singular\n",
      "Dependency type dobj means:\n",
      " direct object\n"
     ]
    }
   ],
   "source": [
    "print(\"Named entity label GPE means:\\n\", spacy.explain(\"GPE\"))\n",
    "print(\"POS tag NNP means:\\n\", spacy.explain(\"NNP\"))\n",
    "print(\"Dependency type dobj means:\\n\", spacy.explain(\"dobj\"))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "0db662f2",
   "metadata": {},
   "source": [
    "Write code to do the following:\n",
    "\n",
    "- Download the `en_core_web_md` model (a slightly larger and more accurate model)\n",
    "- Make one `nlp` object using the `en_core_web_md` model and one using the `en_core_web_sm` model from above\n",
    "- Run each one on \"Fruit flies like a banana\"\n",
    "- Print out the POS tags and dependencies produced by each model and identify any differences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9183d38c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting en-core-web-md==3.8.0\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_md-3.8.0/en_core_web_md-3.8.0-py3-none-any.whl (33.5 MB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m33.5/33.5 MB\u001b[0m \u001b[31m12.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mm eta \u001b[36m0:00:01\u001b[0m[36m0:00:01\u001b[0m\n",
      "\u001b[?25h\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.0.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('en_core_web_md')\n"
     ]
    }
   ],
   "source": [
    "# TODO\n",
    "\n",
    "# Solution\n",
    "!python -m spacy download en_core_web_md"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e884264a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fruit\n",
      "flies\n",
      "like\n",
      "a\n",
      "banana\n"
     ]
    }
   ],
   "source": [
    "# Solution continued\n",
    "\n",
    "# Load the small English pipeline\n",
    "nlp_small = spacy.load(\"en_core_web_sm\")\n",
    "nlp_medium = spacy.load(\"en_core_web_md\")\n",
    "\n",
    "# Process a text\n",
    "text = \"Fruit flies like a banana\"\n",
    "doc_small = nlp_small(text)\n",
    "doc_medium = nlp_medium(text)\n",
    "\n",
    "# Iterate over the tokens\n",
    "for i in range(len(doc_small)):\n",
    "    print(doc_small[i])\n",
    "    if doc_small[i].pos_ != doc_medium[i].pos_:\n",
    "        print(\"POS difference:\", doc_small[i].pos_, doc_medium[i].pos_)\n",
    "    if doc_small[i].dep_ != doc_medium[i].dep_:\n",
    "        print(\"Dep difference:\", doc_small[i].dep_, doc_medium[i].dep_)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "5ad154f4",
   "metadata": {},
   "source": [
    "# Matching\n",
    "\n",
    "Next, we'll take a look at spaCy's matcher, which lets you write rules to find words and phrases in text.\n",
    "\n",
    "If you are familiar with regular expressions, then they may be what you think of when looking for specific patterns in text. However, they are over strings. Here, we want to define patterns over `Doc` and `Token` objects too. For example, we might want to find the word \"duck\" but only if it's a verb, not a noun.\n",
    "\n",
    "Match patterns are lists of dictionaries. Each dictionary describes one token. The keys are the names of token attributes, mapped to their expected values. To use a pattern, we initialise it with the shared vocabulary, `nlp.vocab`. You'll learn more about this later – for now, just remember to always pass it in. The `matcher.add` method lets you add a pattern. The first argument is a unique ID to identify which pattern was matched. The second argument is a list of patterns.\n",
    "\n",
    "To match the pattern on a text, we can call the matcher on any doc. This will return the matches. Each tuple consists of three values: the  ID of the pattern that matched, the start index and the end index of the matched span. This means we can iterate over the matches and create a Span object: a slice of the doc at the start and end index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "bb9febce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"iPhone X\" - match for pattern 9528407286733565721 in span (0, 2)\n",
      "\"iPhone X\" - match for pattern 9528407286733565721 in span (5, 7)\n"
     ]
    }
   ],
   "source": [
    "# Import the Matcher\n",
    "from spacy.matcher import Matcher\n",
    "\n",
    "# Load a pipeline and create the nlp object\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Initialize the matcher with the shared vocab\n",
    "matcher = Matcher(nlp.vocab)\n",
    "\n",
    "# Add the pattern to the matcher\n",
    "pattern = [{\"TEXT\": \"iPhone\"}, {\"TEXT\": \"X\"}]\n",
    "matcher.add(\"IPHONE_PATTERN\", [pattern])\n",
    "\n",
    "# Process some text\n",
    "doc = nlp(\"iPhone X news! Upcoming iPhone X release date leaked\")\n",
    "\n",
    "# Call the matcher on the doc\n",
    "matches = matcher(doc)\n",
    "\n",
    "# Iterate over the matches\n",
    "for pattern_id, start, end in matches:\n",
    "    # Get the matched span\n",
    "    matched_span = doc[start:end]\n",
    "    print('\"{}\" - match for pattern {} in span ({}, {})'.format(matched_span.text, pattern_id, start, end))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "eeab2bf8",
   "metadata": {},
   "source": [
    "Here's an example of a more complex pattern using lexical attributes.\n",
    "\n",
    "We're looking for a five token sequence:\n",
    "- A token consisting of only digits.\n",
    "- Three case-insensitive tokens for \"fifa\", \"world\" and \"cup\" (`LOWER` indicates that once lowercased, the token must match the one we provide).\n",
    "- A token that consists of punctuation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d1f4c014",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"2018 FIFA World Cup:\" - match for pattern 7469410445798573543 in span (0, 5)\n"
     ]
    }
   ],
   "source": [
    "pattern = [\n",
    "    {\"IS_DIGIT\": True},\n",
    "    {\"LOWER\": \"fifa\"},\n",
    "    {\"LOWER\": \"world\"},\n",
    "    {\"LOWER\": \"cup\"},\n",
    "    {\"IS_PUNCT\": True}\n",
    "]\n",
    "matcher = Matcher(nlp.vocab)\n",
    "matcher.add(\"SOCCER_PATTERN\", [pattern])\n",
    "\n",
    "doc = nlp(\"2018 FIFA World Cup: France won!\")\n",
    "\n",
    "matches = matcher(doc)\n",
    "for pattern_id, start, end in matches:\n",
    "    matched_span = doc[start:end]\n",
    "    print('\"{}\" - match for pattern {} in span ({}, {})'.format(matched_span.text, pattern_id, start, end))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "8ec9e893",
   "metadata": {},
   "source": [
    "In this example, we're looking for two tokens. A verb with the lemma \"love\", followed by a noun. Remember that a lemma is a canonical form of a word that has multiple variations, e.g., 'run' is a lemma for 'ran', 'running', and 'run'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f0390c1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"loved vanilla\" - match for pattern 4358456325055851256 in span (1, 3)\n",
      "\"love chocolate\" - match for pattern 4358456325055851256 in span (6, 8)\n"
     ]
    }
   ],
   "source": [
    "pattern = [\n",
    "    {\"LEMMA\": \"love\", \"POS\": \"VERB\"},\n",
    "    {\"POS\": \"NOUN\"}\n",
    "]\n",
    "matcher = Matcher(nlp.vocab)\n",
    "matcher.add(\"LOVE_PATTERN\", [pattern])\n",
    "\n",
    "doc = nlp(\"I loved vanilla but now I love chocolate more.\")\n",
    "\n",
    "matches = matcher(doc)\n",
    "for pattern_id, start, end in matches:\n",
    "    matched_span = doc[start:end]\n",
    "    print('\"{}\" - match for pattern {} in span ({}, {})'.format(matched_span.text, pattern_id, start, end))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c0a80d6b",
   "metadata": {},
   "source": [
    "Operators and quantifiers let you define how often a token should be matched. They can be added using the \"OP\" key. \"OP\" can have one of four values:\n",
    "\n",
    "- An \"!\" negates the token, so it's matched 0 times.\n",
    "- A \"?\" makes the token optional, and matches it 0 or 1 times.\n",
    "- A \"+\" matches a token 1 or more times.\n",
    "- And finally, an \"*\" matches 0 or more times.\n",
    "\n",
    "Operators can make your patterns a lot more powerful, but they also add more complexity – so use them wisely.\n",
    "\n",
    "Here, the \"?\" operator makes the determiner token optional, so it will match a token with the lemma \"buy\", an optional article and a noun."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1dd103c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"bought a smartphone\" - match for pattern 8301083161022457728 in span (1, 4)\n",
      "\"buying apps\" - match for pattern 8301083161022457728 in span (8, 10)\n"
     ]
    }
   ],
   "source": [
    "pattern = [\n",
    "    {\"LEMMA\": \"buy\"},\n",
    "    {\"POS\": \"DET\", \"OP\": \"?\"},  # optional: match 0 or 1 times\n",
    "    {\"POS\": \"NOUN\"}\n",
    "]\n",
    "matcher = Matcher(nlp.vocab)\n",
    "matcher.add(\"BUY_PATTERN\", [pattern])\n",
    "\n",
    "doc = nlp(\"I bought a smartphone. Now I'm buying apps.\")\n",
    "\n",
    "matches = matcher(doc)\n",
    "for pattern_id, start, end in matches:\n",
    "    matched_span = doc[start:end]\n",
    "    print('\"{}\" - match for pattern {} in span ({}, {})'.format(matched_span.text, pattern_id, start, end))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c2d59a6f",
   "metadata": {},
   "source": [
    "Try writing code to match names of iOS versions in the text below, e.g., \"iOS 5\", and \"iOS 13\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "016120d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"iOS 7\" - match for pattern 2870259198116999432 in span (25, 27)\n",
      "\"iOS 11\" - match for pattern 2870259198116999432 in span (31, 33)\n",
      "\"iOS 10\" - match for pattern 2870259198116999432 in span (40, 42)\n"
     ]
    }
   ],
   "source": [
    "text = \"\"\"After making the iOS update you won't notice a radical system-wide\n",
    "redesign: nothing like the aesthetic upheaval we got with iOS 7. Most of\n",
    "iOS 11's furniture remains the same as in iOS 10. But you will discover some\n",
    "tweaks once you delve a little deeper.\"\"\"\n",
    "\n",
    "# TODO\n",
    "\n",
    "# Solution\n",
    "matcher = Matcher(nlp.vocab)\n",
    "\n",
    "# Write a pattern for full iOS versions (\"iOS 7\", \"iOS 11\", \"iOS 10\")\n",
    "pattern = [{\"TEXT\": \"iOS\"}, {\"IS_DIGIT\": True}]\n",
    "\n",
    "# Add the pattern to the matcher and apply the matcher to the doc\n",
    "matcher.add(\"IOS_VERSION_PATTERN\", [pattern])\n",
    "\n",
    "# Iterate over the matches and print the span text\n",
    "doc = nlp(text)\n",
    "matches = matcher(doc)\n",
    "for pattern_id, start, end in matches:\n",
    "    matched_span = doc[start:end]\n",
    "    print('\"{}\" - match for pattern {} in span ({}, {})'.format(matched_span.text, pattern_id, start, end))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "0140239a",
   "metadata": {},
   "source": [
    "Now try writing code to match forms of “download” (tokens with the lemma “download”), followed by a token with the part-of-speech tag \"PROPN\" (proper noun)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "1719b348",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"downloaded Fortnite\" - match for pattern 1475109908168048428 in span (1, 3)\n",
      "\"downloading Minecraft\" - match for pattern 1475109908168048428 in span (22, 24)\n",
      "\"download Winzip\" - match for pattern 1475109908168048428 in span (55, 57)\n"
     ]
    }
   ],
   "source": [
    "text = \"\"\"i downloaded Fortnite on my laptop and can't open the game at all. Help?\n",
    "so when I was downloading Minecraft, I got the Windows version where it\n",
    "is the '.zip' folder and I used the default program to unpack it... do\n",
    "I also need to download Winzip?\"\"\"\n",
    "\n",
    "# TODO\n",
    "\n",
    "# Solution\n",
    "matcher = Matcher(nlp.vocab)\n",
    "\n",
    "# Write a pattern that matches a form of \"download\" plus proper noun\n",
    "pattern = [{\"LEMMA\": \"download\"}, {\"POS\": \"PROPN\"}]\n",
    "\n",
    "# Add the pattern to the matcher and apply the matcher to the doc\n",
    "matcher.add(\"DOWNLOAD_THINGS_PATTERN\", [pattern])\n",
    "\n",
    "# Iterate over the matches and print the span text\n",
    "doc = nlp(text)\n",
    "matches = matcher(doc)\n",
    "for pattern_id, start, end in matches:\n",
    "    matched_span = doc[start:end]\n",
    "    print('\"{}\" - match for pattern {} in span ({}, {})'.format(matched_span.text, pattern_id, start, end))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "da165a7a",
   "metadata": {},
   "source": [
    "Finally, write a pattern that matches an adjective (`ADJ`) followed by one or two nouns (`NOUN`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "9a865b9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"beautiful design\" - match for pattern 5488211386492616699 in span (6, 8)\n",
      "\"smart search\" - match for pattern 5488211386492616699 in span (9, 11)\n",
      "\"automatic labels\" - match for pattern 5488211386492616699 in span (12, 14)\n",
      "\"optional voice\" - match for pattern 5488211386492616699 in span (15, 17)\n",
      "\"optional voice responses\" - match for pattern 5488211386492616699 in span (15, 18)\n"
     ]
    }
   ],
   "source": [
    "text = \"Features of the app include a beautiful design, smart search, automatic labels and optional voice responses.\"\n",
    "\n",
    "# TODO\n",
    "\n",
    "# Solution\n",
    "matcher = Matcher(nlp.vocab)\n",
    "\n",
    "# Write a pattern that matches a form of \"download\" plus proper noun\n",
    "pattern = [{\"POS\": \"ADJ\"}, {\"POS\": \"NOUN\"}, {\"POS\": \"NOUN\", \"OP\": \"?\"}]\n",
    "\n",
    "# Add the pattern to the matcher and apply the matcher to the doc\n",
    "matcher.add(\"ADJ_NOUN_PATTERN\", [pattern])\n",
    "\n",
    "# Iterate over the matches and print the span text\n",
    "doc = nlp(text)\n",
    "matches = matcher(doc)\n",
    "for pattern_id, start, end in matches:\n",
    "    matched_span = doc[start:end]\n",
    "    print('\"{}\" - match for pattern {} in span ({}, {})'.format(matched_span.text, pattern_id, start, end))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "8c514fd1",
   "metadata": {},
   "source": [
    "# Customising Processing\n",
    "\n",
    "spaCy stores all shared data in a vocabulary, the Vocab. This includes words, but also the labels schemes for tags and entities. To save memory, all strings are encoded to hash IDs. If a word occurs more than once, we don't need to save it every time. Instead, spaCy uses a hash function to generate an ID and stores the string only once in the string store. The string store is available as nlp.vocab.strings.\n",
    "\n",
    "It's a lookup table that works in both directions. You can look up a string and get its hash, and look up a hash to get its string value. Internally, spaCy only communicates in hash IDs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b3fe66b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14949295185858420483\n",
      "hat\n"
     ]
    }
   ],
   "source": [
    "nlp = spacy.blank(\"en\")\n",
    "\n",
    "# Look up the hash for the word \"hat\"\n",
    "hat_hash = nlp.vocab.strings[\"hat\"]\n",
    "print(hat_hash)\n",
    "\n",
    "# Look up the cat_hash to get the string\n",
    "hat_string = nlp.vocab.strings[hat_hash]\n",
    "print(hat_string)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "5a4dc504",
   "metadata": {},
   "source": [
    "Lexemes are context-independent entries in the vocabulary. You can get a lexeme by looking up a string or a hash ID in the vocab. Lexemes expose attributes, just like tokens. They hold context-independent information about a word, like the text, or whether the word consists of alphabetic characters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "8fdd6c8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tea 6041671307218480733 True\n"
     ]
    }
   ],
   "source": [
    "lexeme = nlp.vocab[\"tea\"]\n",
    "\n",
    "# Print the lexical attributes\n",
    "print(lexeme.text, lexeme.orth, lexeme.is_alpha)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c79b422e",
   "metadata": {},
   "source": [
    "So far, we've been creating `Doc`s by passing in some text and letting spaCy tokenise the text. We can also provide pre-tokenised text:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f255abe8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello world!\n"
     ]
    }
   ],
   "source": [
    "# Import the Doc class\n",
    "from spacy.tokens import Doc\n",
    "\n",
    "# The words to create the doc from\n",
    "words = [\"Hello\", \"world\", \"!\"]\n",
    "# Boolean values, one for each word, indicating if there is a space after it in the text\n",
    "spaces = [True, False, False]\n",
    "\n",
    "# Create a doc manually\n",
    "doc = Doc(nlp.vocab, words=words, spaces=spaces)\n",
    "print(doc)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "75b238e9",
   "metadata": {},
   "source": [
    "Earlier we created spans from `Doc`s, but we can also create them manually and we can manually change the list of labeled spans in the doc (the entities, `ents`):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "34825718",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello world!\n",
      "(Hello world,)\n"
     ]
    }
   ],
   "source": [
    "# Import the Doc and Span classes\n",
    "from spacy.tokens import Span\n",
    "\n",
    "# Create a span manually\n",
    "span = Span(doc, 0, 2)\n",
    "\n",
    "# Create a span with a label\n",
    "span_with_label = Span(doc, 0, 2, label=\"GREETING\")\n",
    "\n",
    "# Add span to the doc.ents\n",
    "doc.ents = [span_with_label]\n",
    "print(doc)\n",
    "print(doc.ents)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "941b22c2",
   "metadata": {},
   "source": [
    "You've already written this plenty of times by now: pass a string of text to the nlp object, and receive a Doc object.\n",
    "\n",
    "But what does the nlp object actually do?\n",
    "\n",
    "First, the tokenizer is applied to turn the string of text into a Doc object. Next, a series of pipeline components is applied to the doc in order. In this case, the tagger, then the parser, then the entity recognizer. Finally, the processed doc is returned, so you can work with it.\n",
    "\n",
    "![image.png](https://course.spacy.io/pipeline.png)\n",
    "\n",
    "spaCy ships with a variety of built-in pipeline components. Here are some of the most common ones that you'll want to use in your projects.\n",
    "\n",
    "- The part-of-speech tagger sets the token.tag and token.pos attributes.\n",
    "- The dependency parser adds the token.dep and token.head attributes and is also responsible for detecting sentences and base noun phrases, also known as noun chunks.\n",
    "- The named entity recognizer adds the detected entities to the doc.ents property. It also sets entity type attributes on the tokens that indicate if a token is part of an entity or not.\n",
    "- Finally, the text classifier sets category labels that apply to the whole text, and adds them to the doc.cats property.\n",
    "\n",
    "Because text categories are always very specific, the text classifier is not included in any of the trained pipelines by default. But you can use it to train your own system.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "6fc25d42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "# List the components for the current nlp pipeline\n",
    "print(nlp.pipe_names)\n",
    "\n",
    "# Print the names and their corresponding objects\n",
    "print(nlp.pipeline)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a1dfa736",
   "metadata": {},
   "source": [
    "You can also create your own components. Each pipeline component is a function or callable that takes a doc, modifies it and returns it, so it can be processed by the next component in the pipeline.\n",
    "\n",
    "To tell spaCy where to find your custom component and how it should be called, you can decorate it using the @Language.component decorator. \n",
    " Once a component is registered, it can be added to the pipeline using the nlp.add_pipe method. The method takes at least one argument: the string name of the component.\n",
    "\n",
    "Below, we create a custom component that will print the length of the document as part of processing it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "3fe0b65d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline: ['custom_component', 'tok2vec', 'tagger', 'parser', 'attribute_ruler', 'lemmatizer', 'ner']\n",
      "Doc length: 4\n"
     ]
    }
   ],
   "source": [
    "from spacy.language import Language\n",
    "\n",
    "# Create the nlp object\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Define a custom component\n",
    "@Language.component(\"custom_component\")\n",
    "def custom_component_function(doc):\n",
    "    # Print the doc's length\n",
    "    print(\"Doc length:\", len(doc))\n",
    "    # Return the doc object\n",
    "    return doc\n",
    "\n",
    "# Add the component first in the pipeline\n",
    "nlp.add_pipe(\"custom_component\", first=True)\n",
    "\n",
    "# Print the pipeline component names\n",
    "print(\"Pipeline:\", nlp.pipe_names)\n",
    "\n",
    "# Run pipeline\n",
    "doc = nlp(\"A sample sentence.\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "42c50d81",
   "metadata": {},
   "source": [
    "Now, we're going to combine several ideas from above in this task. Implement a new pipeline component that uses a matcher to find the following insects:\n",
    "\n",
    "- flies\n",
    "- mosquitos\n",
    "- moths\n",
    "\n",
    "Your matcher should then modify the documents entities to have a span for each insect, with the label `INSECT`. Remember to use lemmas to account for different ways of writing an insect's name and to use POS tags to ensure you only identify nouns.\n",
    "\n",
    "Run your new pipeline on the provided text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "a2821821",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('moths', 'INSECT'), ('mosquitos', 'INSECT'), ('fly', 'INSECT'), ('Mosquitos', 'INSECT')]\n"
     ]
    }
   ],
   "source": [
    "text = \"Qantas flies all sorts of cargo! That includes moths, mosquitos, and even the occasional fly. Mosquitos are particular hard to transport safely.\"\n",
    "\n",
    "# TODO\n",
    "\n",
    "# Solution\n",
    "import spacy\n",
    "from spacy.language import Language\n",
    "from spacy.matcher import PhraseMatcher\n",
    "from spacy.tokens import Span\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "matcher = Matcher(nlp.vocab)\n",
    "for insect in [\"moth\", \"fly\", \"mosquito\"]:\n",
    "    pattern = [{\"POS\": \"NOUN\", \"LEMMA\": insect}]\n",
    "    matcher.add(\"INSECT\", [pattern])\n",
    "\n",
    "# Define the custom component\n",
    "@Language.component(\"insect_component\")\n",
    "def insect_component_function(doc):\n",
    "    # Apply the matcher to the doc\n",
    "    matches = matcher(doc)\n",
    "    # Create a Span for each match and assign the label \"INSECT\"\n",
    "    spans = [Span(doc, start, end, label=\"INSECT\") for match_id, start, end in matches]\n",
    "    # Overwrite the doc.ents with the matched spans\n",
    "    doc.ents = spans\n",
    "    return doc\n",
    "\n",
    "# Add the component to the pipeline after the \"ner\" component\n",
    "nlp.add_pipe(\"insect_component\", after=\"ner\")\n",
    "\n",
    "# Process the text and print the text and label for the doc.ents\n",
    "doc = nlp(text)\n",
    "print([(ent.text, ent.label_) for ent in doc.ents])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "409c4897",
   "metadata": {},
   "source": [
    "# Word Vectors\n",
    "\n",
    "spaCy can compare two objects and predict how similar they are – for example, documents, spans or single tokens. The Doc, Token and Span objects have a .similarity method that takes another object and returns the cosine similarity of their representations.\n",
    "\n",
    "For words, the word vector is its representation. For spans and docs, the average of word vectors for the words inside them is used.\n",
    "\n",
    "One thing that's very important: In order to use similarity, you need a larger spaCy pipeline that has word vectors included. Of the ones we've used so far, `en_core_web_md` does have word vectors, but `en_core_web_sm` does not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "f941f2b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting en-core-web-md==3.8.0\n",
      "  Using cached https://github.com/explosion/spacy-models/releases/download/en_core_web_md-3.8.0/en_core_web_md-3.8.0-py3-none-any.whl (33.5 MB)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.0.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('en_core_web_md')\n",
      "[-6.2388e-01  2.1805e-01  2.9327e-01  1.6850e-01 -4.8781e-01  1.3231e-01\n",
      " -5.4369e-01  1.8621e-02  4.4877e-01  1.3853e+00 -6.4706e-01  2.7613e-01\n",
      " -4.1182e-02  1.0464e-01  5.6399e-01 -4.5635e-01  3.9892e-02  1.5938e+00\n",
      "  4.0477e-02  1.3862e-01  1.7741e-01  8.0298e-03  9.3180e-02  9.7868e-02\n",
      " -1.1572e-01 -4.5268e-01  2.8416e-01 -7.4673e-02  2.9874e-01 -9.4577e-01\n",
      " -3.9780e-01  2.3396e-01  2.5034e-01 -4.7702e-01  3.3774e-01  2.9141e-01\n",
      " -2.2539e-01 -2.1549e-01  1.9321e-02  6.2851e-01 -2.9876e-02 -4.0990e-01\n",
      " -3.7827e-01  2.5225e-01 -2.6000e-01  2.9677e-01 -3.1861e-01  2.4687e-01\n",
      " -2.1345e-01 -5.7641e-02 -1.8985e-01 -8.1988e-02 -2.9714e-01  6.4590e-02\n",
      "  5.1448e-01 -5.1608e-01  4.7546e-01 -5.4936e-03 -2.9206e-01 -2.4819e-01\n",
      "  4.2659e-02 -1.3031e-01  5.8668e-01  6.5725e-01 -5.3470e-01 -4.8405e-01\n",
      "  2.9476e-01 -5.4968e-01 -8.9108e-02 -4.3224e-02 -1.8776e-01  5.6585e-01\n",
      "  5.6270e-01 -2.9567e-01  9.3576e-02 -3.0206e-02  3.3060e-01  8.0487e-01\n",
      "  5.7836e-01 -9.2357e-01  3.4386e-01  5.9752e-02 -3.2329e-01 -2.5743e-02\n",
      " -1.1146e-01 -9.7874e-01  1.4880e+00  9.6775e-01 -6.5339e-01  4.3041e-01\n",
      " -3.6090e-01 -1.8606e-01 -2.0447e-01 -3.6253e-01 -4.3361e-01 -3.1384e-01\n",
      " -4.5111e-01 -3.5929e-01 -9.3226e-02 -7.5689e-02  4.9116e-01 -9.4785e-02\n",
      " -2.3481e-01  5.5241e-01 -2.1856e-01 -1.2892e+00  3.6064e-01 -2.3190e-01\n",
      "  5.2711e-01 -4.9145e-02 -9.7221e-02 -5.5916e-01 -4.2153e-01 -3.8806e-01\n",
      "  3.6130e-01 -1.3304e-01  3.7184e-02  1.8662e-01  4.8692e-01  7.2283e-01\n",
      " -6.9026e-02  8.9823e-02 -3.4763e-02 -4.4230e-01 -1.9718e-01 -1.9133e-01\n",
      "  4.3469e-01 -1.6197e-01 -3.4121e-01  1.1533e-01 -3.7108e-01 -2.2534e-01\n",
      "  1.7260e-01  3.6153e-01 -8.1007e-02  2.8317e-01  3.0089e-01 -1.4058e-01\n",
      " -4.9626e-01 -4.3925e-01 -1.6939e+00 -1.7923e-01  9.3354e-01  1.9417e-01\n",
      " -2.9976e-01 -6.0479e-01 -9.0119e-01  1.2185e-01  1.7497e-01  5.4467e-02\n",
      "  7.8145e-01  5.1395e-01  3.2760e-01  6.4446e-03 -6.8385e-01 -2.1926e-01\n",
      "  1.2744e-01 -2.5916e-01  8.2790e-02 -4.7987e-02 -3.4334e-01  5.0777e-01\n",
      " -4.8346e-01  2.3135e-01 -3.2923e-01 -3.9766e-01 -2.4829e-01 -9.0369e-01\n",
      "  4.7786e-02 -3.8452e-01  6.4058e-01 -2.6015e-01  1.2925e-01 -2.1061e-01\n",
      " -1.9551e-01 -5.0431e-01  2.0986e-02  7.9793e-02  1.0182e-03 -8.4906e-02\n",
      "  1.3982e-01 -6.2111e-01  1.7669e-01 -3.2753e-01  9.6525e-02 -3.3279e-01\n",
      "  2.4736e-02 -1.0611e-01  9.4917e-02 -3.9043e-01 -3.3990e-01 -3.0088e-01\n",
      " -1.1428e-01 -5.6823e-02  5.3313e-01 -6.3613e-02  4.3446e-01  1.7208e-01\n",
      "  4.1389e-03 -9.7232e-02 -2.8755e-01  3.6951e-01 -4.4410e-01 -7.1549e-01\n",
      " -1.0220e-01  1.1183e-01  1.3770e-01  3.3517e-01  7.6214e-01 -7.9057e-02\n",
      "  1.6167e-01 -1.2268e-01 -2.7257e-01  7.7642e-02 -7.0241e-01 -7.3880e-01\n",
      " -2.0641e-01 -4.1729e-01 -2.7843e-01  4.9860e-01 -6.5709e-01  2.6043e-02\n",
      "  5.9173e-02 -1.7001e-01  4.0966e-01  3.4607e-01  5.9666e-01  5.8379e-01\n",
      " -7.2867e-01  3.6411e-01 -4.8127e-01 -1.5866e-01 -3.7411e-02 -4.2054e-02\n",
      " -3.2052e-01 -8.6177e-02  1.2500e-02  8.7525e-02  1.0441e-01 -4.4027e-01\n",
      " -6.8288e-01 -1.1401e-01  1.5559e-02 -4.5564e-01  4.2021e-01  1.5455e-01\n",
      " -1.3655e-01 -5.4086e-01 -4.6889e-01  1.3512e-01 -6.9909e-02 -1.3501e-01\n",
      "  3.1214e-01 -2.8541e-01  1.2070e-01  7.7749e-01 -5.2590e-01  2.2565e-01\n",
      " -2.7404e-01  1.3236e-01  4.2170e-02 -1.7310e-02 -1.1971e-02  5.2605e-01\n",
      " -2.8189e-01  5.2890e-01 -2.4039e-01  1.2001e-01 -3.1343e-01 -7.2700e-01\n",
      "  4.0258e-01  3.6994e-01  3.6950e-01  1.7014e-01 -4.7925e-01  3.4136e-01\n",
      "  5.2517e-03  3.3088e-01  2.4373e-01  7.6566e-01  3.7584e-01 -7.2268e-02\n",
      "  3.0606e-01 -5.2114e-01 -1.5466e-01  6.0057e-01  6.9077e-01 -2.2702e-01\n",
      " -5.1078e-01  3.9906e-01  2.6872e-01  1.1266e-01  1.8253e-01 -5.2116e-01\n",
      " -2.4463e-01  1.9130e-01 -7.9633e-02 -9.6244e-01  2.8478e-01  2.7701e-01]\n",
      "Comparing sentences: 0.8382381200790405\n",
      "Comparing 'pizza' and 'paste': 1.0000001192092896\n"
     ]
    }
   ],
   "source": [
    "# Load a larger pipeline with vectors\n",
    "nlp = spacy.load(\"en_core_web_md\")\n",
    "\n",
    "# Look at one word vector\n",
    "doc = nlp(\"I love chocolate\")\n",
    "print(doc[2].vector)\n",
    "\n",
    "# Compare two documents\n",
    "doc1 = nlp(\"I like fast food\")\n",
    "doc2 = nlp(\"I like pizza\")\n",
    "print(\"Comparing sentences:\", doc1.similarity(doc2))\n",
    "\n",
    "# Compare two tokens\n",
    "doc = nlp(\"I like pizza and pasta\")\n",
    "token1 = doc[2]\n",
    "token2 = doc[4]\n",
    "print(\"Comparing 'pizza' and 'paste':\", token1.similarity(token2))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "753aa616",
   "metadata": {},
   "source": [
    "# Useful Tips\n",
    "\n",
    "**Fast Processing** - If you need to process a lot of texts and create a lot of `Doc` objects in a row, the `nlp.pipe` method can speed this up significantly. It processes the texts as a stream and yields Doc objects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "e5e7a568",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First example!\n",
      "Second example.\n"
     ]
    }
   ],
   "source": [
    "LOTS_OF_TEXTS = [\"First example!\", \"Second example.\"]\n",
    "\n",
    "for doc in nlp.pipe(LOTS_OF_TEXTS):\n",
    "    print(doc)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "5f089daf",
   "metadata": {},
   "source": [
    "**Context** - If you want to keep some information associated with each document, you can specify a context and it will be passed through the nlp pipe command. You can then write code to save it as a new property of the document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b25f052",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is a text 1\n",
      "And another text 2\n"
     ]
    }
   ],
   "source": [
    "data = [\n",
    "    (\"This is a text\", {\"id\": 1, \"page_number\": 15}),\n",
    "    (\"And another text\", {\"id\": 2, \"page_number\": 16}),\n",
    "]\n",
    "\n",
    "# Define new class members for the document\n",
    "# 给 spaCy 的 Doc 类型添加两个自定义属性：id 和 page_number。\n",
    "try:\n",
    "    Doc.set_extension(\"id\", default=None)\n",
    "    Doc.set_extension(\"page_number\", default=None)\n",
    "except ValueError:\n",
    "    print(\"Catching an error that happens if you run this cell of the notebook twice.\")\n",
    "    \n",
    "# Put values in the class as we iterate over the documents\n",
    "for doc, context in nlp.pipe(data, as_tuples=True):\n",
    "    doc._.id = context[\"id\"]\n",
    "    doc._.page_number = context[\"page_number\"]\n",
    "    print(doc.text, doc._.id)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "30d641f3",
   "metadata": {},
   "source": [
    "**Skip Pipeline Steps** - Sometimes you don't need all of the spaCy components. Skip some "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "1d447b2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tags with normal pipeline: ['DT', 'VBZ', 'DT', 'JJ', 'NN', '.']\n",
      "Tags with just make_doc: ['', '', '', '', '', '']\n",
      "(American, College Park, Georgia)\n",
      "Tags without tagger: ['', '', '', '', '', '', '', '', '', '', '', '', '', '', '_SP', '', '', '', '', '', '', '', '', '', '', '', '', '']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jkum0593/teaching/comp5046-2025/env-comp5046/lib/python3.12/site-packages/spacy/pipeline/lemmatizer.py:211: UserWarning: [W108] The rule-based lemmatizer did not find POS annotation for one or more tokens. Check that your pipeline includes components that assign token.pos, typically 'tagger'+'attribute_ruler' or 'morphologizer'.\n",
      "  warnings.warn(Warnings.W108)\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(\"This is a fun course.\")\n",
    "print(\"Tags with normal pipeline:\", [t.tag_ for t in doc])\n",
    "\n",
    "# Create a doc, but don't run any pipeline steps\n",
    "doc = nlp.make_doc(\"This is a fun course.\")\n",
    "print(\"Tags with just make_doc:\", [t.tag_ for t in doc])\n",
    "\n",
    "text = \"\"\"Chick-fil-A is an American fast food restaurant chain headquartered in \n",
    "the city of College Park, Georgia, specializing in chicken sandwiches.\"\"\"\n",
    "\n",
    "# Disable the tagger and lemmatizer\n",
    "with nlp.select_pipes(disable=[\"tagger\", \"parser\"]):\n",
    "    # Process the text\n",
    "    doc = nlp(text)\n",
    "    # Print the entities in the doc\n",
    "    print(doc.ents)\n",
    "    print(\"Tags without tagger:\", [t.tag_ for t in doc])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
