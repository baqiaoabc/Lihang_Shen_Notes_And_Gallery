{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e600cb0c",
   "metadata": {},
   "source": [
    "# Step0: Set up Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "e1df3c4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.2 -> 25.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting en-core-web-lg==3.8.0\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_lg-3.8.0/en_core_web_lg-3.8.0-py3-none-any.whl (400.7 MB)\n",
      "     ---------------------------------------- 0.0/400.7 MB ? eta -:--:--\n",
      "      -------------------------------------- 5.8/400.7 MB 32.0 MB/s eta 0:00:13\n",
      "     - ------------------------------------ 13.4/400.7 MB 33.5 MB/s eta 0:00:12\n",
      "     - ------------------------------------ 20.7/400.7 MB 34.4 MB/s eta 0:00:12\n",
      "     -- ----------------------------------- 28.3/400.7 MB 34.5 MB/s eta 0:00:11\n",
      "     --- ---------------------------------- 34.9/400.7 MB 33.6 MB/s eta 0:00:11\n",
      "     ---- --------------------------------- 42.2/400.7 MB 34.0 MB/s eta 0:00:11\n",
      "     ---- --------------------------------- 49.8/400.7 MB 34.1 MB/s eta 0:00:11\n",
      "     ----- -------------------------------- 57.1/400.7 MB 34.4 MB/s eta 0:00:11\n",
      "     ------ ------------------------------- 64.7/400.7 MB 34.4 MB/s eta 0:00:10\n",
      "     ------ ------------------------------- 72.1/400.7 MB 34.3 MB/s eta 0:00:10\n",
      "     ------- ------------------------------ 79.4/400.7 MB 34.5 MB/s eta 0:00:10\n",
      "     -------- ----------------------------- 87.0/400.7 MB 34.5 MB/s eta 0:00:10\n",
      "     -------- ----------------------------- 94.6/400.7 MB 34.5 MB/s eta 0:00:09\n",
      "     --------- --------------------------- 102.0/400.7 MB 34.6 MB/s eta 0:00:09\n",
      "     ---------- -------------------------- 109.3/400.7 MB 34.7 MB/s eta 0:00:09\n",
      "     ---------- -------------------------- 116.9/400.7 MB 34.7 MB/s eta 0:00:09\n",
      "     ----------- ------------------------- 124.5/400.7 MB 34.7 MB/s eta 0:00:08\n",
      "     ------------ ------------------------ 131.9/400.7 MB 34.7 MB/s eta 0:00:08\n",
      "     ------------ ------------------------ 139.5/400.7 MB 34.8 MB/s eta 0:00:08\n",
      "     ------------- ----------------------- 146.8/400.7 MB 34.7 MB/s eta 0:00:08\n",
      "     -------------- ---------------------- 154.1/400.7 MB 34.8 MB/s eta 0:00:08\n",
      "     -------------- ---------------------- 161.7/400.7 MB 34.8 MB/s eta 0:00:07\n",
      "     --------------- --------------------- 169.3/400.7 MB 34.8 MB/s eta 0:00:07\n",
      "     ---------------- -------------------- 176.7/400.7 MB 34.9 MB/s eta 0:00:07\n",
      "     ----------------- ------------------- 184.3/400.7 MB 34.8 MB/s eta 0:00:07\n",
      "     ----------------- ------------------- 191.6/400.7 MB 34.8 MB/s eta 0:00:07\n",
      "     ------------------ ------------------ 199.2/400.7 MB 34.9 MB/s eta 0:00:06\n",
      "     ------------------- ----------------- 206.0/400.7 MB 34.7 MB/s eta 0:00:06\n",
      "     ------------------- ----------------- 213.4/400.7 MB 34.7 MB/s eta 0:00:06\n",
      "     -------------------- ---------------- 221.0/400.7 MB 34.7 MB/s eta 0:00:06\n",
      "     --------------------- --------------- 228.6/400.7 MB 34.8 MB/s eta 0:00:05\n",
      "     --------------------- --------------- 235.9/400.7 MB 34.8 MB/s eta 0:00:05\n",
      "     ---------------------- -------------- 243.3/400.7 MB 34.8 MB/s eta 0:00:05\n",
      "     ----------------------- ------------- 251.1/400.7 MB 34.8 MB/s eta 0:00:05\n",
      "     ----------------------- ------------- 258.5/400.7 MB 34.8 MB/s eta 0:00:05\n",
      "     ------------------------ ------------ 265.8/400.7 MB 34.9 MB/s eta 0:00:04\n",
      "     ------------------------- ----------- 273.4/400.7 MB 34.9 MB/s eta 0:00:04\n",
      "     ------------------------- ----------- 280.8/400.7 MB 34.8 MB/s eta 0:00:04\n",
      "     -------------------------- ---------- 288.1/400.7 MB 34.8 MB/s eta 0:00:04\n",
      "     --------------------------- --------- 295.7/400.7 MB 35.0 MB/s eta 0:00:03\n",
      "     ---------------------------- -------- 303.3/400.7 MB 35.0 MB/s eta 0:00:03\n",
      "     ---------------------------- -------- 310.6/400.7 MB 35.0 MB/s eta 0:00:03\n",
      "     ----------------------------- ------- 317.2/400.7 MB 34.8 MB/s eta 0:00:03\n",
      "     ----------------------------- ------- 324.8/400.7 MB 34.9 MB/s eta 0:00:03\n",
      "     ------------------------------ ------ 332.1/400.7 MB 34.9 MB/s eta 0:00:02\n",
      "     ------------------------------- ----- 339.7/400.7 MB 34.8 MB/s eta 0:00:02\n",
      "     -------------------------------- ---- 347.1/400.7 MB 34.8 MB/s eta 0:00:02\n",
      "     -------------------------------- ---- 354.7/400.7 MB 34.8 MB/s eta 0:00:02\n",
      "     --------------------------------- --- 362.3/400.7 MB 34.8 MB/s eta 0:00:02\n",
      "     ---------------------------------- -- 369.6/400.7 MB 34.9 MB/s eta 0:00:01\n",
      "     ---------------------------------- -- 377.2/400.7 MB 34.8 MB/s eta 0:00:01\n",
      "     ----------------------------------- - 384.6/400.7 MB 34.9 MB/s eta 0:00:01\n",
      "     ------------------------------------  392.2/400.7 MB 34.9 MB/s eta 0:00:01\n",
      "     ------------------------------------  399.5/400.7 MB 34.8 MB/s eta 0:00:01\n",
      "     ------------------------------------  400.6/400.7 MB 34.8 MB/s eta 0:00:01\n",
      "     ------------------------------------  400.6/400.7 MB 34.8 MB/s eta 0:00:01\n",
      "     ------------------------------------- 400.7/400.7 MB 32.9 MB/s eta 0:00:00\n",
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('en_core_web_lg')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.2 -> 25.0.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install spacy --quiet\n",
    "!python -m spacy download en_core_web_lg\n",
    "\n",
    "import json\n",
    "import re\n",
    "import unicodedata\n",
    "import spacy\n",
    "from spacy.language import Language\n",
    "from spacy.tokenizer import Tokenizer\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "import re\n",
    "import spacy\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler\n",
    "\n",
    "from __future__ import unicode_literals, print_function, division\n",
    "from io import open\n",
    "import unicodedata\n",
    "import re\n",
    "import random\n",
    "\n",
    "import time\n",
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from torch import optim\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "52745ece",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n"
     ]
    }
   ],
   "source": [
    "device = 'cpu'\n",
    "if torch.cuda.is_available():\n",
    "    device = \"cuda\"\n",
    "print(f\"Using {device} device\")\n",
    "\n",
    "def asMinutes(s):\n",
    "    \"\"\"\n",
    "    Converts seconds into a minutes and seconds format.\n",
    "    \n",
    "    Parameters:\n",
    "    - s: The time in seconds.\n",
    "    \n",
    "    Returns:\n",
    "    - A string representing the time in minutes and seconds ('Xd Xm').\n",
    "    \"\"\"\n",
    "    m = math.floor(s / 60)  # Convert seconds to minutes, discarding any remainder.\n",
    "    s -= m * 60  # Calculate the remaining seconds.\n",
    "    return '%dm %ds' % (m, s)  # Format and return the string.\n",
    "\n",
    "def timeSince(since, percent):\n",
    "    \"\"\"\n",
    "    Calculates and formats the time elapsed since a starting point and estimates remaining time.\n",
    "    \n",
    "    Parameters:\n",
    "    - since: The starting time (usually obtained via time.time()).\n",
    "    - percent: The completion percentage of the task.\n",
    "    \n",
    "    Returns:\n",
    "    - A string indicating both the elapsed time and the estimated remaining time.\n",
    "    \"\"\"\n",
    "    now = time.time()  # Get the current time.\n",
    "    s = now - since  # Calculate elapsed time since the start.\n",
    "    es = s / (percent)  # Estimate the total time based on the current progress.\n",
    "    rs = es - s  # Calculate the remaining time by subtracting elapsed time from the total estimated time.\n",
    "    \n",
    "    # Format and return the elapsed and remaining times as a string.\n",
    "    return '%s (- %s)' % (asMinutes(s), asMinutes(rs))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38a5d237",
   "metadata": {},
   "source": [
    "# Step1: Methods Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d013398",
   "metadata": {},
   "source": [
    "For this task, I need to implement LSTM for Encoder and Decoder. \n",
    "\n",
    "1. Data pre-processing\n",
    "    - Read data from a JSON file and split it into query and question datasets, each with train, dev, and test splits.\n",
    "\n",
    "2. Create a Spacy tokenizer to process the input and output sentences.\n",
    "    - maybe need to consider about vocab\n",
    "\n",
    "3. Data Loader:\n",
    "    - Use the tokenizer to tokenize each input sentence and its corresponding label, adding the following special tokens: `<sos>`, `<eos>`, and `<pad>`.\n",
    "    - Pad the tokenized sentences so that each batch has the same length.\n",
    "\n",
    "4. Define the LSTM Encoder and Decoder:\n",
    "    - Implement an Encoder and a Decoder using LSTM.\n",
    "    - Combine the Encoder and Decoder into a sequence-to-sequence (seq2seq) model.\n",
    "    - Optionally, use teacher forcing during training to enhance convergence.\n",
    "    - Optionally, use bidirectional or more layers.\n",
    "\n",
    "5. Define the training method:\n",
    "    - Perform the feedforward step.\n",
    "    - Calculate the loss between the predicted output and the target labels.\n",
    "    - Perform backpropagation to compute the gradients.\n",
    "    - Apply gradient clipping to prevent exploding gradients.\n",
    "    - Record and print the loss in the terminal during training.\n",
    "\n",
    "6. Define the testing method to evaluate the model's performance on the test dataset.\n",
    "\n",
    "7. Set the hyperparameters for the seq2seq model, such as:\n",
    "    - Learning rate\n",
    "    - Batch size\n",
    "    - Number of epochs\n",
    "    - Hidden dimensions\n",
    "\n",
    "8. Train the model using both the question and query training datasets.\n",
    "\n",
    "9. Test the model using both the question and query testing datasets.\n",
    "    - Remember to ignore `<sos>`, `<eos>`, and `<pad>`.\n",
    "    - Remember not only shortest sql query is valid."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23628e40",
   "metadata": {},
   "source": [
    "# Step 2: Pre-process Raw Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "196ad310",
   "metadata": {},
   "outputs": [],
   "source": [
    "def unicodeToAscii(s):\n",
    "    # Convert a Unicode string 's' to plain ASCII.\n",
    "    # This is done by first normalizing the string into its decomposed form using 'NFD',\n",
    "    # which separates characters from their accents. Then, it filters out all nonspacing marks (Mn).\n",
    "    return ''.join(\n",
    "        c for c in unicodedata.normalize('NFD', s)\n",
    "        if unicodedata.category(c) != 'Mn'\n",
    "    )\n",
    "\n",
    "# Does not allow sql to have multiple space between each word\n",
    "def normalize_whitespace(text):\n",
    "    return re.sub(r'\\s+', ' ', text).strip()\n",
    "\n",
    "def preprocess_sentence(s:str) -> str:\n",
    "    \"\"\"\n",
    "    Preprocesses sentence text for consistency\n",
    "    \"\"\"\n",
    "    s = s.strip()\n",
    "    s = normalize_whitespace(s)\n",
    "    s = unicodeToAscii(s)\n",
    "    s = s.strip()\n",
    "    return s\n",
    "\n",
    "def preprocess_dataset(dataset_loc = \"atis.json\",split_type=None, split=['dev', 'test', 'train']):\n",
    "    \n",
    "    # Read Dataset JSON file\n",
    "    with open(dataset_loc) as f:\n",
    "        dataset_json = json.load(f)\n",
    "\n",
    "    processed_dataset = []\n",
    "    variable_names = set()\n",
    "    sql_templates = set()\n",
    "\n",
    "    for sample in dataset_json:\n",
    "        processed_sample = {}\n",
    "\n",
    "        # Preprocess sql queries\n",
    "        sql = [preprocess_sentence(query) for query in sample['sql']]\n",
    "\n",
    "        # All valid sql queries for this examples sorted by their length\n",
    "        sql = sorted(sql, key=len)\n",
    "\n",
    "        # Adds shorests sql template to the set of sql templates\n",
    "        sql_templates.add(sql[0])\n",
    "        \n",
    "\n",
    "        # Dictionary for variables/placeholders metadata\n",
    "        variables_metadata = sample[\"variables\"]\n",
    "\n",
    "        # Delete 'location' key from variables dictionary\n",
    "        # variable_type_mapping = {var['name']:var['type'] for var in variables_metadata}\n",
    "        for var in variables_metadata:\n",
    "            # Add current variable to set of all possible variable names\n",
    "            variable_names.add(var.get(\"name\"))\n",
    "            var.pop('location', None)\n",
    "        # query split for this sample\n",
    "        query_split = sample['query-split']\n",
    "\n",
    "        # Skips sample if its not the specified split_type or split\n",
    "        if(split_type == \"query\"):\n",
    "            if(query_split not in split):\n",
    "                continue\n",
    "\n",
    "        # Process each sentence\n",
    "        for sentence in sample['sentences']:\n",
    "            # Skips sample if its not the specified split_type or split\n",
    "            if(split_type == \"question\"):\n",
    "                if(sentence['question-split'] not in split):\n",
    "                    continue\n",
    "            # variables/placeholder mapping dictionary\n",
    "            variables = sentence['variables']\n",
    "\n",
    "            # Sentence text with variables/placeholders\n",
    "            text_with_vars = preprocess_sentence(sentence['text'])\n",
    "\n",
    "            # Replacing variables/placeholders in current sentence and sql query with their values from the variables dictionary\n",
    "            text_with_vars_replaced = text_with_vars\n",
    "            sql_with_vars_replaced = sql\n",
    "\n",
    "            # Replace sentence and all sql variables with their values\n",
    "            for var in variables:\n",
    "                text_with_vars_replaced = text_with_vars_replaced.replace(var,variables[var])\n",
    "                sql_with_vars_replaced = [query.replace(var,variables[var]) for query in sql_with_vars_replaced]\n",
    "\n",
    "            # Taggingg expected output\n",
    "            sentence_var_tagging_labels = []\n",
    "            for word in text_with_vars.split():\n",
    "                if(word in variables):\n",
    "                    sentence_var_tagging_labels.append(word)\n",
    "                else:\n",
    "                    sentence_var_tagging_labels.append(\"-\")\n",
    "\n",
    "            # Appends preprocessed dictionary of current sentence to the processesed_dataset list\n",
    "            processed_dataset.append({\n",
    "                \"text_with_vars\":text_with_vars,\n",
    "                \"text_with_vars_replaced\":text_with_vars_replaced,\n",
    "                \"sentence_var_tagging_labels\":sentence_var_tagging_labels,\n",
    "                \"vars_metadata\":variables_metadata,\n",
    "                \"variables\":variables,\n",
    "                \"sql_with_vars\": sql,\n",
    "                \"shortest_sql_with_vars\":sql[0],\n",
    "                \"sql_with_vars_replaced\": sql_with_vars_replaced,\n",
    "                \"shortest_sql_with_vars_replaced\":sql_with_vars_replaced[0],\n",
    "                \"query_split\":sample['query-split'],\n",
    "                \"question_split\":sentence['question-split']\n",
    "            })\n",
    "    \n",
    "    return processed_dataset,variable_names,sql_templates\n",
    "\n",
    "\n",
    "question_train_data, question_train_vars, question_train_sqls = preprocess_dataset(dataset_loc=\"atis.json\", split_type=\"question\", split=[\"train\"])\n",
    "question_test_data, question_test_vars, question_test_sqls = preprocess_dataset(dataset_loc=\"atis.json\", split_type=\"question\", split=[\"test\"])\n",
    "question_dev_data, question_dev_vars, question_dev_sqls = preprocess_dataset(dataset_loc=\"atis.json\", split_type=\"question\", split=[\"dev\"])\n",
    "\n",
    "\n",
    "query_train_data, query_train_vars, query_train_sqls = preprocess_dataset(dataset_loc=\"atis.json\", split_type=\"query\", split=[\"train\"])\n",
    "query_test_data, query_test_vars, query_test_sqls = preprocess_dataset(dataset_loc=\"atis.json\", split_type=\"query\", split=[\"test\"])\n",
    "query_dev_data, query_dev_vars, query_dev_sqls = preprocess_dataset(dataset_loc=\"atis.json\", split_type=\"query\", split=[\"dev\"])\n",
    "\n",
    "\n",
    "def unicodeToAscii(s):\n",
    "    # Convert  to plain ASCII by (1) normalizing the string into its decomposed form using 'NFD',\n",
    "    # which separates characters from their accents, and (2) filtering out all nonspacing marks (Mn).\n",
    "    return ''.join(\n",
    "        c for c in unicodedata.normalize('NFD', s)\n",
    "        if unicodedata.category(c) != 'Mn'\n",
    "    )\n",
    "\n",
    "# Lowercase, trim, and remove non-letter characters\n",
    "def normalizeString(s):\n",
    "    # First, convert the string to lowercase and strip leading and trailing whitespaces.\n",
    "    # This helps in reducing the variation between different uses of capitalization and spaces.\n",
    "    s = s.lower().strip()\n",
    "\n",
    "    # Convert the string from Unicode to ASCII, removing diacritics (e.g., accents) from characters.\n",
    "    # This is crucial for languages with accented characters, making the text processing uniform.\n",
    "    s = unicodeToAscii(s)\n",
    "\n",
    "    # Insert a space before any punctuation marks (.!?).\n",
    "    # This ensures punctuation is treated as a separate word, aiding in tokenization for NLP tasks.\n",
    "    # For example, \"hello!\" becomes \"hello !\".\n",
    "    s = re.sub(r\"([.!?])\", r\" \\1\", s)\n",
    "\n",
    "    # Replace any sequence of characters that are not letters or punctuation marks (.!?)\n",
    "    # with a single space. This step removes numbers and special characters,\n",
    "    # focusing on retaining only textual information that's crucial for most NLP tasks.\n",
    "    s = re.sub(r\"[^a-zA-Z!?]+\", r\" \", s)\n",
    "\n",
    "    # Finally, strip leading and trailing whitespaces that might have been added\n",
    "    # during the normalization process, ensuring the output is tidy.\n",
    "    return s.strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "280e7b2c",
   "metadata": {},
   "source": [
    "# Step 3: Create tokenizer, vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60185748",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading lines...\n",
      "Read 4347 sentence pairs\n",
      "Trimmed to 4345 sentence pairs\n",
      "Counting words...\n",
      "Counted words:\n",
      "natural 671\n",
      "sql 742\n",
      "['how much is a first class ticket from boston to san francisco', 'SELECT DISTINCT FAREalias0.FARE_ID FROM AIRPORT_SERVICE AS AIRPORT_SERVICEalias0 , AIRPORT_SERVICE AS AIRPORT_SERVICEalias1 , CITY AS CITYalias0 , CITY AS CITYalias1 , FARE AS FAREalias0 , FARE_BASIS AS FARE_BASISalias0 WHERE ( CITYalias0.CITY_CODE = AIRPORT_SERVICEalias0.CITY_CODE AND CITYalias0.CITY_NAME = \"BOSTON\" AND CITYalias1.CITY_CODE = AIRPORT_SERVICEalias1.CITY_CODE AND CITYalias1.CITY_NAME = \"SAN FRANCISCO\" AND FAREalias0.FROM_AIRPORT = AIRPORT_SERVICEalias0.AIRPORT_CODE AND FAREalias0.TO_AIRPORT = AIRPORT_SERVICEalias1.AIRPORT_CODE ) AND FARE_BASISalias0.CLASS_TYPE = \"FIRST\" AND FAREalias0.FARE_BASIS_CODE = FARE_BASISalias0.FARE_BASIS_CODE ;']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#====================================================================================\n",
    "SOS_token = 0\n",
    "EOS_token = 1\n",
    "PAD_token = 2\n",
    "UNK_token = 3\n",
    "\n",
    "# Defines a class 'Lang' to manage language-specific data.\n",
    "class Lang:\n",
    "    # The class constructor that initializes a new instance of the language data handler.\n",
    "    def __init__(self, name):\n",
    "        self.name = name  # The name of the language (e.g., 'natural', 'sql').\n",
    "\n",
    "        self.word2index = {}  # A dictionary to map words to their numeric index.\n",
    "        self.index2word = {0: \"SOS\", 1: \"EOS\", 2: \"PAD\", 3: \"UNK\"}  # A dictionary to map numeric indices back to words, pre-filled with special tokens.\n",
    "\n",
    "        self.word2count = {}  # A dictionary to count occurrences of each word.\n",
    "        self.n_words = 4  # The total number of unique words in the vocabulary, starting with 2 to account for the special tokens.\n",
    "\n",
    "    # Adds a sentence to the language model, incrementing the vocabulary and word counts.\n",
    "    def addSentence(self, sentence):\n",
    "        for word in sentence.split(' '):  # Splits the sentence into words and processes each word.\n",
    "            self.addWord(word)\n",
    "\n",
    "    # Adds a word to the language model, updating the necessary mappings and counts.\n",
    "    def addWord(self, word):\n",
    "        if word not in self.word2index.keys():\n",
    "            # If the word is new, it is added to all relevant dictionaries and counters.\n",
    "            self.word2index[word] = self.n_words  # Maps the word to the current count of unique words.\n",
    "            self.word2count[word] = 1  # Initializes the word's count to 1.\n",
    "            self.index2word[self.n_words] = word  # Maps the current count of unique words back to the word.\n",
    "            self.n_words += 1  # Increments the total count of unique words.\n",
    "        else:\n",
    "            # If the word already exists, just increments its count.\n",
    "            self.word2count[word] += 1\n",
    "\n",
    "\n",
    "def readLangs(data):\n",
    "    print(\"Reading lines...\")\n",
    "    # data can be train, test, dev\n",
    "\n",
    "    # [[input1, sql1],[input2, sql2],...]\n",
    "    pairs = []\n",
    "    for sample in data:\n",
    "        # 获取 text_with_vars_replaced 和 shortest_sql_with_vars_replaced\n",
    "        text_with_vars_replaced = sample['text_with_vars_replaced']\n",
    "        shortest_sql_with_vars_replaced = sample['shortest_sql_with_vars_replaced']\n",
    "        \n",
    "        # 将它们组合成一个元组并添加到 pairs 列表\n",
    "        pairs.append([normalizeString(text_with_vars_replaced), shortest_sql_with_vars_replaced])\n",
    "\n",
    "    natural_lang = Lang('natural')\n",
    "    sql_lang = Lang('sql')\n",
    "\n",
    "    # Return the 'Lang' objects for input and output languages, and the list of sentence pairs.\n",
    "    return natural_lang, sql_lang, pairs\n",
    "\n",
    "def filterPair(p):\n",
    "    # Determine if a given pair of sentences ('p') should be kept based on length and prefix criteria.\n",
    "    \n",
    "    # Check if the first sentence in the pair is longer than the MAX_LENGTH.\n",
    "    if len(p[0].split(' ')) >= MAX_LENGTH:\n",
    "        return False  # Exclude the pair if the first sentence is too long.\n",
    "    \n",
    "    # Check if the second sentence in the pair is longer than the MAX_LENGTH.\n",
    "    elif len(p[1].split(' ')) >= MAX_LENGTH:\n",
    "        return False  # Exclude the pair if the second sentence is too long.\n",
    "    \n",
    "    return True\n",
    "\n",
    "def filterPairs(pairs):\n",
    "    # Filter a list of sentence pairs using the filterPair criteria.\n",
    "    \n",
    "    keep = []  # Initialize an empty list to store pairs that meet the filtering criteria.\n",
    "    for pair in pairs:\n",
    "        # For each pair in the input list, check if it should be kept.\n",
    "        if filterPair(pair):\n",
    "            keep.append(pair)  # Add the pair to the 'keep' list if it passes the filter.\n",
    "    return keep  # Return the list of pairs that meet the filtering criteria.\n",
    "\n",
    "MAX_LENGTH = 250\n",
    "\n",
    "def prepareData(data):\n",
    "    input_lang, output_lang, pairs = readLangs(data)\n",
    "    print(\"Read %s sentence pairs\" % len(pairs))\n",
    "\n",
    "    pairs = filterPairs(pairs)\n",
    "    print(\"Trimmed to %s sentence pairs\" % len(pairs))\n",
    "\n",
    "    print(\"Counting words...\")\n",
    "    # Process each sentence pair, adding the words from each sentence to their respective\n",
    "    # language's vocabulary.\n",
    "    for pair in pairs:\n",
    "        input_lang.addSentence(pair[0])\n",
    "        output_lang.addSentence(pair[1])\n",
    "\n",
    "    print(\"Counted words:\")\n",
    "    print(input_lang.name, input_lang.n_words)\n",
    "    print(output_lang.name, output_lang.n_words)\n",
    "    return input_lang, output_lang, pairs\n",
    "\n",
    "# Example usage of the prepareData function.\n",
    "# Prepares the data for English to French translation (can be reversed).\n",
    "input_lang_test, output_lang_test, pairs_test = prepareData(question_train_data)\n",
    "# Print a random sentence pair from the prepared data to demonstrate the outcome.\n",
    "print(random.choice(pairs_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "33adb008",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 'SOS', 1: 'EOS', 2: 'PAD', 3: 'UNK', 4: 'SELECT', 5: 'DISTINCT', 6: 'FLIGHTalias0.FLIGHT_ID', 7: 'FROM', 8: 'AIRPORT', 9: 'AS', 10: 'AIRPORTalias0', 11: ',', 12: 'FLIGHT', 13: 'FLIGHTalias0', 14: 'WHERE', 15: 'AIRPORTalias0.AIRPORT_CODE', 16: '=', 17: '\"MKE\"', 18: 'AND', 19: 'FLIGHTalias0.TO_AIRPORT', 20: ';', 21: '\"DAL\"', 22: 'AIRPORT_SERVICE', 23: 'AIRPORT_SERVICEalias0', 24: 'AIRPORT_SERVICEalias1', 25: 'CITY', 26: 'CITYalias0', 27: 'CITYalias1', 28: 'DATE_DAY', 29: 'DATE_DAYalias0', 30: 'DAYS', 31: 'DAYSalias0', 32: '(', 33: 'CITYalias1.CITY_CODE', 34: 'AIRPORT_SERVICEalias1.CITY_CODE', 35: 'CITYalias1.CITY_NAME', 36: '\"BOSTON\"', 37: 'DATE_DAYalias0.DAY_NUMBER', 38: '9', 39: 'DATE_DAYalias0.MONTH_NUMBER', 40: '8', 41: 'DATE_DAYalias0.YEAR', 42: '1991', 43: 'DAYSalias0.DAY_NAME', 44: 'DATE_DAYalias0.DAY_NAME', 45: 'FLIGHTalias0.FLIGHT_DAYS', 46: 'DAYSalias0.DAYS_CODE', 47: 'AIRPORT_SERVICEalias1.AIRPORT_CODE', 48: ')', 49: 'CITYalias0.CITY_CODE', 50: 'AIRPORT_SERVICEalias0.CITY_CODE', 51: 'CITYalias0.CITY_NAME', 52: '\"DENVER\"', 53: 'FLIGHTalias0.FROM_AIRPORT', 54: 'AIRPORT_SERVICEalias0.AIRPORT_CODE', 55: '\"ORLANDO\"', 56: '26', 57: '7', 58: '\"TACOMA\"', 59: '\"PITTSBURGH\"', 60: '\"DETROIT\"', 61: '23', 62: '4', 63: '\"PHOENIX\"', 64: '\"SAN', 65: 'FRANCISCO\"', 66: '27', 67: '\"LAS', 68: 'VEGAS\"', 69: '\"NEW', 70: 'YORK\"', 71: '\"BALTIMORE\"', 72: '21', 73: '2', 74: '\"ATLANTA\"', 75: '24', 76: '5', 77: '\"PHILADELPHIA\"', 78: '\"MILWAUKEE\"', 79: '\"WASHINGTON\"', 80: '15', 81: '10', 82: '22', 83: '3', 84: 'JOSE\"', 85: '20', 86: '1', 87: '\"DALLAS\"', 88: '\"CHARLOTTE\"', 89: '\"TAMPA\"', 90: '\"BURBANK\"', 91: '28', 92: '6', 93: '\"MIAMI\"', 94: '19', 95: '11', 96: '30', 97: '31', 98: '\"OAKLAND\"', 99: '25', 100: '\"ST.', 101: 'PETERSBURG\"', 102: '\"SALT', 103: 'LAKE', 104: 'CITY\"', 105: '\"NEWARK\"', 106: '29', 107: '\"HOUSTON\"', 108: '16', 109: '\"SEATTLE\"', 110: '\"MINNEAPOLIS\"', 111: '\"MONTREAL\"', 112: '1992', 113: 'DIEGO\"', 114: '\"INDIANAPOLIS\"', 115: '12', 116: '\"COLUMBUS\"', 117: '\"LONG', 118: 'BEACH\"', 119: '\"MEMPHIS\"', 120: '\"TORONTO\"', 121: '17', 122: '\"CHICAGO\"', 123: '\"NASHVILLE\"', 124: '\"LOS', 125: 'ANGELES\"', 126: '\"KANSAS', 127: '\"CLEVELAND\"', 128: '18', 129: '\"DALLAS', 130: 'FORT', 131: 'WORTH\"', 132: 'FAREalias0.FARE_ID', 133: 'FARE', 134: 'FAREalias0', 135: 'FLIGHT_FARE', 136: 'FLIGHT_FAREalias0', 137: 'FAREalias0.ONE_DIRECTION_COST', 138: 'MAX(', 139: 'FAREalias1.ONE_DIRECTION_COST', 140: 'AIRPORT_SERVICEalias2', 141: 'AIRPORT_SERVICEalias3', 142: 'CITYalias2', 143: 'CITYalias3', 144: 'FAREalias1', 145: 'FLIGHTalias1', 146: 'FLIGHT_FAREalias1', 147: 'CITYalias2.CITY_CODE', 148: 'AIRPORT_SERVICEalias2.CITY_CODE', 149: 'CITYalias2.CITY_NAME', 150: 'CITYalias3.CITY_CODE', 151: 'AIRPORT_SERVICEalias3.CITY_CODE', 152: 'CITYalias3.CITY_NAME', 153: 'FLIGHTalias1.FROM_AIRPORT', 154: 'AIRPORT_SERVICEalias2.AIRPORT_CODE', 155: 'FLIGHTalias1.TO_AIRPORT', 156: 'AIRPORT_SERVICEalias3.AIRPORT_CODE', 157: 'FAREalias1.ROUND_TRIP_REQUIRED', 158: '\"NO\"', 159: 'FLIGHT_FAREalias1.FARE_ID', 160: 'FAREalias1.FARE_ID', 161: 'FLIGHTalias1.AIRLINE_CODE', 162: '\"AA\"', 163: 'FLIGHTalias1.FLIGHT_ID', 164: 'FLIGHT_FAREalias1.FLIGHT_ID', 165: 'FLIGHT_FAREalias0.FARE_ID', 166: 'FLIGHTalias0.AIRLINE_CODE', 167: 'FLIGHT_FAREalias0.FLIGHT_ID', 168: '\"SATURDAY\"', 169: '\"WEDNESDAY\"', 170: '\"MONDAY\"', 171: '\"SUNDAY\"', 172: '\"TUESDAY\"', 173: '\"THURSDAY\"', 174: '\"ONTARIO\"', 175: 'LOUIS\"', 176: '\"FRIDAY\"', 177: '\"WESTCHESTER', 178: 'COUNTY\"', 179: '\"CINCINNATI\"', 180: '\"FORT', 181: 'PAUL\"', 182: 'FLIGHT_STOP', 183: 'FLIGHT_STOPalias0', 184: 'FLIGHT_STOPalias0.STOP_AIRPORT', 185: 'FLIGHT_STOPalias0.FLIGHT_ID', 186: 'OR', 187: 'FLIGHTalias0.ARRIVAL_TIME', 188: '<', 189: '1200', 190: 'FLIGHTalias0.DEPARTURE_TIME', 191: '819', 192: '1600', 193: '645', 194: '1700', 195: '400', 196: '1300', 197: 'FAREalias0.ROUND_TRIP_COST', 198: 'IS', 199: 'NOT', 200: 'NULL', 201: 'AIRLINEalias0.AIRLINE_CODE', 202: 'AIRLINE', 203: 'AIRLINEalias0', 204: '\"DL\"', 205: '\"YX\"', 206: '\"CO\"', 207: '\"US\"', 208: '\"EA\"', 209: '\"UA\"', 210: '\"TW\"', 211: '\"LH\"', 212: '\"WN\"', 213: '\"FF\"', 214: '\"NW\"', 215: '\"AS\"', 216: '\"AC\"', 217: 'DATE_DAYalias1', 218: 'DAYSalias1', 219: 'DAYSalias1.DAYS_CODE', 220: 'DAYSalias1.DAY_NAME', 221: 'DATE_DAYalias1.DAY_NAME', 222: 'DATE_DAYalias1.YEAR', 223: 'DATE_DAYalias1.MONTH_NUMBER', 224: 'DATE_DAYalias1.DAY_NUMBER', 225: 'FLIGHTalias0.STOPS', 226: '0', 227: 'MIN(', 228: 'FLIGHTalias1.ARRIVAL_TIME', 229: 'FLIGHTalias2.DEPARTURE_TIME', 230: 'AIRPORT_SERVICEalias4', 231: 'AIRPORT_SERVICEalias5', 232: 'CITYalias4', 233: 'CITYalias5', 234: 'FLIGHTalias2', 235: 'CITYalias5.CITY_CODE', 236: 'AIRPORT_SERVICEalias5.CITY_CODE', 237: 'CITYalias5.CITY_NAME', 238: 'FLIGHTalias2.TO_AIRPORT', 239: 'AIRPORT_SERVICEalias5.AIRPORT_CODE', 240: 'FLIGHTalias2.ARRIVAL_TIME', 241: 'CITYalias4.CITY_CODE', 242: 'AIRPORT_SERVICEalias4.CITY_CODE', 243: 'CITYalias4.CITY_NAME', 244: 'FLIGHTalias2.FROM_AIRPORT', 245: 'AIRPORT_SERVICEalias4.AIRPORT_CODE', 246: 'AIRCRAFT', 247: 'AIRCRAFTalias0', 248: 'EQUIPMENT_SEQUENCE', 249: 'EQUIPMENT_SEQUENCEalias0', 250: 'AIRCRAFTalias0.AIRCRAFT_CODE', 251: '\"J31\"', 252: 'EQUIPMENT_SEQUENCEalias0.AIRCRAFT_CODE', 253: 'FLIGHTalias0.AIRCRAFT_CODE_SEQUENCE', 254: 'EQUIPMENT_SEQUENCEalias0.AIRCRAFT_CODE_SEQUENCE', 255: 'FOOD_SERVICEalias0.MEAL_DESCRIPTION', 256: 'FOOD_SERVICE', 257: 'FOOD_SERVICEalias0', 258: 'FOOD_SERVICEalias0.MEAL_CODE', 259: '\"D/S\"', 260: '\"S/\"', 261: '\"S\"', 262: '\"SD/D\"', 263: '\"LS\"', 264: 'STATE', 265: 'STATEalias0', 266: '13', 267: 'STATEalias0.STATE_CODE', 268: 'CITYalias0.STATE_CODE', 269: 'STATEalias0.STATE_NAME', 270: '\"NORTH', 271: 'CAROLINA\"', 272: '\"state_name0\"', 273: 'FARE_BASIS', 274: 'FARE_BASISalias0', 275: 'FARE_BASISalias0.CLASS_TYPE', 276: '\"FIRST\"', 277: 'FAREalias0.FARE_BASIS_CODE', 278: 'FARE_BASISalias0.FARE_BASIS_CODE', 279: '\"COACH\"', 280: '\"BUSINESS\"', 281: 'FLIGHT_STOPalias1', 282: 'FLIGHT_STOPalias1.STOP_AIRPORT', 283: 'FLIGHT_STOPalias1.FLIGHT_ID', 284: 'AIRPORTalias1', 285: 'AIRPORTalias1.AIRPORT_CODE', 286: 'GROUND_SERVICEalias0.TRANSPORT_TYPE', 287: 'GROUND_SERVICE', 288: 'GROUND_SERVICEalias0', 289: '\"PIT\"', 290: 'GROUND_SERVICEalias0.AIRPORT_CODE', 291: '\"LIMOUSINE\"', 292: 'BETWEEN', 293: '1800', 294: '2200', 295: '800', 296: 'FAREalias0.FROM_AIRPORT', 297: 'FAREalias0.TO_AIRPORT', 298: '\"THRIFT\"', 299: 'GROUND_SERVICEalias0.CITY_CODE', 300: '1145', 301: '\"DC\"', 302: 'CITYalias1.STATE_CODE', 303: 'FARE_BASISalias0.BASIS_DAYS', 304: '230', 305: '650', 306: '1505', 307: '630', 308: '1310', 309: '2100', 310: '1524', 311: '<=', 312: '1900', 313: 'FLIGHTalias0.FLIGHT_NUMBER', 314: 'FAREalias0.ROUND_TRIP_REQUIRED', 315: 'COUNT(', 316: 'FLIGHTalias1.DEPARTURE_TIME', 317: '\"GEORGIA\"', 318: '\"CALIFORNIA\"', 319: 'JERSEY\"', 320: '\"FLORIDA\"', 321: 'FARE_BASISalias0.ECONOMY', 322: '\"YES\"', 323: 'FARE_BASISalias0.BOOKING_CLASS', 324: '\"B\"', 325: 'FAREalias1.ROUND_TRIP_COST', 326: '1100', 327: '>', 328: '100', 329: '1000', 330: '2359', 331: '>=', 332: '600', 333: 'FLIGHTalias0.MEAL_CODE', 334: '\"DINNER\"', 335: 'FLIGHT_STOPalias0.ARRIVAL_TIME', 336: 'FARE_BASISalias1', 337: 'FARE_BASISalias1.CLASS_TYPE', 338: 'FAREalias1.FARE_BASIS_CODE', 339: 'FARE_BASISalias1.FARE_BASIS_CODE', 340: 'DAYSalias2', 341: 'DAYSalias3', 342: 'DAYSalias4', 343: 'DAYSalias3.DAY_NAME', 344: 'DAYSalias4.DAY_NAME', 345: 'DAYSalias3.DAYS_CODE', 346: 'DAYSalias4.DAYS_CODE', 347: 'DAYSalias2.DAY_NAME', 348: 'DAYSalias2.DAYS_CODE', 349: '2000', 350: '700', 351: 'AIRCRAFTalias0.BASIC_TYPE', 352: '\"737\"', 353: 'AIRCRAFTalias0.MANUFACTURER', 354: '\"BOEING\"', 355: '\"767\"', 356: '300', 357: '\"BOS\"', 358: '\"DFW\"', 359: '\"MCO\"', 360: '\"ATL\"', 361: '\"BWI\"', 362: '\"PHL\"', 363: '\"SFO\"', 364: '2230', 365: '2130', 366: '\"LGA\"', 367: 'AIRPORT_SERVICEalias0.MILES_DISTANT', 368: '\"YYZ\"', 369: '\"SLC\"', 370: '\"LAX\"', 371: 'AIRPORTalias0.MINIMUM_CONNECT_TIME', 372: '\"IAH\"', 373: 'FAREalias1.FROM_AIRPORT', 374: 'FAREalias1.TO_AIRPORT', 375: '\"CA\"', 376: 'EQUIPMENT_SEQUENCEalias1', 377: 'AIRCRAFTalias0.CAPACITY', 378: 'AIRCRAFTalias1.CAPACITY', 379: 'AIRCRAFTalias1', 380: 'EQUIPMENT_SEQUENCEalias2', 381: 'EQUIPMENT_SEQUENCEalias2.AIRCRAFT_CODE', 382: 'AIRCRAFTalias1.AIRCRAFT_CODE', 383: 'FLIGHTalias2.AIRCRAFT_CODE_SEQUENCE', 384: 'EQUIPMENT_SEQUENCEalias2.AIRCRAFT_CODE_SEQUENCE', 385: 'EQUIPMENT_SEQUENCEalias1.AIRCRAFT_CODE', 386: 'FLIGHTalias1.AIRCRAFT_CODE_SEQUENCE', 387: 'EQUIPMENT_SEQUENCEalias1.AIRCRAFT_CODE_SEQUENCE', 388: '\"BREAKFAST\"', 389: '\"LUNCH\"', 390: '\"SA\"', 391: '500', 392: '1400', 393: '1230', 394: '1500', 395: '\"ML\"', 396: 'FOOD_SERVICEalias1', 397: 'FOOD_SERVICEalias1.MEAL_CODE', 398: 'FLIGHTalias1.MEAL_CODE', 399: 'FOOD_SERVICEalias1.MEAL_DESCRIPTION', 400: '2400', 401: '1045', 402: 'FLIGHTalias0.CONNECTIONS', 403: '41', 404: 'FLIGHTalias0.TIME_ELAPSED', 405: '60', 406: '1830', 407: '1730', 408: '1930', 409: '1130', 410: '1630', 411: '900', 412: '\"QX\"', 413: 'FLIGHTalias1.FLIGHT_DAYS', 414: 'RESTRICTIONalias0.ADVANCE_PURCHASE', 415: 'RESTRICTIONalias0.APPLICATION', 416: 'RESTRICTIONalias0.MAXIMUM_STAY', 417: 'RESTRICTIONalias0.MINIMUM_STAY', 418: 'RESTRICTIONalias0.NO_DISCOUNTS', 419: 'RESTRICTIONalias0.RESTRICTION_CODE', 420: 'RESTRICTIONalias0.SATURDAY_STAY_REQUIRED', 421: 'RESTRICTIONalias0.STOPOVERS', 422: 'RESTRICTION', 423: 'RESTRICTIONalias0', 424: '\"AP/57\"', 425: '\"AP/80\"', 426: '\"AP/68\"', 427: '\"AP/55\"', 428: '1330', 429: '601', 430: '1759', 431: 'DATE_DAYalias2', 432: 'DATE_DAYalias3', 433: 'DATE_DAYalias2.DAY_NUMBER', 434: 'DATE_DAYalias2.MONTH_NUMBER', 435: 'DATE_DAYalias2.YEAR', 436: 'DATE_DAYalias3.DAY_NUMBER', 437: 'DATE_DAYalias3.MONTH_NUMBER', 438: 'DATE_DAYalias3.YEAR', 439: 'DATE_DAYalias2.DAY_NAME', 440: 'DATE_DAYalias3.DAY_NAME', 441: 'FARE_BASISalias1.BASIS_DAYS', 442: '830', 443: '730', 444: '14', 445: '3724', 446: '771', 447: '71', 448: '212', 449: '345', 450: '3357', 451: '106', 452: '296', 453: '281', 454: '402', 455: '271', 456: '\"EWR\"', 457: '\"BNA\"', 458: '\"HOU\"', 459: '\"MIA\"', 460: '\"BUR\"', 461: '\"DAILY\"', 462: '1845', 463: '1745', 464: '1530', 465: '1430', 466: '1159', 467: 'AIRLINEalias0.AIRLINE_NAME', 468: 'LIKE', 469: '\"CANADIAN', 470: 'AIRLINES\"', 471: '\"QO\"', 472: '\"F\"', 473: '\"Y\"', 474: '\"M\"', 475: '\"FN\"', 476: '\"QW\"', 477: '\"C\"', 478: '\"YN\"', 479: '\"COLORADO\"', 480: '*', 481: 'GROUP', 482: 'BY', 483: '2226', 484: '845', 485: '1923', 486: '2300', 487: '718', 488: '200', 489: '1024', 490: '823', 491: '1205', 492: '2220', 493: 'FLIGHTalias2.AIRLINE_CODE', 494: '1291', 495: 'GROUND_SERVICEalias0.GROUND_FARE', 496: '\"OAK\"', 497: 'STATEalias1', 498: 'STATEalias1.STATE_CODE', 499: 'STATEalias1.STATE_NAME', 500: '\"OHIO\"', 501: '\"MISSOURI\"', 502: '\"UTAH\"', 503: '\"TENNESSEE\"', 504: '1401', 505: 'FARE_BASISalias2', 506: 'FARE_BASISalias2.CLASS_TYPE', 507: 'FARE_BASISalias2.FARE_BASIS_CODE', 508: '1715', 509: '1645', 510: 'CITYalias2.STATE_CODE', 511: 'FLIGHT_LEG', 512: 'FLIGHT_LEGalias0', 513: 'FLIGHT_LEGalias0.LEG_FLIGHT', 514: 'FLIGHT_LEGalias0.FLIGHT_ID', 515: '\"CP\"', 516: '\"NX\"', 517: '\"HP\"', 518: 'AIRPORTalias2', 519: 'AIRPORTalias3', 520: 'AIRPORTalias2.AIRPORT_CODE', 521: 'AIRPORTalias3.AIRPORT_CODE', 522: 'FLIGHTalias1.CONNECTIONS', 523: '1940', 524: 'connections0', 525: 'IN', 526: '\"RENTAL', 527: 'CAR\"', 528: '\"TAXI\"', 529: '\"QUEBEC\"', 530: '\"MINNESOTA\"', 531: 'DAYSalias5', 532: 'DAYSalias6', 533: 'DAYSalias7', 534: 'DAYSalias8', 535: 'DAYSalias9', 536: 'FARE_BASISalias3', 537: 'FARE_BASISalias4', 538: 'FARE_BASISalias5', 539: 'DAYSalias8.DAY_NAME', 540: 'DAYSalias9.DAY_NAME', 541: 'DAYSalias8.DAYS_CODE', 542: 'DAYSalias9.DAYS_CODE', 543: 'DAYSalias7.DAY_NAME', 544: 'DAYSalias7.DAYS_CODE', 545: 'DAYSalias6.DAY_NAME', 546: 'DAYSalias6.DAYS_CODE', 547: 'DAYSalias5.DAY_NAME', 548: 'DAYSalias5.DAYS_CODE', 549: 'FARE_BASISalias5.BASIS_DAYS', 550: 'FARE_BASISalias5.FARE_BASIS_CODE', 551: 'FARE_BASISalias4.BASIS_DAYS', 552: 'FARE_BASISalias4.FARE_BASIS_CODE', 553: 'FARE_BASISalias3.BASIS_DAYS', 554: 'FARE_BASISalias3.FARE_BASIS_CODE', 555: 'FARE_BASISalias2.BASIS_DAYS', 556: 'FLIGHTalias1.TIME_ELAPSED', 557: 'CLASS_OF_SERVICEalias0.BOOKING_CLASS', 558: 'CLASS_OF_SERVICE', 559: 'CLASS_OF_SERVICEalias0', 560: '\"H\"', 561: '\"Q\"', 562: '555', 563: '825', 564: '720', 565: '1220', 566: 'DATE_DAYalias4', 567: 'DATE_DAYalias4.DAY_NUMBER', 568: 'DATE_DAYalias4.MONTH_NUMBER', 569: 'DATE_DAYalias4.YEAR', 570: 'DATE_DAYalias4.DAY_NAME', 571: '\"RAPID', 572: 'TRANSIT\"', 573: 'AIRPORT_SERVICEalias0.MINUTES_DISTANT', 574: '1288', 575: '466', 576: 'CITYalias1.COUNTRY_NAME', 577: '\"CANADA\"', 578: 'FLIGHTalias1.FLIGHT_NUMBER', 579: '139', 580: '82', 581: 'FLIGHTalias3', 582: 'FLIGHT_LEGalias1', 583: 'FLIGHTalias3.FLIGHT_NUMBER', 584: 'FLIGHTalias2.FLIGHT_ID', 585: 'FLIGHT_LEGalias1.FLIGHT_ID', 586: 'FLIGHTalias3.AIRLINE_CODE', 587: 'FLIGHTalias3.FLIGHT_ID', 588: 'FLIGHT_LEGalias1.LEG_FLIGHT', 589: 'FAREalias0.FARE_AIRLINE', 590: 'FAREalias0.RESTRICTION_CODE', 591: '\"747\"', 592: '1030', 593: 'FLIGHTalias1.STOPS', 594: '1628', 595: '269', 596: '2358', 597: '1425', 598: '2134', 599: '838', 600: 'FLIGHT_FAREalias2', 601: 'FLIGHTalias2.FLIGHT_DAYS', 602: 'FLIGHT_FAREalias2.FARE_ID', 603: 'FLIGHT_FAREalias2.FLIGHT_ID', 604: '\"SNACK\"', 605: '\"USAIR\"', 606: '\"DELTA\"', 607: '815', 608: '\"MICHIGAN\"', 609: '1615', 610: '1083', 611: '\"DC10\"', 612: '\"727\"', 613: '\"UNITED\"', 614: '1039', 615: '\"JFK\"', 616: 'FAREalias2.ONE_DIRECTION_COST', 617: 'FAREalias2', 618: 'FAREalias3', 619: 'FLIGHT_FAREalias3', 620: 'FAREalias3.FARE_BASIS_CODE', 621: 'FAREalias2.FARE_ID', 622: 'FLIGHT_FAREalias3.FARE_ID', 623: 'FAREalias3.FARE_ID', 624: 'FLIGHT_FAREalias3.FLIGHT_ID', 625: '2153', 626: '\"ORD\"', 627: '\"D9S\"', 628: '\"M80\"', 629: '\"D10\"', 630: '\"73S\"', 631: '\"100\"', 632: '\"733\"', 633: '\"72S\"', 634: 'AIRCRAFTalias0.PROPULSION', 635: '\"TURBOPROP\"', 636: '2030', 637: 'AIRPORTalias0.AIRPORT_NAME', 638: '\"STAPLETON', 639: 'INTERNATIONAL\"', 640: '932', 641: '98', 642: 'AIRPORTalias0.AIRPORT_LOCATION', 643: '\"LESTER', 644: 'PEARSON\"', 645: '\"GENERAL', 646: 'MITCHELL', 647: '329', 648: '\"ONT\"', 649: '\"CONTINENTAL', 650: '\"DEN\"', 651: '2159', 652: '301', 653: '1245', 654: '1115', 655: '\"757\"', 656: 'ALL', 657: 'AIRCRAFTalias2.CAPACITY', 658: 'AIRCRAFTalias2', 659: 'AIRCRAFTalias1.PROPULSION', 660: 'AIRCRAFTalias3.CAPACITY', 661: 'AIRCRAFTalias3', 662: 'AIRCRAFTalias3.PROPULSION', 663: '\"ARIZONA\"', 664: 'FARE_BASISalias1.ECONOMY', 665: 'FOOD_SERVICEalias0.COMPARTMENT', 666: 'FOOD_SERVICEalias0.MEAL_NUMBER', 667: '\"INDIANA\"', 668: '930', 669: '1017', 670: 'STATEalias2', 671: 'STATEalias3', 672: 'STATEalias2.STATE_CODE', 673: 'STATEalias2.STATE_NAME', 674: 'STATEalias3.STATE_CODE', 675: 'CITYalias3.STATE_CODE', 676: 'STATEalias3.STATE_NAME', 677: '852', 678: '459', 679: '270', 680: '315', 681: '1222', 682: 'CITYalias0.TIME_ZONE_CODE', 683: 'FARE_BASISalias1.DISCOUNTED', 684: '539', 685: '201', 686: '1765', 687: '\"BB\"', 688: '\"GA\"', 689: 'AIRPORT_SERVICEalias1.MILES_DISTANT', 690: 'AIRPORT_SERVICEalias2.MILES_DISTANT', 691: 'AIRLINES', 692: '755', 693: '540', 694: 'AIRCRAFTalias1.BASIC_TYPE', 695: '1410', 696: 'CLASS_OF_SERVICEalias0.CLASS_DESCRIPTION', 697: 'CLASS_OF_SERVICEalias0.RANK', 698: '\"IAD\"', 699: '\"economy0\"', 700: '\"fare_basis_code0\"', 701: '1850', 702: '416', 703: '311', 704: '746', 705: '2010', 706: '2330', 707: '4400', 708: '705', 709: '813', 710: '\"NEVADA\"', 711: '1620', 712: '\"JET\"', 713: '343', 714: '\"AIR', 715: 'TAXI', 716: 'OPERATION\"', 717: '530', 718: '430', 719: '324', 720: '257', 721: '210', 722: '\"F28\"', 723: 'AIRPORTalias0.COUNTRY_NAME', 724: 'AIRPORTalias0.STATE_CODE', 725: 'AIRPORTalias0.TIME_ZONE_CODE', 726: 'FLIGHT_STOPalias0.STOP_NUMBER', 727: '229', 728: '323', 729: '137338', 730: '279', 731: '\"TX\"', 732: '\"DALLAS/FORT', 733: 'WORTH', 734: '352', 735: '\"STAPLETON\"', 736: 'AIRPORTalias1.AIRPORT_NAME', 737: '150', 738: '1110', 739: '505', 740: 'FLIGHTalias2.FLIGHT_NUMBER', 741: '163'}\n"
     ]
    }
   ],
   "source": [
    "print(output_lang_test.index2word)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6734f62",
   "metadata": {},
   "source": [
    "# Step 4: Build LSTM Encoder and Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "ea544af6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderRNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, dropout_p=0.1):\n",
    "        super(EncoderRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size  # Hidden size for LSTM.\n",
    "\n",
    "        # Embedding layer to convert token indices to dense vectors.\n",
    "        self.embedding = nn.Embedding(input_size, hidden_size)\n",
    "        \n",
    "        # LSTM layer instead of GRU, this is the main change.\n",
    "        self.lstm = nn.LSTM(hidden_size, hidden_size, batch_first=True)\n",
    "        \n",
    "        # Dropout layer for regularization.\n",
    "        self.dropout = nn.Dropout(dropout_p)\n",
    "\n",
    "    def forward(self, input):\n",
    "        # Embedding the input tokens.\n",
    "        embedded = self.embedding(input)\n",
    "        \n",
    "        # Dropout for regularization.\n",
    "        embedded = self.dropout(embedded)\n",
    "        \n",
    "        # Passing through LSTM instead of GRU.\n",
    "        output, (hidden, cell) = self.lstm(embedded)\n",
    "\n",
    "        # Return LSTM output, hidden, and cell states.\n",
    "        return output, (hidden, cell)\n",
    "    \n",
    "class AdditiveAttention(nn.Module):\n",
    "    def __init__(self, hidden_size):\n",
    "        super(AdditiveAttention, self).__init__()\n",
    "        self.Wa = nn.Linear(hidden_size, hidden_size)\n",
    "        self.Ua = nn.Linear(hidden_size, hidden_size)\n",
    "        self.Va = nn.Linear(hidden_size, 1)\n",
    "        self.out_size = hidden_size * 2  # Combined context and decoder states.\n",
    "\n",
    "    def forward(self, query, keys):\n",
    "        scores = self.Va(torch.tanh(self.Wa(query) + self.Ua(keys)))\n",
    "        scores = scores.squeeze(2).unsqueeze(1)\n",
    "\n",
    "        weights = F.softmax(scores, dim=-1)\n",
    "        context = torch.bmm(weights, keys)\n",
    "        return context, weights\n",
    "\n",
    "class NoAttention(nn.Module):\n",
    "    def __init__(self, hidden_size):\n",
    "        super(NoAttention, self).__init__()\n",
    "        self.out_size = hidden_size\n",
    "\n",
    "    def forward(self, query, keys):\n",
    "        context = torch.zeros([query.shape[0], query.shape[1], 0]).to(device)\n",
    "        weights = torch.zeros(keys.shape).to(device)\n",
    "        return context, weights\n",
    "\n",
    "class DotProductAttention(nn.Module):\n",
    "    def __init__(self, hidden_size):\n",
    "        super(DotProductAttention, self).__init__()\n",
    "        self.out_size = hidden_size *2\n",
    "    \n",
    "    def forward(self, query, keys):\n",
    "        scores = (query * keys).sum(-1)\n",
    "        scores = scores.unsqueeze(1)\n",
    "\n",
    "        weights = F.softmax(scores, dim=-1)\n",
    "        context = torch.bmm(weights, keys)\n",
    "        return context, weights\n",
    "\n",
    "def get_attention_module(name, hidden_size):\n",
    "    if name == 'none':\n",
    "        return NoAttention(hidden_size)\n",
    "    elif name == \"additive\":\n",
    "        return AdditiveAttention(hidden_size)\n",
    "    elif name == \"dot-product\":\n",
    "        return DotProductAttention(hidden_size)\n",
    "    else:\n",
    "        raise Exception(f\"Attention type {name} is not defined\")\n",
    "\n",
    "class AttnDecoderRNN(nn.Module):\n",
    "    def __init__(self, hidden_size, output_size, attention_type=\"none\", dropout_p=0.1):\n",
    "        super(AttnDecoderRNN, self).__init__()\n",
    "        self.embedding = nn.Embedding(output_size, hidden_size)\n",
    "        self.attention = get_attention_module(attention_type, hidden_size)\n",
    "        # Replacing GRU with LSTM here.\n",
    "        self.lstm = nn.LSTM(self.attention.out_size, hidden_size, batch_first=True)\n",
    "        self.out = nn.Linear(hidden_size, output_size)\n",
    "        self.dropout = nn.Dropout(dropout_p)\n",
    "\n",
    "    def forward(self, encoder_outputs, encoder_hidden, target_tensor=None):\n",
    "        batch_size = encoder_outputs.size(0)\n",
    "        decoder_input = torch.empty(batch_size, 1, dtype=torch.long, device=device).fill_(SOS_token)\n",
    "        decoder_hidden = encoder_hidden  # LSTM hidden state (hidden, cell)\n",
    "\n",
    "        decoder_outputs = []\n",
    "        attentions = []\n",
    "\n",
    "        for i in range(MAX_LENGTH):\n",
    "            decoder_output, decoder_hidden, attn_weights = self.forward_step(\n",
    "                decoder_input, decoder_hidden, encoder_outputs\n",
    "            )\n",
    "            decoder_outputs.append(decoder_output)\n",
    "            attentions.append(attn_weights)\n",
    "\n",
    "            if target_tensor is not None:\n",
    "                decoder_input = target_tensor[:, i].unsqueeze(1)  # Teacher forcing\n",
    "            else:\n",
    "                _, topi = decoder_output.topk(1)\n",
    "                decoder_input = topi.squeeze(-1).detach()  # Inference mode\n",
    "\n",
    "        decoder_outputs = torch.cat(decoder_outputs, dim=1)\n",
    "        decoder_outputs = F.log_softmax(decoder_outputs, dim=-1)\n",
    "        attentions = torch.cat(attentions, dim=1)\n",
    "\n",
    "        return decoder_outputs, decoder_hidden, attentions\n",
    "\n",
    "    def forward_step(self, input, hidden, encoder_outputs):\n",
    "        embedded = self.dropout(self.embedding(input))\n",
    "\n",
    "        query = hidden[0].permute(1, 0, 2)  # LSTM hidden state (hidden, cell), need to use hidden[0]\n",
    "        context, attn_weights = self.attention(query, encoder_outputs)\n",
    "        input_lstm = torch.cat((embedded, context), dim=2)\n",
    "\n",
    "        output, hidden = self.lstm(input_lstm, hidden)  # LSTM updates\n",
    "        output = self.out(output)\n",
    "\n",
    "        return output, hidden, attn_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4145242e",
   "metadata": {},
   "source": [
    "# Step: Training Prepare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "b86ca9da",
   "metadata": {},
   "outputs": [],
   "source": [
    "def indexesFromSentence(lang, sentence):\n",
    "    # Converts a sentence into a list of word indices according to a given language's vocabulary.\n",
    "    return [lang.word2index[word] for word in sentence.split(' ')]\n",
    "\n",
    "\n",
    "def get_dataloader(batch_size, data):\n",
    "    input_lang, output_lang, pairs = prepareData(data)\n",
    "\n",
    "    n = len(pairs)\n",
    "    input_ids = np.zeros((n, MAX_LENGTH), dtype=np.int32)  # Initializes a numpy array for input sentence indices.\n",
    "    target_ids = np.zeros((n, MAX_LENGTH), dtype=np.int32)  # Initializes a numpy array for target sentence indices.\n",
    "\n",
    "    for idx, (inp, tgt) in enumerate(pairs):\n",
    "        inp_ids = indexesFromSentence(input_lang, inp) + [EOS_token]  # Gets input indices, appends EOS token.\n",
    "        tgt_ids = indexesFromSentence(output_lang, tgt) + [EOS_token]  # Gets target indices, appends EOS token.\n",
    "        # Fills the respective numpy arrays with indices.\n",
    "        input_ids[idx, :len(inp_ids)] = inp_ids\n",
    "        target_ids[idx, :len(tgt_ids)] = tgt_ids\n",
    "\n",
    "    # Converts the numpy arrays to PyTorch tensors and moves them to the specified device.\n",
    "    train_data = TensorDataset(torch.LongTensor(input_ids).to(device),\n",
    "                               torch.LongTensor(target_ids).to(device))\n",
    "    # Creates a DataLoader with random sampling for batch generation.\n",
    "    train_sampler = RandomSampler(train_data)\n",
    "    train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n",
    "\n",
    "    return input_lang, output_lang, train_dataloader, pairs\n",
    "\n",
    "\n",
    "\n",
    "def train_epoch(dataloader, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion):\n",
    "    \"\"\"\n",
    "    Trains the model for one epoch using the given dataloader, encoder, decoder, and optimizers.\n",
    "\n",
    "    Parameters:\n",
    "    - dataloader: DataLoader providing batches of input and target tensors.\n",
    "    - encoder: The encoder model which processes the input tensors.\n",
    "    - decoder: The decoder model which generates the output sequence.\n",
    "    - encoder_optimizer: Optimizer for updating the encoder's weights.\n",
    "    - decoder_optimizer: Optimizer for updating the decoder's weights.\n",
    "    - criterion: Loss function to calculate the difference between\n",
    "                 the decoder's outputs and the target tensors.\n",
    "\n",
    "    Returns:\n",
    "    - The average loss over all batches in this epoch.\n",
    "    \"\"\"\n",
    "    total_loss = 0  # Initialize total loss for this epoch.\n",
    "\n",
    "    # Iterate over batches of data in the dataloader.\n",
    "    for data in dataloader:\n",
    "        input_tensor, target_tensor = data  # Unpack the batch into input and target tensors.\n",
    "\n",
    "        # Clear gradients before processing the batch.\n",
    "        encoder_optimizer.zero_grad()\n",
    "        decoder_optimizer.zero_grad()\n",
    "\n",
    "        # Pass the input tensor through the encoder.\n",
    "        encoder_outputs, encoder_hidden = encoder(input_tensor)\n",
    "\n",
    "        # Pass the encoder's outputs and hidden state to the decoder, along with the target tensor.\n",
    "        decoder_outputs, _, _ = decoder(encoder_outputs, encoder_hidden, target_tensor)\n",
    "\n",
    "        # Compute the loss between the decoder's output and the actual target tensor.\n",
    "        # The .view(-1, decoder_outputs.size(-1)) reshapes the decoder's output\n",
    "        # to a 2D tensor where rows correspond to batch elements concatenated together,\n",
    "        # and columns correspond to the output size. The target is similarly flattened.\n",
    "        loss = criterion(decoder_outputs.view(-1, decoder_outputs.size(-1)), target_tensor.view(-1))\n",
    "\n",
    "        loss.backward()  # Compute the gradient of the loss with respect to model parameters.\n",
    "\n",
    "        # Update the encoder and decoder parameters based on gradients.\n",
    "        encoder_optimizer.step()\n",
    "        decoder_optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()  # Accumulate the loss.\n",
    "\n",
    "    # Calculate the average loss per batch for this epoch.\n",
    "    return total_loss / len(dataloader)\n",
    "\n",
    "\n",
    "def train(train_dataloader, encoder, decoder, n_epochs, learning_rate=0.001, print_every=1):\n",
    "    \"\"\"\n",
    "    Trains an encoder-decoder model.\n",
    "\n",
    "    Parameters:\n",
    "    - train_dataloader: DataLoader providing batches of data for training.\n",
    "    - encoder: The encoder part of the sequence-to-sequence model.\n",
    "    - decoder: The decoder part of the sequence-to-sequence model.\n",
    "    - n_epochs: Total number of epochs to train the models.\n",
    "    - learning_rate: Learning rate for the optimizers.\n",
    "    - print_every: Frequency of reporting the average loss.\n",
    "    \"\"\"\n",
    "    start = time.time()  # Record the start time for calculating elapsed time.\n",
    "    print_loss_total = 0  # Sum of losses, reset every 'print_every' epochs.\n",
    "\n",
    "    # Initialize optimizers for both encoder and decoder with the Adam algorithm.\n",
    "    encoder_optimizer = optim.Adam(encoder.parameters(), lr=learning_rate)\n",
    "    decoder_optimizer = optim.Adam(decoder.parameters(), lr=learning_rate)\n",
    "\n",
    "    # Define the loss function. NLLLoss is common for classification problems.\n",
    "    criterion = nn.NLLLoss()\n",
    "\n",
    "    # Training loop over the specified number of epochs.\n",
    "    for epoch in range(1, n_epochs + 1):\n",
    "        # Perform one epoch of training and return the loss.\n",
    "        loss = train_epoch(train_dataloader, encoder, decoder,\n",
    "                           encoder_optimizer, decoder_optimizer, criterion)\n",
    "        print_loss_total += loss  # Accumulate loss.\n",
    "\n",
    "        # Every 'print_every' epochs, print the average loss and reset the total loss.\n",
    "        if epoch % print_every == 0:\n",
    "            print_loss_avg = print_loss_total / print_every  # Calculate average loss.\n",
    "            print_loss_total = 0  # Reset total loss for the next 'print_every' epochs.\n",
    "            # Print a summary: elapsed time, current epoch, progress (%), and average loss.\n",
    "            print('%s (%d %d%%) %.4f' % (timeSince(start, epoch / n_epochs),\n",
    "                                         epoch, epoch / n_epochs * 100, print_loss_avg))\n",
    "            \n",
    "\n",
    "def tensorFromSentence(lang, sentence):\n",
    "    # Converts a sentence into a PyTorch tensor of word indices, appending the EOS (End of Sentence) token.\n",
    "    indexes = indexesFromSentence(lang, sentence)\n",
    "    indexes.append(EOS_token)  # Appends the EOS token's index to signify the end of the sentence.\n",
    "    # Converts the list of indices into a PyTorch tensor and returns it.\n",
    "    return torch.tensor(indexes, dtype=torch.long, device=DEVICE).view(1, -1)\n",
    "\n",
    "\n",
    "def evaluate(encoder, decoder, sentence, input_lang, output_lang):\n",
    "    # Temporarily disables gradient calculations to save memory and computations since they are not needed.\n",
    "    with torch.no_grad():\n",
    "        # Convert the input sentence into a tensor of word indices.\n",
    "        input_tensor = tensorFromSentence(input_lang, sentence)\n",
    "\n",
    "        # Pass the input tensor through the encoder to obtain its outputs and final hidden state.\n",
    "        encoder_outputs, encoder_hidden = encoder(input_tensor)\n",
    "\n",
    "        # Pass the encoder outputs and hidden state into the decoder to produce the output sequence.\n",
    "        decoder_outputs, decoder_hidden, decoder_attn = decoder(encoder_outputs, encoder_hidden)\n",
    "\n",
    "        # Select the top prediction (highest probability) from the decoder's output at each time step.\n",
    "        _, topi = decoder_outputs.topk(1)\n",
    "        decoded_ids = topi.squeeze()  # Remove extraneous dimensions.\n",
    "\n",
    "        decoded_words = []  # To store the decoded words.\n",
    "        for idx in decoded_ids:\n",
    "            # Check for the EOS token. If found, append '<EOS>' to the decoded words and stop decoding.\n",
    "            if idx.item() == EOS_token:\n",
    "                decoded_words.append('<EOS>')\n",
    "                break\n",
    "            # Convert each index back to a word and append to the list of decoded words.\n",
    "            decoded_words.append(output_lang.index2word[idx.item()])\n",
    "\n",
    "    # Return the list of decoded words and any attention weights from the decoder.\n",
    "    return decoded_words, decoder_attn\n",
    "\n",
    "def evaluateRandomly(encoder, decoder, input_lang, output_lang, pairs, n=5):\n",
    "    # Sets the encoder and decoder to evaluation mode, which turns off dropout and batch normalization,\n",
    "    # ensuring consistent behavior for inference.\n",
    "    encoder.eval()\n",
    "    decoder.eval()\n",
    "\n",
    "    # Loop over n examples chosen randomly.\n",
    "    for i in range(n):\n",
    "        # Randomly select a sentence pair from the global 'pairs' list.\n",
    "        pair = random.choice(pairs)\n",
    "        \n",
    "        # Print the input sentence from the pair.\n",
    "        print('>', pair[0])\n",
    "        # Print the target (correct) translation or response.\n",
    "        print('=', pair[1])\n",
    "        \n",
    "        # Use the 'evaluate' function to generate the output sentence for the input sentence.\n",
    "        output_words, _ = evaluate(encoder, decoder, pair[0], input_lang, output_lang)\n",
    "        # Join the list of output words into a single sentence.\n",
    "        output_sentence = ' '.join(output_words)\n",
    "        \n",
    "        # Print the model's translation or response.\n",
    "        print('<', output_sentence)\n",
    "        print('')  # Print a newline for readability between each evaluated pair.\n",
    "\n",
    "def evaluateAll(encoder, decoder, input_lang, output_lang, pairs):\n",
    "    # Sets the encoder and decoder to evaluation mode, which turns off dropout and batch normalization,\n",
    "    # ensuring consistent behavior for inference.\n",
    "    encoder.eval()\n",
    "    decoder.eval()\n",
    "    \n",
    "    correct = 0\n",
    "\n",
    "    # Loop over n examples chosen randomly.\n",
    "    for pair in pairs:\n",
    "        # Randomly select a sentence pair from the global 'pairs' list.\n",
    "        # Use the 'evaluate' function to generate the output sentence for the input sentence.\n",
    "        output_words, _ = evaluate(encoder, decoder, pair[0], input_lang, output_lang)\n",
    "        # Join the list of output words into a single sentence.\n",
    "        if output_words[-1] == \"<EOS>\":\n",
    "            output_sentence = ' '.join(output_words[:-1])\n",
    "        else:\n",
    "            output_sentence = ' '.join(output_words)\n",
    "        if output_sentence == pair[1]:\n",
    "            correct += 1\n",
    "    \n",
    "    return correct/len(pairs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0f7470b",
   "metadata": {},
   "source": [
    "# Begin Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a94e5ddd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading lines...\n",
      "Read 4347 sentence pairs\n",
      "Trimmed to 4345 sentence pairs\n",
      "Counting words...\n",
      "Counted words:\n",
      "natural 671\n",
      "sql 742\n"
     ]
    }
   ],
   "source": [
    "# Set the size of the hidden layers in the encoder and decoder models.\n",
    "hidden_size = 128\n",
    "# Specify the batch size for training, determining how many examples are processed together.\n",
    "batch_size = 32\n",
    "\n",
    "input_lang, output_lang, train_dataloader, train_pairs = get_dataloader(batch_size, question_train_data)\n",
    "\n",
    "encoder = EncoderRNN(input_lang.n_words, hidden_size).to(device)\n",
    "decoder = AttnDecoderRNN(hidden_size, output_lang.n_words, \"none\").to(device)\n",
    "\n",
    "train(train_dataloader, encoder, decoder, 10, print_every=1)\n",
    "\n",
    "evaluateRandomly(encoder, decoder, input_lang, output_lang, train_pairs)\n",
    "evaluateAll(encoder, decoder, input_lang, output_lang, train_pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a263712",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1m 5s (- 9m 48s) (1 10%) 1.2584\n",
      "2m 10s (- 8m 42s) (2 20%) 0.3852\n",
      "3m 15s (- 7m 37s) (3 30%) 0.2436\n",
      "4m 21s (- 6m 32s) (4 40%) 0.1853\n",
      "5m 26s (- 5m 26s) (5 50%) 0.1554\n",
      "6m 31s (- 4m 21s) (6 60%) 0.1371\n",
      "7m 36s (- 3m 15s) (7 70%) 0.1249\n",
      "8m 42s (- 2m 10s) (8 80%) 0.1151\n",
      "9m 47s (- 1m 5s) (9 90%) 0.1065\n",
      "10m 52s (- 0m 0s) (10 100%) 0.0994\n",
      "> what city is the airport MCO in\n",
      "= SELECT DISTINCT CITYalias0.CITY_CODE FROM AIRPORT AS AIRPORTalias0 , AIRPORT_SERVICE AS AIRPORT_SERVICEalias0 , CITY AS CITYalias0 WHERE AIRPORTalias0.AIRPORT_CODE = \"MCO\" AND AIRPORTalias0.AIRPORT_CODE = AIRPORT_SERVICEalias0.AIRPORT_CODE AND CITYalias0.CITY_CODE = AIRPORT_SERVICEalias0.CITY_CODE ;\n",
      "< SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS\n",
      "\n",
      "> does UA fly from DENVER to BALTIMORE\n",
      "= SELECT DISTINCT AIRLINEalias0.AIRLINE_CODE FROM AIRLINE AS AIRLINEalias0 , AIRPORT_SERVICE AS AIRPORT_SERVICEalias0 , AIRPORT_SERVICE AS AIRPORT_SERVICEalias1 , CITY AS CITYalias0 , CITY AS CITYalias1 , FLIGHT AS FLIGHTalias0 WHERE ( ( FLIGHTalias0.AIRLINE_CODE = \"UA\" ) AND CITYalias1.CITY_CODE = AIRPORT_SERVICEalias1.CITY_CODE AND CITYalias1.CITY_NAME = \"BALTIMORE\" AND FLIGHTalias0.TO_AIRPORT = AIRPORT_SERVICEalias1.AIRPORT_CODE ) AND CITYalias0.CITY_CODE = AIRPORT_SERVICEalias0.CITY_CODE AND CITYalias0.CITY_NAME = \"DENVER\" AND FLIGHTalias0.AIRLINE_CODE = AIRLINEalias0.AIRLINE_CODE AND FLIGHTalias0.FROM_AIRPORT = AIRPORT_SERVICEalias0.AIRPORT_CODE ;\n",
      "< SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS\n",
      "\n",
      "> please list all flights leaving on thursday morning from NEW YORK to TORONTO\n",
      "= SELECT DISTINCT FLIGHTalias0.FLIGHT_ID FROM AIRPORT_SERVICE AS AIRPORT_SERVICEalias0 , AIRPORT_SERVICE AS AIRPORT_SERVICEalias1 , CITY AS CITYalias0 , CITY AS CITYalias1 , DATE_DAY AS DATE_DAYalias0 , DAYS AS DAYSalias0 , FLIGHT AS FLIGHTalias0 WHERE ( ( CITYalias0.CITY_CODE = AIRPORT_SERVICEalias0.CITY_CODE AND CITYalias0.CITY_NAME = \"NEW YORK\" AND CITYalias1.CITY_CODE = AIRPORT_SERVICEalias1.CITY_CODE AND CITYalias1.CITY_NAME = \"TORONTO\" AND FLIGHTalias0.FROM_AIRPORT = AIRPORT_SERVICEalias0.AIRPORT_CODE AND FLIGHTalias0.TO_AIRPORT = AIRPORT_SERVICEalias1.AIRPORT_CODE ) AND DATE_DAYalias0.DAY_NUMBER = 24 AND DATE_DAYalias0.MONTH_NUMBER = 5 AND DATE_DAYalias0.YEAR = 1991 AND DAYSalias0.DAY_NAME = DATE_DAYalias0.DAY_NAME AND FLIGHTalias0.FLIGHT_DAYS = DAYSalias0.DAYS_CODE ) AND FLIGHTalias0.DEPARTURE_TIME BETWEEN 0 AND 1200 ;\n",
      "< SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS\n",
      "\n",
      "> what airlines fly from TORONTO to SAN DIEGO with a stopover in DENVER\n",
      "= SELECT DISTINCT AIRLINEalias0.AIRLINE_CODE FROM AIRLINE AS AIRLINEalias0 , AIRPORT_SERVICE AS AIRPORT_SERVICEalias0 , AIRPORT_SERVICE AS AIRPORT_SERVICEalias1 , AIRPORT_SERVICE AS AIRPORT_SERVICEalias2 , CITY AS CITYalias0 , CITY AS CITYalias1 , CITY AS CITYalias2 , FLIGHT AS FLIGHTalias0 , FLIGHT_STOP AS FLIGHT_STOPalias0 WHERE ( CITYalias1.CITY_CODE = AIRPORT_SERVICEalias1.CITY_CODE AND CITYalias1.CITY_NAME = \"SAN DIEGO\" AND CITYalias2.CITY_CODE = AIRPORT_SERVICEalias2.CITY_CODE AND CITYalias2.CITY_NAME = \"DENVER\" AND FLIGHT_STOPalias0.STOP_AIRPORT = AIRPORT_SERVICEalias2.AIRPORT_CODE AND FLIGHTalias0.FLIGHT_ID = FLIGHT_STOPalias0.FLIGHT_ID AND FLIGHTalias0.TO_AIRPORT = AIRPORT_SERVICEalias1.AIRPORT_CODE ) AND CITYalias0.CITY_CODE = AIRPORT_SERVICEalias0.CITY_CODE AND CITYalias0.CITY_NAME = \"TORONTO\" AND FLIGHTalias0.AIRLINE_CODE = AIRLINEalias0.AIRLINE_CODE AND FLIGHTalias0.FROM_AIRPORT = AIRPORT_SERVICEalias0.AIRPORT_CODE ;\n",
      "< SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS\n",
      "\n",
      "> show me all flights from ATLANTA to DALLAS round trip less than 1100 dollars\n",
      "= SELECT DISTINCT FLIGHTalias0.FLIGHT_ID FROM AIRPORT_SERVICE AS AIRPORT_SERVICEalias0 , AIRPORT_SERVICE AS AIRPORT_SERVICEalias1 , CITY AS CITYalias0 , CITY AS CITYalias1 , FARE AS FAREalias0 , FLIGHT AS FLIGHTalias0 , FLIGHT_FARE AS FLIGHT_FAREalias0 WHERE ( CITYalias1.CITY_CODE = AIRPORT_SERVICEalias1.CITY_CODE AND CITYalias1.CITY_NAME = \"DALLAS\" AND FAREalias0.ROUND_TRIP_COST < 1100 AND FLIGHT_FAREalias0.FARE_ID = FAREalias0.FARE_ID AND FLIGHTalias0.FLIGHT_ID = FLIGHT_FAREalias0.FLIGHT_ID AND FLIGHTalias0.TO_AIRPORT = AIRPORT_SERVICEalias1.AIRPORT_CODE ) AND CITYalias0.CITY_CODE = AIRPORT_SERVICEalias0.CITY_CODE AND CITYalias0.CITY_NAME = \"ATLANTA\" AND FLIGHTalias0.FROM_AIRPORT = AIRPORT_SERVICEalias0.AIRPORT_CODE ;\n",
      "< SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder2 = EncoderRNN(input_lang.n_words, hidden_size).to(device)\n",
    "decoder2 = AttnDecoderRNN(hidden_size, output_lang.n_words, \"additive\").to(device)\n",
    "\n",
    "train(train_dataloader, encoder2, decoder2, 10, print_every=1)\n",
    "\n",
    "evaluateRandomly(encoder2, decoder2, input_lang, output_lang, train_pairs)\n",
    "evaluateAll(encoder2, decoder2, input_lang, output_lang, train_pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8aa78f41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0m 43s (- 6m 32s) (1 10%) 1.2471\n",
      "1m 27s (- 5m 48s) (2 20%) 0.4123\n",
      "2m 10s (- 5m 4s) (3 30%) 0.2583\n",
      "2m 54s (- 4m 21s) (4 40%) 0.1948\n",
      "3m 37s (- 3m 37s) (5 50%) 0.1628\n",
      "4m 20s (- 2m 53s) (6 60%) 0.1437\n",
      "5m 4s (- 2m 10s) (7 70%) 0.1308\n",
      "5m 47s (- 1m 26s) (8 80%) 0.1216\n",
      "6m 30s (- 0m 43s) (9 90%) 0.1144\n",
      "7m 14s (- 0m 0s) (10 100%) 0.1091\n",
      "> show me the earliest flight on 8 2 from BOSTON to DENVER that serves a meal\n",
      "= SELECT DISTINCT FLIGHTalias0.FLIGHT_ID FROM AIRPORT_SERVICE AS AIRPORT_SERVICEalias0 , AIRPORT_SERVICE AS AIRPORT_SERVICEalias1 , CITY AS CITYalias0 , CITY AS CITYalias1 , DATE_DAY AS DATE_DAYalias0 , DAYS AS DAYSalias0 , FLIGHT AS FLIGHTalias0 , FOOD_SERVICE AS FOOD_SERVICEalias0 WHERE ( ( ( DATE_DAYalias0.DAY_NUMBER = 2 AND DATE_DAYalias0.MONTH_NUMBER = 8 AND DATE_DAYalias0.YEAR = 1991 AND DAYSalias0.DAY_NAME = DATE_DAYalias0.DAY_NAME AND FLIGHTalias0.FLIGHT_DAYS = DAYSalias0.DAYS_CODE AND FOOD_SERVICEalias0.MEAL_CODE = FLIGHTalias0.MEAL_CODE ) AND CITYalias1.CITY_CODE = AIRPORT_SERVICEalias1.CITY_CODE AND CITYalias1.CITY_NAME = \"DENVER\" AND FLIGHTalias0.TO_AIRPORT = AIRPORT_SERVICEalias1.AIRPORT_CODE ) AND CITYalias0.CITY_CODE = AIRPORT_SERVICEalias0.CITY_CODE AND CITYalias0.CITY_NAME = \"BOSTON\" AND FLIGHTalias0.FROM_AIRPORT = AIRPORT_SERVICEalias0.AIRPORT_CODE ) AND FLIGHTalias0.DEPARTURE_TIME = ( SELECT MIN( FLIGHTalias1.DEPARTURE_TIME ) FROM AIRPORT_SERVICE AS AIRPORT_SERVICEalias2 , AIRPORT_SERVICE AS AIRPORT_SERVICEalias3 , CITY AS CITYalias2 , CITY AS CITYalias3 , DATE_DAY AS DATE_DAYalias1 , DAYS AS DAYSalias1 , FLIGHT AS FLIGHTalias1 , FOOD_SERVICE AS FOOD_SERVICEalias1 WHERE ( ( DATE_DAYalias1.DAY_NUMBER = 2 AND DATE_DAYalias1.MONTH_NUMBER = 8 AND DATE_DAYalias1.YEAR = 1991 AND DAYSalias1.DAY_NAME = DATE_DAYalias1.DAY_NAME AND FLIGHTalias1.FLIGHT_DAYS = DAYSalias1.DAYS_CODE AND FOOD_SERVICEalias1.MEAL_CODE = FLIGHTalias1.MEAL_CODE ) AND CITYalias3.CITY_CODE = AIRPORT_SERVICEalias3.CITY_CODE AND CITYalias3.CITY_NAME = \"DENVER\" AND FLIGHTalias1.TO_AIRPORT = AIRPORT_SERVICEalias3.AIRPORT_CODE ) AND CITYalias2.CITY_CODE = AIRPORT_SERVICEalias2.CITY_CODE AND CITYalias2.CITY_NAME = \"BOSTON\" AND FLIGHTalias1.FROM_AIRPORT = AIRPORT_SERVICEalias2.AIRPORT_CODE ) ;\n",
      "< SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS\n",
      "\n",
      "> what flights are there from MINNEAPOLIS to NEWARK on CO\n",
      "= SELECT DISTINCT FLIGHTalias0.FLIGHT_ID FROM AIRPORT_SERVICE AS AIRPORT_SERVICEalias0 , AIRPORT_SERVICE AS AIRPORT_SERVICEalias1 , CITY AS CITYalias0 , CITY AS CITYalias1 , FLIGHT AS FLIGHTalias0 WHERE ( CITYalias0.CITY_CODE = AIRPORT_SERVICEalias0.CITY_CODE AND CITYalias0.CITY_NAME = \"NEWARK\" AND CITYalias1.CITY_CODE = AIRPORT_SERVICEalias1.CITY_CODE AND CITYalias1.CITY_NAME = \"MINNEAPOLIS\" AND FLIGHTalias0.TO_AIRPORT = AIRPORT_SERVICEalias0.AIRPORT_CODE AND FLIGHTalias0.FROM_AIRPORT = AIRPORT_SERVICEalias1.AIRPORT_CODE ) AND FLIGHTalias0.AIRLINE_CODE = \"CO\" ;\n",
      "< SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS\n",
      "\n",
      "> what are all flights from BOSTON to PITTSBURGH on WEDNESDAY\n",
      "= SELECT DISTINCT FLIGHTalias0.FLIGHT_ID FROM AIRPORT_SERVICE AS AIRPORT_SERVICEalias0 , AIRPORT_SERVICE AS AIRPORT_SERVICEalias1 , CITY AS CITYalias0 , CITY AS CITYalias1 , DAYS AS DAYSalias0 , FLIGHT AS FLIGHTalias0 WHERE ( CITYalias1.CITY_CODE = AIRPORT_SERVICEalias1.CITY_CODE AND CITYalias1.CITY_NAME = \"PITTSBURGH\" AND DAYSalias0.DAY_NAME = \"WEDNESDAY\" AND FLIGHTalias0.FLIGHT_DAYS = DAYSalias0.DAYS_CODE AND FLIGHTalias0.TO_AIRPORT = AIRPORT_SERVICEalias1.AIRPORT_CODE ) AND CITYalias0.CITY_CODE = AIRPORT_SERVICEalias0.CITY_CODE AND CITYalias0.CITY_NAME = \"BOSTON\" AND FLIGHTalias0.FROM_AIRPORT = AIRPORT_SERVICEalias0.AIRPORT_CODE ;\n",
      "< SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS\n",
      "\n",
      "> show me all flights from DALLAS to PITTSBURGH on monday which leave after 2000 o'clock pm\n",
      "= SELECT DISTINCT FLIGHTalias0.FLIGHT_ID FROM AIRPORT_SERVICE AS AIRPORT_SERVICEalias0 , AIRPORT_SERVICE AS AIRPORT_SERVICEalias1 , CITY AS CITYalias0 , CITY AS CITYalias1 , DATE_DAY AS DATE_DAYalias0 , DAYS AS DAYSalias0 , FLIGHT AS FLIGHTalias0 WHERE ( ( DATE_DAYalias0.DAY_NUMBER = 21 AND DATE_DAYalias0.MONTH_NUMBER = 2 AND DATE_DAYalias0.YEAR = 1991 AND DAYSalias0.DAY_NAME = DATE_DAYalias0.DAY_NAME AND FLIGHTalias0.DEPARTURE_TIME > 2000 AND FLIGHTalias0.FLIGHT_DAYS = DAYSalias0.DAYS_CODE ) AND CITYalias1.CITY_CODE = AIRPORT_SERVICEalias1.CITY_CODE AND CITYalias1.CITY_NAME = \"PITTSBURGH\" AND FLIGHTalias0.TO_AIRPORT = AIRPORT_SERVICEalias1.AIRPORT_CODE ) AND CITYalias0.CITY_CODE = AIRPORT_SERVICEalias0.CITY_CODE AND CITYalias0.CITY_NAME = \"DALLAS\" AND FLIGHTalias0.FROM_AIRPORT = AIRPORT_SERVICEalias0.AIRPORT_CODE ;\n",
      "< SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS\n",
      "\n",
      "> show me the flights from INDIANAPOLIS to ORLANDO with round trip fares less than 1288\n",
      "= SELECT DISTINCT FLIGHTalias0.FLIGHT_ID FROM AIRPORT_SERVICE AS AIRPORT_SERVICEalias0 , AIRPORT_SERVICE AS AIRPORT_SERVICEalias1 , CITY AS CITYalias0 , CITY AS CITYalias1 , FARE AS FAREalias0 , FLIGHT AS FLIGHTalias0 , FLIGHT_FARE AS FLIGHT_FAREalias0 WHERE ( CITYalias1.CITY_CODE = AIRPORT_SERVICEalias1.CITY_CODE AND CITYalias1.CITY_NAME = \"ORLANDO\" AND FAREalias0.ROUND_TRIP_COST < 1288 AND FLIGHT_FAREalias0.FARE_ID = FAREalias0.FARE_ID AND FLIGHTalias0.FLIGHT_ID = FLIGHT_FAREalias0.FLIGHT_ID AND FLIGHTalias0.TO_AIRPORT = AIRPORT_SERVICEalias1.AIRPORT_CODE ) AND CITYalias0.CITY_CODE = AIRPORT_SERVICEalias0.CITY_CODE AND CITYalias0.CITY_NAME = \"INDIANAPOLIS\" AND FLIGHTalias0.FROM_AIRPORT = AIRPORT_SERVICEalias0.AIRPORT_CODE ;\n",
      "< SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder3 = EncoderRNN(input_lang.n_words, hidden_size).to(device)\n",
    "decoder3 = AttnDecoderRNN(hidden_size, output_lang.n_words, \"dot-product\").to(device)\n",
    "\n",
    "train(train_dataloader, encoder3, decoder3, 10, print_every=1)\n",
    "\n",
    "evaluateRandomly(encoder3, decoder3, input_lang, output_lang, train_pairs)\n",
    "evaluateAll(encoder3, decoder3, input_lang, output_lang, train_pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a69f962b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading lines...\n",
      "Read 4347 sentence pairs\n",
      "Trimmed to 4315 sentence pairs\n",
      "Counting words...\n",
      "Counted words:\n",
      "natural 896\n",
      "sql 712\n",
      "1m 5s (- 20m 44s) (1 5%) 1.2523\n",
      "2m 10s (- 19m 35s) (2 10%) 0.3895\n",
      "3m 15s (- 18m 30s) (3 15%) 0.2438\n",
      "4m 21s (- 17m 24s) (4 20%) 0.1837\n",
      "5m 26s (- 16m 20s) (5 25%) 0.1523\n",
      "6m 32s (- 15m 14s) (6 30%) 0.1335\n",
      "7m 37s (- 14m 9s) (7 35%) 0.1214\n",
      "8m 42s (- 13m 4s) (8 40%) 0.1115\n",
      "9m 48s (- 11m 58s) (9 45%) 0.1034\n",
      "10m 53s (- 10m 53s) (10 50%) 0.0967\n",
      "11m 58s (- 9m 47s) (11 55%) 0.0905\n",
      "13m 3s (- 8m 42s) (12 60%) 0.0853\n",
      "14m 9s (- 7m 37s) (13 65%) 0.0808\n",
      "15m 14s (- 6m 31s) (14 70%) 0.0765\n",
      "16m 19s (- 5m 26s) (15 75%) 0.0728\n",
      "17m 24s (- 4m 21s) (16 80%) 0.0689\n",
      "18m 30s (- 3m 15s) (17 85%) 0.0654\n",
      "19m 35s (- 2m 10s) (18 90%) 0.0615\n",
      "20m 40s (- 1m 5s) (19 95%) 0.0576\n",
      "21m 46s (- 0m 0s) (20 100%) 0.0533\n",
      "> i'd like to go from BOSTON to SAN FRANCISCO\n",
      "= SELECT DISTINCT FLIGHTalias0.FLIGHT_ID FROM AIRPORT_SERVICE AS AIRPORT_SERVICEalias0 , AIRPORT_SERVICE AS AIRPORT_SERVICEalias1 , CITY AS CITYalias0 , CITY AS CITYalias1 , FLIGHT AS FLIGHTalias0 WHERE CITYalias0.CITY_CODE = AIRPORT_SERVICEalias0.CITY_CODE AND CITYalias0.CITY_NAME = \"BOSTON\" AND CITYalias1.CITY_CODE = AIRPORT_SERVICEalias1.CITY_CODE AND CITYalias1.CITY_NAME = \"SAN FRANCISCO\" AND FLIGHTalias0.FROM_AIRPORT = AIRPORT_SERVICEalias0.AIRPORT_CODE AND FLIGHTalias0.TO_AIRPORT = AIRPORT_SERVICEalias1.AIRPORT_CODE ;\n",
      "< SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS\n",
      "\n",
      "> what is the total schedule for DL 's flights to all airports\n",
      "= SELECT DISTINCT FLIGHTalias0.ARRIVAL_TIME , FLIGHTalias0.DEPARTURE_TIME FROM AIRPORT AS AIRPORTalias0 , FLIGHT AS FLIGHTalias0 , FLIGHT AS FLIGHTalias1 , FLIGHT_LEG AS FLIGHT_LEGalias0 WHERE FLIGHTalias0.AIRLINE_CODE = \"DL\" AND FLIGHTalias0.FLIGHT_ID = FLIGHT_LEGalias0.LEG_FLIGHT AND FLIGHTalias1.FLIGHT_ID = FLIGHT_LEGalias0.FLIGHT_ID AND FLIGHTalias1.TO_AIRPORT = AIRPORTalias0.AIRPORT_CODE ;\n",
      "< SELECT MIN( FLIGHTalias1.DEPARTURE_TIME ) FROM AIRPORT_SERVICE AS AIRPORT_SERVICEalias1 , CITY AS CITYalias1 , FARE AS FAREalias0 , FARE_BASIS AS FARE_BASISalias0 , FLIGHT AS FLIGHTalias0 , FLIGHT_FARE AS FLIGHT_FAREalias0 WHERE ( ( ( DATE_DAYalias0.DAY_NUMBER = 23 DATE_DAYalias0.MONTH_NUMBER AND DATE_DAYalias0.YEAR = 1991 AND DAYSalias0.DAY_NAME = DATE_DAYalias0.DAY_NAME AND DAYSalias1.DAY_NAME = DATE_DAYalias1.DAY_NAME AND FARE_BASISalias0.BASIS_DAYS = DAYSalias1.DAYS_CODE AND FAREalias0.FARE_BASIS_CODE = FARE_BASISalias0.FARE_BASIS_CODE ; <EOS>\n",
      "\n",
      "> what airlines have BUSINESS class\n",
      "= SELECT DISTINCT AIRLINEalias0.AIRLINE_CODE FROM AIRLINE AS AIRLINEalias0 , FARE AS FAREalias0 , FARE_BASIS AS FARE_BASISalias0 , FLIGHT AS FLIGHTalias0 , FLIGHT_FARE AS FLIGHT_FAREalias0 WHERE FARE_BASISalias0.CLASS_TYPE = \"BUSINESS\" AND FAREalias0.FARE_BASIS_CODE = FARE_BASISalias0.FARE_BASIS_CODE AND FLIGHT_FAREalias0.FARE_ID = FAREalias0.FARE_ID AND FLIGHTalias0.AIRLINE_CODE = AIRLINEalias0.AIRLINE_CODE AND FLIGHTalias0.FLIGHT_ID = FLIGHT_FAREalias0.FLIGHT_ID ;\n",
      "< SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS\n",
      "\n",
      "> what does it cost to fly from BOSTON to SAN FRANCISCO on UA flight 21\n",
      "= SELECT DISTINCT FAREalias0.FARE_ID FROM AIRPORT_SERVICE AS AIRPORT_SERVICEalias0 , AIRPORT_SERVICE AS AIRPORT_SERVICEalias1 , CITY AS CITYalias0 , CITY AS CITYalias1 , FARE AS FAREalias0 , FLIGHT AS FLIGHTalias0 , FLIGHT_FARE AS FLIGHT_FAREalias0 WHERE ( ( CITYalias1.CITY_CODE = AIRPORT_SERVICEalias1.CITY_CODE AND CITYalias1.CITY_NAME = \"SAN FRANCISCO\" AND FLIGHTalias0.FLIGHT_NUMBER = 21 AND FLIGHTalias0.TO_AIRPORT = AIRPORT_SERVICEalias1.AIRPORT_CODE ) AND CITYalias0.CITY_CODE = AIRPORT_SERVICEalias0.CITY_CODE AND CITYalias0.CITY_NAME = \"BOSTON\" AND FLIGHTalias0.FROM_AIRPORT = AIRPORT_SERVICEalias0.AIRPORT_CODE ) AND FLIGHT_FAREalias0.FARE_ID = FAREalias0.FARE_ID AND FLIGHTalias0.AIRLINE_CODE = \"UA\" AND FLIGHTalias0.FLIGHT_ID = FLIGHT_FAREalias0.FLIGHT_ID ;\n",
      "< SELECT MIN( FAREalias1.ONE_DIRECTION_COST ) FROM AIRPORT_SERVICE AS AIRPORT_SERVICEalias2 , AIRPORT_SERVICE AS AIRPORT_SERVICEalias3 , CITY AS CITYalias2 , CITY AS CITYalias3 , DATE_DAY AS DATE_DAYalias1 , DAYS AS DAYSalias1 , FARE AS FAREalias0 , FARE_BASIS AS FARE_BASISalias0 WHERE ( ( DATE_DAYalias0.DAY_NUMBER = 29 AND DATE_DAYalias0.MONTH_NUMBER = 4 AND DATE_DAYalias0.YEAR = 1991 AND DAYSalias0.DAY_NAME = DATE_DAYalias0.DAY_NAME AND DAYSalias1.DAY_NAME = DATE_DAYalias1.DAY_NAME AND FARE_BASISalias0.BASIS_DAYS = DAYSalias1.DAYS_CODE AND FAREalias0.FARE_BASIS_CODE = FARE_BASISalias0.FARE_BASIS_CODE AND FLIGHT_FAREalias0.FARE_ID = FAREalias0.FARE_ID AND FLIGHTalias0.FLIGHT_ID = FLIGHT_FAREalias0.FLIGHT_ID ) AND FLIGHTalias0.AIRLINE_CODE = \"AA\" ; <EOS>\n",
      "\n",
      "> please show me flights from PHILADELPHIA to BALTIMORE between 1000 and 1400\n",
      "= SELECT DISTINCT FLIGHTalias0.FLIGHT_ID FROM AIRPORT_SERVICE AS AIRPORT_SERVICEalias0 , AIRPORT_SERVICE AS AIRPORT_SERVICEalias1 , CITY AS CITYalias0 , CITY AS CITYalias1 , FLIGHT AS FLIGHTalias0 WHERE ( ( FLIGHTalias0.DEPARTURE_TIME <= 1400 AND FLIGHTalias0.DEPARTURE_TIME >= 1000 ) AND CITYalias1.CITY_CODE = AIRPORT_SERVICEalias1.CITY_CODE AND CITYalias1.CITY_NAME = \"BALTIMORE\" AND FLIGHTalias0.TO_AIRPORT = AIRPORT_SERVICEalias1.AIRPORT_CODE ) AND CITYalias0.CITY_CODE = AIRPORT_SERVICEalias0.CITY_CODE AND CITYalias0.CITY_NAME = \"PHILADELPHIA\" AND FLIGHTalias0.FROM_AIRPORT = AIRPORT_SERVICEalias0.AIRPORT_CODE ;\n",
      "< SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class BiEncoderRNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, dropout_p=0.1):\n",
    "        super(BiEncoderRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.embedding = nn.Embedding(input_size, hidden_size)\n",
    "        self.lstm = nn.LSTM(hidden_size, hidden_size, batch_first=True, bidirectional=True)\n",
    "        self.dropout = nn.Dropout(dropout_p)\n",
    "\n",
    "    def forward(self, input):\n",
    "        embedded = self.dropout(self.embedding(input))\n",
    "        output, (h_n, c_n) = self.lstm(embedded)\n",
    "        # 双向LSTM会输出 2 个方向的最后隐藏状态，我们将它拼接\n",
    "        h_n = torch.cat((h_n[0], h_n[1]), dim=1).unsqueeze(0)  # [1, batch, hidden*2]\n",
    "        c_n = torch.cat((c_n[0], c_n[1]), dim=1).unsqueeze(0)  # [1, batch, hidden*2]\n",
    "        return output, (h_n, c_n)\n",
    "\n",
    "class AdditiveAttention(nn.Module):\n",
    "    def __init__(self, hidden_size):\n",
    "        super(AdditiveAttention, self).__init__()\n",
    "        self.Wa = nn.Linear(hidden_size, hidden_size)\n",
    "        self.Ua = nn.Linear(hidden_size, hidden_size)\n",
    "        self.Va = nn.Linear(hidden_size, 1)\n",
    "        self.out_size = hidden_size * 2\n",
    "\n",
    "    def forward(self, query, keys):\n",
    "        scores = self.Va(torch.tanh(self.Wa(query) + self.Ua(keys)))\n",
    "        scores = scores.squeeze(2).unsqueeze(1)\n",
    "        weights = F.softmax(scores, dim=-1)\n",
    "        context = torch.bmm(weights, keys)\n",
    "        return context, weights\n",
    "\n",
    "class AttnDecoderRNN(nn.Module):\n",
    "    def __init__(self, hidden_size, output_size, attention_type=\"none\", dropout_p=0.1):\n",
    "        super(AttnDecoderRNN, self).__init__()\n",
    "        self.embedding = nn.Embedding(output_size, hidden_size)\n",
    "        self.attention = get_attention_module(attention_type, hidden_size)\n",
    "        self.lstm = nn.LSTM(self.attention.out_size, hidden_size, batch_first=True)\n",
    "        self.out = nn.Linear(hidden_size, output_size)\n",
    "        self.dropout = nn.Dropout(dropout_p)\n",
    "\n",
    "    def forward(self, encoder_outputs, encoder_hidden, target_tensor=None):\n",
    "        batch_size = encoder_outputs.size(0)\n",
    "        decoder_input = torch.empty(batch_size, 1, dtype=torch.long, device=device).fill_(SOS_token)\n",
    "        decoder_hidden = encoder_hidden  # (h_n, c_n)\n",
    "        decoder_outputs = []\n",
    "        attentions = []\n",
    "\n",
    "        for i in range(MAX_LENGTH):\n",
    "            decoder_output, decoder_hidden, attn_weights = self.forward_step(\n",
    "                decoder_input, decoder_hidden, encoder_outputs\n",
    "            )\n",
    "            decoder_outputs.append(decoder_output)\n",
    "            attentions.append(attn_weights)\n",
    "\n",
    "            if target_tensor is not None:\n",
    "                decoder_input = target_tensor[:, i].unsqueeze(1)  # Teacher forcing\n",
    "            else:\n",
    "                _, topi = decoder_output.topk(1)\n",
    "                decoder_input = topi.squeeze(-1).detach()\n",
    "\n",
    "        decoder_outputs = torch.cat(decoder_outputs, dim=1)\n",
    "        decoder_outputs = F.log_softmax(decoder_outputs, dim=-1)\n",
    "        attentions = torch.cat(attentions, dim=1)\n",
    "\n",
    "        return decoder_outputs, decoder_hidden, attentions\n",
    "\n",
    "    def forward_step(self, input, hidden, encoder_outputs):\n",
    "        embedded = self.dropout(self.embedding(input))\n",
    "        h_n, c_n = hidden\n",
    "        query = h_n.permute(1, 0, 2)  # [batch, 1, hidden]\n",
    "        context, attn_weights = self.attention(query, encoder_outputs)\n",
    "        input_lstm = torch.cat((embedded, context), dim=2)\n",
    "        output, (h_n, c_n) = self.lstm(input_lstm, (h_n, c_n))\n",
    "        output = self.out(output)\n",
    "        return output, (h_n, c_n), attn_weights\n",
    "\n",
    "\n",
    "enc_hidden_size = 64\n",
    "hidden_size = 128\n",
    "batch_size = 32\n",
    "\n",
    "input_lang, output_lang, train_dataloader, train_pairs = get_dataloader(batch_size, question_train_data)\n",
    "\n",
    "encoder4 = BiEncoderRNN(input_lang.n_words, enc_hidden_size).to(device)\n",
    "decoder4 = AttnDecoderRNN(hidden_size, output_lang.n_words, \"additive\").to(device)\n",
    "\n",
    "train(train_dataloader, encoder4, decoder4, 20, print_every=1)\n",
    "\n",
    "evaluateRandomly(encoder4, decoder4, input_lang, output_lang, train_pairs)\n",
    "evaluateAll(encoder4, decoder4, input_lang, output_lang, train_pairs)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
