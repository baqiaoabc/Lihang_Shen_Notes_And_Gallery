{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cyz6dO6I66o1",
        "outputId": "eec11c4b-1b6b-44c9-a673-ef216b766ed6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2.0.1+cu117\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import json\n",
        "import spacy\n",
        "import numpy as np\n",
        "from pprint import pprint\n",
        "from spacy.language import Language\n",
        "from spacy.tokens import Doc\n",
        "from spacy.tokenizer import Tokenizer\n",
        "import re\n",
        "import unicodedata\n",
        "from tqdm import tqdm\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.nn.utils.rnn import pad_sequence, pack_padded_sequence, pad_packed_sequence\n",
        "from collections import defaultdict\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "print(torch.__version__)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "Km3s9h_37Rth"
      },
      "outputs": [],
      "source": [
        "def unicodeToAscii(s):\n",
        "    # Convert a Unicode string 's' to plain ASCII.\n",
        "    # This is done by first normalizing the string into its decomposed form using 'NFD',\n",
        "    # which separates characters from their accents. Then, it filters out all nonspacing marks (Mn).\n",
        "    return ''.join(\n",
        "        c for c in unicodedata.normalize('NFD', s)\n",
        "        if unicodedata.category(c) != 'Mn'\n",
        "    )\n",
        "\n",
        "def normalize_whitespace(text):\n",
        "  return re.sub(r'\\s+', ' ', text).strip()\n",
        "\n",
        "def preprocess_sentence(s:str) -> str:\n",
        "    \"\"\"\n",
        "    Preprocesses sentence text for consistency\n",
        "    \"\"\"\n",
        "    s = s.strip()\n",
        "    s = normalize_whitespace(s)\n",
        "    s = unicodeToAscii(s)\n",
        "    s = s.strip()\n",
        "    return s\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s1tUUFY-7ZvD",
        "outputId": "4933930e-2cec-4d55-b838-6fa49500486f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting en-core-web-lg==3.8.0\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_lg-3.8.0/en_core_web_lg-3.8.0-py3-none-any.whl (400.7 MB)\n",
            "     ---------------------------------------- 0.0/400.7 MB ? eta -:--:--\n",
            "     --------------------------------------- 3.7/400.7 MB 31.3 MB/s eta 0:00:13\n",
            "      ------------------------------------- 10.5/400.7 MB 31.2 MB/s eta 0:00:13\n",
            "     - ------------------------------------ 16.0/400.7 MB 27.2 MB/s eta 0:00:15\n",
            "     - ------------------------------------ 21.0/400.7 MB 28.8 MB/s eta 0:00:14\n",
            "     -- ----------------------------------- 27.3/400.7 MB 26.6 MB/s eta 0:00:15\n",
            "     -- ----------------------------------- 31.5/400.7 MB 27.7 MB/s eta 0:00:14\n",
            "     --- ---------------------------------- 38.0/400.7 MB 26.3 MB/s eta 0:00:14\n",
            "     --- ---------------------------------- 41.9/400.7 MB 26.9 MB/s eta 0:00:14\n",
            "     ---- --------------------------------- 49.5/400.7 MB 26.5 MB/s eta 0:00:14\n",
            "     ----- -------------------------------- 53.7/400.7 MB 25.7 MB/s eta 0:00:14\n",
            "     ----- -------------------------------- 61.1/400.7 MB 26.7 MB/s eta 0:00:13\n",
            "     ------ ------------------------------- 64.7/400.7 MB 25.8 MB/s eta 0:00:14\n",
            "     ------ ------------------------------- 72.1/400.7 MB 26.4 MB/s eta 0:00:13\n",
            "     ------- ------------------------------ 75.8/400.7 MB 25.8 MB/s eta 0:00:13\n",
            "     ------- ------------------------------ 83.4/400.7 MB 26.5 MB/s eta 0:00:12\n",
            "     -------- ----------------------------- 86.8/400.7 MB 25.8 MB/s eta 0:00:13\n",
            "     -------- ----------------------------- 94.1/400.7 MB 26.3 MB/s eta 0:00:12\n",
            "     --------- ---------------------------- 97.5/400.7 MB 25.7 MB/s eta 0:00:12\n",
            "     --------- --------------------------- 104.6/400.7 MB 26.1 MB/s eta 0:00:12\n",
            "     ---------- -------------------------- 108.5/400.7 MB 25.8 MB/s eta 0:00:12\n",
            "     ---------- -------------------------- 115.3/400.7 MB 26.1 MB/s eta 0:00:11\n",
            "     ----------- ------------------------- 119.3/400.7 MB 25.7 MB/s eta 0:00:11\n",
            "     ----------- ------------------------- 125.8/400.7 MB 26.1 MB/s eta 0:00:11\n",
            "     ------------ ------------------------ 131.1/400.7 MB 25.8 MB/s eta 0:00:11\n",
            "     ------------ ------------------------ 136.3/400.7 MB 26.1 MB/s eta 0:00:11\n",
            "     ------------- ----------------------- 142.3/400.7 MB 25.9 MB/s eta 0:00:10\n",
            "     ------------- ----------------------- 146.8/400.7 MB 26.1 MB/s eta 0:00:10\n",
            "     -------------- ---------------------- 153.6/400.7 MB 26.0 MB/s eta 0:00:10\n",
            "     -------------- ---------------------- 157.3/400.7 MB 26.1 MB/s eta 0:00:10\n",
            "     --------------- --------------------- 164.9/400.7 MB 25.9 MB/s eta 0:00:10\n",
            "     --------------- --------------------- 168.3/400.7 MB 25.6 MB/s eta 0:00:10\n",
            "     ---------------- -------------------- 175.6/400.7 MB 25.9 MB/s eta 0:00:09\n",
            "     ---------------- -------------------- 181.4/400.7 MB 25.9 MB/s eta 0:00:09\n",
            "     ----------------- ------------------- 188.7/400.7 MB 26.2 MB/s eta 0:00:09\n",
            "     ----------------- ------------------- 192.7/400.7 MB 26.0 MB/s eta 0:00:09\n",
            "     ------------------ ------------------ 199.2/400.7 MB 26.2 MB/s eta 0:00:08\n",
            "     ------------------ ------------------ 203.9/400.7 MB 26.0 MB/s eta 0:00:08\n",
            "     ------------------- ----------------- 209.7/400.7 MB 26.2 MB/s eta 0:00:08\n",
            "     ------------------- ----------------- 215.2/400.7 MB 26.0 MB/s eta 0:00:08\n",
            "     -------------------- ---------------- 220.2/400.7 MB 26.2 MB/s eta 0:00:07\n",
            "     -------------------- ---------------- 226.8/400.7 MB 26.0 MB/s eta 0:00:07\n",
            "     --------------------- --------------- 230.7/400.7 MB 26.1 MB/s eta 0:00:07\n",
            "     --------------------- --------------- 238.0/400.7 MB 26.1 MB/s eta 0:00:07\n",
            "     ---------------------- -------------- 241.7/400.7 MB 25.9 MB/s eta 0:00:07\n",
            "     ---------------------- -------------- 249.0/400.7 MB 26.1 MB/s eta 0:00:06\n",
            "     ----------------------- ------------- 252.7/400.7 MB 25.9 MB/s eta 0:00:06\n",
            "     ------------------------ ------------ 260.3/400.7 MB 26.1 MB/s eta 0:00:06\n",
            "     ------------------------ ------------ 264.0/400.7 MB 25.9 MB/s eta 0:00:06\n",
            "     ------------------------- ----------- 271.3/400.7 MB 26.0 MB/s eta 0:00:05\n",
            "     ------------------------- ----------- 275.0/400.7 MB 25.9 MB/s eta 0:00:05\n",
            "     -------------------------- ---------- 282.3/400.7 MB 25.9 MB/s eta 0:00:05\n",
            "     -------------------------- ---------- 286.3/400.7 MB 25.9 MB/s eta 0:00:05\n",
            "     --------------------------- --------- 293.6/400.7 MB 26.2 MB/s eta 0:00:05\n",
            "     --------------------------- --------- 297.8/400.7 MB 26.0 MB/s eta 0:00:04\n",
            "     ---------------------------- -------- 304.1/400.7 MB 26.2 MB/s eta 0:00:04\n",
            "     ---------------------------- -------- 308.5/400.7 MB 25.9 MB/s eta 0:00:04\n",
            "     ----------------------------- ------- 314.6/400.7 MB 26.2 MB/s eta 0:00:04\n",
            "     ----------------------------- ------- 319.8/400.7 MB 25.9 MB/s eta 0:00:04\n",
            "     ------------------------------ ------ 325.1/400.7 MB 26.2 MB/s eta 0:00:03\n",
            "     ------------------------------ ------ 330.8/400.7 MB 25.9 MB/s eta 0:00:03\n",
            "     ------------------------------ ------ 335.5/400.7 MB 26.2 MB/s eta 0:00:03\n",
            "     ------------------------------- ----- 342.4/400.7 MB 25.9 MB/s eta 0:00:03\n",
            "     ------------------------------- ----- 346.0/400.7 MB 26.2 MB/s eta 0:00:03\n",
            "     -------------------------------- ---- 353.4/400.7 MB 26.0 MB/s eta 0:00:02\n",
            "     -------------------------------- ---- 357.3/400.7 MB 26.0 MB/s eta 0:00:02\n",
            "     --------------------------------- --- 364.9/400.7 MB 26.1 MB/s eta 0:00:02\n",
            "     ---------------------------------- -- 368.6/400.7 MB 26.0 MB/s eta 0:00:02\n",
            "     ---------------------------------- -- 375.9/400.7 MB 26.0 MB/s eta 0:00:01\n",
            "     ----------------------------------- - 379.3/400.7 MB 26.0 MB/s eta 0:00:01\n",
            "     ----------------------------------- - 386.7/400.7 MB 26.0 MB/s eta 0:00:01\n",
            "     ------------------------------------  390.3/400.7 MB 26.0 MB/s eta 0:00:01\n",
            "     ------------------------------------  397.7/400.7 MB 26.0 MB/s eta 0:00:01\n",
            "     ------------------------------------  400.6/400.7 MB 26.0 MB/s eta 0:00:01\n",
            "     ------------------------------------  400.6/400.7 MB 26.0 MB/s eta 0:00:01\n",
            "     ------------------------------------  400.6/400.7 MB 26.0 MB/s eta 0:00:01\n",
            "     ------------------------------------- 400.7/400.7 MB 24.9 MB/s eta 0:00:00\n",
            "\u001b[38;5;2mâœ” Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_lg')\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "[notice] A new release of pip is available: 24.2 -> 25.0.1\n",
            "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
          ]
        }
      ],
      "source": [
        "!python -m spacy download en_core_web_lg\n",
        "nlp = spacy.load('en_core_web_lg')\n",
        "\n",
        "# Create a custom component to merge entities\n",
        "@Language.component(\"entity_merger\")\n",
        "def entity_merger(doc):\n",
        "    \"\"\"\n",
        "    Custom component of the spacy nlp pipeline which merges geographical location entity tokens into a single token\n",
        "    For example: 'New York' would noramlly be split into 2 tokens 'New' and 'York' but this will combine into a single 'New York' token\n",
        "    This is implemented because city_name type variables could have the value 'New York' and for effective tagging we aim to keep the tokenisation scheme consistent to the dataset\n",
        "    \"\"\"\n",
        "    # Iterate over the entities in reverse order (to avoid index issues when merging)\n",
        "    with doc.retokenize() as retokenizer:\n",
        "        for ent in reversed(list(doc.ents)):\n",
        "            # Merge the entity tokens into one token\n",
        "            if(ent.label_ in [\"GPE\", \"ORG\"]):\n",
        "                attrs = {\"LEMMA\": ent.text}\n",
        "                retokenizer.merge(ent, attrs=attrs)\n",
        "    return doc\n",
        "\n",
        "# Add the custom component after NER\n",
        "nlp.add_pipe(\"entity_merger\", after=\"ner\")\n",
        "\n",
        "def whitespace_tokenizer(nlp):\n",
        "    # Create a custom tokenizer that splits only on whitespace\n",
        "    return Tokenizer(nlp.vocab, token_match=re.compile(r'\\S+').match)\n",
        "\n",
        "nlp.tokenizer = whitespace_tokenizer(nlp)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "vitzN-sA7fV2"
      },
      "outputs": [],
      "source": [
        "class ATISClassificationDataset(Dataset):\n",
        "\n",
        "    def __init__(self, dataset_loc, nlp, split_type='query', split=['train'], tokenizer=None):\n",
        "\n",
        "        self.nlp = nlp\n",
        "        self.data = []\n",
        "\n",
        "        self.variable_names = set()\n",
        "        self.sql_templates = set()\n",
        "\n",
        "        # Reads Json\n",
        "        with open(dataset_loc) as f:\n",
        "            dataset_json = json.load(f)\n",
        "\n",
        "        for sample in dataset_json:\n",
        "\n",
        "            processed_sample = {}\n",
        "\n",
        "            # All valid sql queries for this examples soretd by length\n",
        "            sql = sorted(sample['sql'],key=len)\n",
        "            # Adds shortest sql query template to list of sql templates\n",
        "            self.sql_templates.add(sql[0])\n",
        "\n",
        "            # Check query split\n",
        "            query_split = sample['query-split']\n",
        "            if split_type == \"query\" and query_split not in split:\n",
        "                continue\n",
        "\n",
        "            # Adds to variables set\n",
        "            variables_metadata = sample[\"variables\"]\n",
        "            for var in variables_metadata:\n",
        "                self.variable_names.add(var.get(\"name\"))\n",
        "\n",
        "            # Process each sentence\n",
        "            for sentence in sample['sentences']:\n",
        "\n",
        "                # Check question split\n",
        "                if split_type == \"question\" and sentence['question-split'] not in split:\n",
        "                    continue\n",
        "\n",
        "                # variables/placeholder mapping dictionary\n",
        "                variables = sentence['variables']\n",
        "\n",
        "                # Sentence text with variables/placeholders\n",
        "                text_with_vars = sentence['text']\n",
        "\n",
        "                # Replacing variables/placeholders in current sentence and sql query with their values from the variables dictionary\n",
        "                text_with_vars_replaced = text_with_vars\n",
        "                sql_with_vars_replaced = sql.copy()\n",
        "\n",
        "                # Replace sentence and all sql variables with their values\n",
        "                for var in variables:\n",
        "                    text_with_vars_replaced = text_with_vars_replaced.replace(var,variables[var])\n",
        "\n",
        "                    # sql_with_vars_replaced = sql_with_vars_replaced.replace(var,variables[var])\n",
        "                    sql_with_vars_replaced = [query.replace(var,variables[var]) for query in sql_with_vars_replaced]\n",
        "\n",
        "                # Taggingg expected output\n",
        "                sentence_var_tagging_labels = []\n",
        "                for word in text_with_vars.split():\n",
        "                    if(word in variables):\n",
        "                        # Use variable name as tag\n",
        "                        sentence_var_tagging_labels.append(word)\n",
        "                    else:\n",
        "                        # Use '-' for non-variable words\n",
        "                        sentence_var_tagging_labels.append(\"no_var\")\n",
        "\n",
        "                # Appends preprocessed dictionary of current sentence to the processesed_dataset list\n",
        "                self.data.append({\n",
        "                    \"text_with_vars\": text_with_vars,\n",
        "                    \"text_with_vars_replaced\":text_with_vars_replaced,\n",
        "                    \"tagging_labels\":sentence_var_tagging_labels,\n",
        "                    \"variables\":variables,\n",
        "                    \"sql_with_vars\": sql,\n",
        "                    \"shortest_sql_with_vars\":sql[0],\n",
        "                    \"sql_with_vars_replaced\": sql_with_vars_replaced,\n",
        "                    \"shortest_sql_with_vars_replaced\": sql_with_vars_replaced[0]\n",
        "                })\n",
        "\n",
        "        # Setup tagging label encoder\n",
        "        # For tagging task - include all variable names plus \"-\" for non-variables\n",
        "        all_tags = [\"-\",\"no_var\"] + list(self.variable_names)\n",
        "        all_tags = np.array(all_tags)\n",
        "        self.tag_encoder = LabelEncoder()\n",
        "        self.tag_encoder.fit(all_tags)\n",
        "\n",
        "        # Setup SQL template label encoder\n",
        "        self.sql_encoder = LabelEncoder()\n",
        "        self.sql_encoder.fit(np.array(list(self.sql_templates)))\n",
        "\n",
        "        # Process sentences using spacy nlp pipeline to get docs, corresponding label ids and tag ids\n",
        "        self.docs = []\n",
        "        self.tag_labels = []\n",
        "        self.sql_labels = []\n",
        "\n",
        "        for sample in self.data:\n",
        "            # Convert sentence with variables into spacy doc\n",
        "            doc = self.nlp(preprocess_sentence(sample['text_with_vars_replaced']))\n",
        "            self.docs.append(doc)\n",
        "\n",
        "            # Covert variable tags into ids foir tagging\n",
        "            tags = sample[\"tagging_labels\"]\n",
        "            # if(len(doc) != len(tags)):\n",
        "            #   print([token for token in doc])\n",
        "            #   print(tags)\n",
        "            self.tag_labels.append(self.tag_encoder.transform(tags))\n",
        "\n",
        "            # Convert sql query with variables into ids for classification\n",
        "            sql_template = sample[\"shortest_sql_with_vars\"]\n",
        "            self.sql_labels.append(self.sql_encoder.transform([sql_template])[0])\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        \"\"\"Get a single sample from the dataset\"\"\"\n",
        "        doc = self.docs[idx]\n",
        "        raw_item = self.data[idx]\n",
        "\n",
        "        # Get token vectors from SpaCy document\n",
        "        # vectors = [token.vector for token in doc]\n",
        "\n",
        "        # Create tensor from vectors\n",
        "        token_vectors = torch.tensor(doc.tensor, dtype=torch.float32)\n",
        "\n",
        "        # Get token texts (needed for variable replacement during inference)\n",
        "        tokens = [token.text for token in doc]\n",
        "\n",
        "        # Get tag labels\n",
        "        tag_labels = torch.tensor(self.tag_labels[idx], dtype=torch.long)\n",
        "\n",
        "        # Get SQL label\n",
        "        sql_label = torch.tensor(self.sql_labels[idx], dtype=torch.long)\n",
        "\n",
        "        return {\n",
        "            \"token_vectors\": token_vectors,\n",
        "            \"tokens\": tokens,\n",
        "            \"tag_labels\": tag_labels,\n",
        "            \"sql_label\": sql_label,\n",
        "            \"raw_item\": raw_item,\n",
        "            \"doc_len\": len(doc),\n",
        "            \"true_sql_text\": raw_item['shortest_sql_with_vars_replaced'] # For inference\n",
        "        }\n",
        "\n",
        "    def get_dataloader(self, batch_size=32, shuffle=True, num_workers=0):\n",
        "        \"\"\"Helper function to create a DataLoader with custom collate function\"\"\"\n",
        "        return DataLoader(\n",
        "            self,\n",
        "            batch_size=batch_size,\n",
        "            shuffle=shuffle,\n",
        "            num_workers=num_workers,\n",
        "            collate_fn=self.collate_fn,\n",
        "            pin_memory=True\n",
        "        )\n",
        "\n",
        "    def collate_fn(self, batch):\n",
        "      \"\"\"Custom collate function that pads sequences to the longest in the batch\"\"\"\n",
        "\n",
        "      # Find max length for both token vectors and tag labels in this batch\n",
        "      max_token_len = max([item[\"doc_len\"] for item in batch])\n",
        "      max_tag_len = max([len(item[\"tag_labels\"]) for item in batch])\n",
        "\n",
        "      # Ensure both use the same max length for consistent padding\n",
        "      max_len = max(max_token_len, max_tag_len)\n",
        "\n",
        "\n",
        "      # PRepare lists to collect tensors and data\n",
        "      token_vectors_list = []\n",
        "      tag_labels_list = []\n",
        "      attention_masks = []\n",
        "      sql_labels = []\n",
        "      tokens_list = []\n",
        "      raw_items = []\n",
        "      true_sql_text_list = []\n",
        "\n",
        "      for item in batch:\n",
        "          # Get original tensors and data\n",
        "          token_vecs = item[\"token_vectors\"]\n",
        "          tags = item[\"tag_labels\"]\n",
        "          tokens = item[\"tokens\"]\n",
        "\n",
        "          # Create attention mask (1 for real tokens, 0 for padding)\n",
        "          seq_len = len(token_vecs)\n",
        "          tag_len = len(tags)\n",
        "          attention_mask = torch.ones(seq_len, dtype=torch.long)\n",
        "\n",
        "          # If padding is needed for tokens\n",
        "          if(seq_len < max_len):\n",
        "            # Pad token vectors\n",
        "            padding = torch.zeros(max_len - seq_len, token_vecs.shape[1], dtype=torch.long)\n",
        "            token_vecs = torch.cat([token_vecs,padding], dim=0)\n",
        "            attention_mask = torch.cat([attention_mask, torch.zeros(max_len - seq_len, dtype=torch.long)])\n",
        "\n",
        "            # Pad tokens list\n",
        "            tokens.extend([\"\"] * (max_len - seq_len))\n",
        "\n",
        "          # Pad tag labels separately to ensure they all have the same length\n",
        "          if(tag_len < max_len):\n",
        "            padding = torch.zeros(max_len - tag_len, dtype=torch.long)\n",
        "            tags = torch.cat([tags, padding], dim=0)\n",
        "\n",
        "          # Add to lists\n",
        "          token_vectors_list.append(token_vecs)\n",
        "          tag_labels_list.append(tags)\n",
        "          attention_masks.append(attention_mask)\n",
        "          sql_labels.append(item[\"sql_label\"])\n",
        "          tokens_list.append(tokens)\n",
        "          raw_items.append(item[\"raw_item\"])\n",
        "          true_sql_text_list.append(item[\"true_sql_text\"])\n",
        "\n",
        "\n",
        "\n",
        "      # Stack tensors\n",
        "      return {\n",
        "          \"token_vectors\": torch.stack(token_vectors_list),\n",
        "          \"tag_labels\": torch.stack(tag_labels_list),\n",
        "          \"attention_mask\": torch.stack(attention_masks),\n",
        "          \"sql_labels\": torch.stack(sql_labels),\n",
        "          \"tokens\": tokens_list,\n",
        "          \"raw_items\": raw_items,\n",
        "          \"true_sql_texts\":true_sql_text_list\n",
        "      }\n",
        "\n",
        "    def get_tag_vocab_size(self):\n",
        "        \"\"\"Returns the size of the tag vocabulary\"\"\"\n",
        "        return len(self.tag_encoder.classes_)\n",
        "\n",
        "    def get_sql_vocab_size(self):\n",
        "        \"\"\"Returns the number of unique SQL templates\"\"\"\n",
        "        return len(self.sql_encoder.classes_)\n",
        "\n",
        "    def get_vector_dim(self):\n",
        "        \"\"\"Returns the dimensionality of token vectors\"\"\"\n",
        "        return self.nlp.get_pipe(\"tok2vec\").model.get_dim(\"nO\")\n",
        "\n",
        "    def decode_tag(self, tag_id):\n",
        "        \"\"\"Convert tag ID back to original variable name or '-' \"\"\"\n",
        "        return self.tag_encoder.inverse_transform([tag_id])[0]\n",
        "\n",
        "    def decode_sql_template(self, sql_id):\n",
        "        \"\"\"Convert SQL template ID back to original SQL template\"\"\"\n",
        "        return self.sql_encoder.inverse_transform([sql_id])[0]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "tiXjMZXg8AEG"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "class LSTMTaggerClassifer(nn.Module):\n",
        "    def __init__(self,\n",
        "                 input_dim:int,\n",
        "                 hidden_dim:int,\n",
        "                 tag_vocab_size:int,\n",
        "                 sql_vocab_size:int,\n",
        "                 num_layers:int = 1,\n",
        "                 dropout:float = 0.25):\n",
        "        \"\"\"\n",
        "        LSTM model for both token tagging and SQL template classification\n",
        "\n",
        "        Args:\n",
        "            input_dim: Dimensionality of input vectors\n",
        "            hidden_dim: Hidden dimension of LSTM\n",
        "            tag_vocab_size: Size of tag vocabulary for tagging task\n",
        "            sql_vocab_size: Number of unique SQL templates\n",
        "            num_layers: Number of LSTM layers\n",
        "            dropout: Dropout probability\n",
        "        \"\"\"\n",
        "        super(LSTMTaggerClassifer, self).__init__()\n",
        "\n",
        "        #LSTM layer\n",
        "        self.lstm = nn.LSTM(\n",
        "            input_dim,\n",
        "            hidden_dim,\n",
        "            num_layers=num_layers,\n",
        "            bidirectional=True,\n",
        "            dropout=dropout if num_layers > 1 else 0,\n",
        "            batch_first=True\n",
        "        )\n",
        "\n",
        "        # Dropout\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "        # Tagging layer (token classification)\n",
        "        # hidden * 2 is for bidirectionality\n",
        "        self.tag_classifier = nn.Linear(hidden_dim * 2, tag_vocab_size)\n",
        "\n",
        "        # SQL Template classification layer\n",
        "        # hidden * 2 is for bidirectionality\n",
        "        self.sql_classifier = nn.Linear(hidden_dim * 2, sql_vocab_size)\n",
        "\n",
        "    def forward(self, token_vectors, attention_mask=None):\n",
        "        \"\"\"\n",
        "        Forward pass\n",
        "\n",
        "        Args:\n",
        "            token_vectors: Token vectors from SpaCy [batch_size, seq_len, input_dim]\n",
        "            attention_mask: Attention mask indicating valid tokens [batch_size, seq_len]\n",
        "\n",
        "        Returns:\n",
        "            tag_logits: Token classification logits [batch_size, seq_len, tag_vocab_size]\n",
        "            sql_logits: SQL template classification logits [batch_size, sql_vocab_size]\n",
        "        \"\"\"\n",
        "        batch_size = token_vectors.shape[0]\n",
        "        seq_len = token_vectors.shape[1]\n",
        "\n",
        "        # Ignore padded vectrors in batch\n",
        "        packed = nn.utils.rnn.pack_padded_sequence(token_vectors, attention_mask.sum(dim=1).cpu().long(), batch_first=True, enforce_sorted=False)\n",
        "        # LSTM\n",
        "        output, (hidden, _) = self.lstm(packed)\n",
        "        # Unpack output back to padded sequences\n",
        "        output, _ = nn.utils.rnn.pad_packed_sequence(output, batch_first=True,total_length=seq_len)\n",
        "        # Appply dropout\n",
        "        output = self.dropout(output)\n",
        "        # Feedforward layer to get tagging logits\n",
        "        tag_logits = self.tag_classifier(output)  # shape: (B, T, num_tags)\n",
        "\n",
        "        # Classification from last hidden states (concat of both directions)\n",
        "        final_hidden = torch.cat((hidden[-2], hidden[-1]), dim=1)\n",
        "        # Apply dropout to final hidden output\n",
        "        # final_hidden = self.dropout(final_hidden)\n",
        "        # Feedforward layer to get SQL classification\n",
        "        sql_logits = self.sql_classifier(final_hidden)\n",
        "\n",
        "        return tag_logits, sql_logits\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "1wCBEdcw8ME8"
      },
      "outputs": [],
      "source": [
        "def evaluate_model(model, data_loader, device=\"cuda\" if torch.cuda.is_available() else \"cpu\", dataset=None):\n",
        "    \"\"\"\n",
        "    Evaluate the LSTM model with detailed metrics\n",
        "\n",
        "    Args:\n",
        "        model: LSTMTaggerClassifier model\n",
        "        data_loader: DataLoader with evaluation data\n",
        "        device: Device to run evaluation on\n",
        "        dataset: SpacyTextDataset instance (for decoding predictions)\n",
        "\n",
        "    Returns:\n",
        "        Dictionary of evaluation metrics\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "    tag_criterion = nn.CrossEntropyLoss(ignore_index=0, reduction='sum')\n",
        "    sql_criterion = nn.CrossEntropyLoss(reduction='sum')\n",
        "\n",
        "    total_tag_loss = 0\n",
        "    total_sql_loss = 0\n",
        "    total_correct_tags = 0\n",
        "    total_valid_tags = 0\n",
        "    total_correct_sql = 0\n",
        "    total_sql_match = 0\n",
        "    total_samples = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch in data_loader:\n",
        "            # Get batch data\n",
        "            token_vectors = batch[\"token_vectors\"].to(device)\n",
        "            attention_mask = batch[\"attention_mask\"].to(device)\n",
        "            tag_labels = batch[\"tag_labels\"].to(device)\n",
        "            sql_labels = batch[\"sql_labels\"].to(device)\n",
        "            raw_items = batch[\"raw_items\"]\n",
        "            tokens = batch[\"tokens\"]\n",
        "\n",
        "            # Forward pass\n",
        "            tag_logits, sql_logits = model(token_vectors, attention_mask)\n",
        "\n",
        "            # Calculate lossess\n",
        "            tag_loss = tag_criterion(tag_logits.view(-1, tag_logits.size(-1)), tag_labels.view(-1))\n",
        "            sql_loss = sql_criterion(sql_logits, sql_labels)\n",
        "\n",
        "            # Calculate metrics\n",
        "            tag_preds = torch.argmax(tag_logits, dim=-1)\n",
        "            sql_preds = torch.argmax(sql_logits, dim=-1)\n",
        "\n",
        "            # Only count valid tags (non-padding)\n",
        "            valid_mask = (tag_labels != 0) & (attention_mask == 1)\n",
        "            total_correct_tags += (tag_preds[valid_mask] == tag_labels[valid_mask]).sum().item()\n",
        "            total_valid_tags += valid_mask.sum().item()\n",
        "\n",
        "            total_correct_sql += (sql_preds == sql_labels).sum().item()\n",
        "\n",
        "            # Calculate SQL match accuracy (check if final SQL is in the list of valid SQLs)\n",
        "            if dataset:\n",
        "                for i in range(len(raw_items)):\n",
        "                    # get predicted template\n",
        "                    template = dataset.decode_sql_template(sql_preds[i].item())\n",
        "\n",
        "                    # Get predicted tags and tokens\n",
        "                    valid_length = attention_mask[i].sum().item()\n",
        "                    item_tokens = tokens[i][:valid_length]\n",
        "                    item_tag_preds = [dataset.decode_tag(t.item()) for t in tag_preds[i, :valid_length]]\n",
        "\n",
        "                    # Build identified variables\n",
        "                    variables = {}\n",
        "                    for j, (token, tag) in enumerate(zip(item_tokens, item_tag_preds)):\n",
        "                        if(tag != '-' or tag != 'no_var'):\n",
        "                            variables[tag] = token\n",
        "\n",
        "                    # Replace variables in template\n",
        "                    final_sql = template.copy()\n",
        "                    for var_name, var_value in variables.items():\n",
        "                        final_sql = final_sql.replace(var_name, var_value)\n",
        "\n",
        "                    if preprocess_sentence(final_sql) in [preprocess_sentence(x) for x in raw_items[i][\"sql_with_vars_replaced\"]]:\n",
        "                      # print(f\"SQL Template predicted:\\n{template}\\n\")\n",
        "                      # print(f\"Variables identified:\\n{variables}\\n\")\n",
        "                      # print(f\"SQL model predicted:\\n{final_sql}\\n\")\n",
        "                      # print(f\"True SQL:\\n{raw_items[i]['sql_with_vars_replaced']}\")\n",
        "                      total_sql_match += 1\n",
        "\n",
        "            total_samples += len(token_vectors)\n",
        "            total_tag_loss += tag_loss.item()\n",
        "            total_sql_loss += sql_loss.item()\n",
        "\n",
        "    metrics = {\n",
        "        \"tag_loss\": total_tag_loss / total_valid_tags if total_valid_tags > 0 else 0,\n",
        "        \"sql_loss\": total_sql_loss / total_samples,\n",
        "        \"tag_acc\": total_correct_tags / total_valid_tags if total_valid_tags > 0 else 0,\n",
        "        \"sql_acc\": total_correct_sql / total_samples,\n",
        "    }\n",
        "\n",
        "    if dataset:\n",
        "        metrics[\"sql_match_acc\"] = total_sql_match / total_samples\n",
        "\n",
        "    # print(f\"EVAL METRICS:\")\n",
        "    # print(metrics)\n",
        "    return metrics\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "NAvNEkMX8fv_"
      },
      "outputs": [],
      "source": [
        "def train_model(model, train_loader, val_loader=None, epochs=1-0, lr=1e-3, device=\"cuda\" if torch.cuda.is_available() else \"cpu\"):\n",
        "    \"\"\"\n",
        "    Train LSTMTaggerClassfier model\n",
        "    \"\"\"\n",
        "    model = model.to(device)\n",
        "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
        "    tag_criterion = nn.CrossEntropyLoss()\n",
        "    sql_criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        # Set model to training model\n",
        "        model.train()\n",
        "        total_loss = 0\n",
        "\n",
        "        # Training loop\n",
        "        for batch in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{epochs}\"):\n",
        "          # Get batch data\n",
        "          token_vectors = batch[\"token_vectors\"].to(device)\n",
        "          attention_mask = batch[\"attention_mask\"].to(device)\n",
        "          tag_labels = batch[\"tag_labels\"].to(device)\n",
        "          sql_labels = batch[\"sql_labels\"].to(device)\n",
        "\n",
        "          # Forward pass\n",
        "          tag_logits, sql_logits = model(token_vectors, attention_mask)\n",
        "\n",
        "          # Handle dimension mismatches\n",
        "          tag_seq_len = tag_logits.size(1)\n",
        "          label_seq_len = tag_labels.size(1)\n",
        "\n",
        "          if tag_seq_len != label_seq_len:\n",
        "              # Either pad or truncate logits/labels to match\n",
        "              if tag_seq_len < label_seq_len:\n",
        "                  # If logits are shorter, truncate labels\n",
        "                  tag_labels = tag_labels[:, :tag_seq_len]\n",
        "              else:\n",
        "                  # If labels are shorter, truncate logits\n",
        "                  tag_logits = tag_logits[:, :label_seq_len, :]\n",
        "\n",
        "          # Calculate losses\n",
        "          # tag_labels shape: torch.Size([32, 19])\n",
        "          # tag_logits shaep: torch.Size([32, 18, 63])\n",
        "          # if(tag_labels.size(1) != tag_logits.size(1)):\n",
        "          #   print(tag_labels[0])\n",
        "          #   print(tag_logits[1])\n",
        "\n",
        "          # print(f\"tag_labels shape: {tag_labels.size()}\")\n",
        "          # print(f\"tag_logits shaep: {tag_logits.size()}\")\n",
        "          # print(f\"input shape: {tag_logits.view(-1, tag_logits.size(-1)).shape}\")\n",
        "          # print(f\"target shape: {tag_labels.view(-1).shape}\")\n",
        "          # print(\"*\"*50)\n",
        "\n",
        "          # Calculate losses\n",
        "          tag_loss = tag_criterion(tag_logits.view(-1, tag_logits.size(-1)), tag_labels.view(-1))\n",
        "          sql_loss = sql_criterion(sql_logits, sql_labels)\n",
        "          loss = tag_loss + sql_loss\n",
        "\n",
        "          # Backwrad pass\n",
        "          optimizer.zero_grad()\n",
        "          loss.backward()\n",
        "          optimizer.step()\n",
        "\n",
        "          total_loss += loss.item()\n",
        "\n",
        "        avg_loss = total_loss/len(train_loader)\n",
        "        print(f\"Epoch {epoch+1}/{epochs}, Training Loss: {avg_loss:.4f}\")\n",
        "\n",
        "        # Validation\n",
        "        if(val_loader):\n",
        "            val_metrics = evaluate_model(model, val_loader, device)\n",
        "            print(f\"Validation - Tag Acc: {val_metrics['tag_acc']:.4f}, SQL Acc: {val_metrics['sql_acc']:.4f}\")\n",
        "\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "D0NUww538lcL"
      },
      "outputs": [],
      "source": [
        "class SQLPipeline:\n",
        "    def __init__(self,model, dataset, device=\"cuda\" if torch.cuda.is_available() else \"cpu\"):\n",
        "        \"\"\"\n",
        "        End-to-end SQL generation pipeline\n",
        "\n",
        "        Args:\n",
        "            model: Trained LSTMTaggerClassifier model\n",
        "            dataset: SpacyTextDataset instance used for training\n",
        "            device: Device to run inference on\n",
        "        \"\"\"\n",
        "        self.model = model.to(device)\n",
        "        self.model.eval()\n",
        "        self.dataset = dataset\n",
        "        self.device = device\n",
        "\n",
        "    def predict(self, text, nlp=None):\n",
        "        \"\"\"\n",
        "        Generate SQL query for input text\n",
        "\n",
        "        Args:\n",
        "            text: Input text query\n",
        "            nlp: Optional SpaCy pipeline (if not provided, uses the dataset's)\n",
        "\n",
        "        Returns:\n",
        "            Dict containing the predicted SQL query, identified variables, and template\n",
        "        \"\"\"\n",
        "\n",
        "        nlp = nlp or self.dataset.nlp\n",
        "\n",
        "        # Process text with SpaCy\n",
        "        doc = nlp(text)\n",
        "\n",
        "        # Extract token vectors and texts\n",
        "        # squeeze(0) used add batch dimension bcos model expects batch but we are processing a single sample\n",
        "        token_vectors = torch.tensor(doc.tensor, dtype=torch.float32).unsqueeze(0).to(self.device)\n",
        "        tokens = [token.text for token in doc]\n",
        "\n",
        "        # Create attention mask\n",
        "        attention_mask = torch.ones(1, len(doc), dtype=torch.float32).to(self.device)\n",
        "\n",
        "        # Forward pass\n",
        "        with torch.no_grad():\n",
        "            tag_logits, sql_logits =self.model(token_vectors, attention_mask)\n",
        "\n",
        "            # Get predicted tag for each token\n",
        "            tag_preds = torch.argmax(tag_logits, dim=-1)[0].cpu().numpy()\n",
        "\n",
        "            # Get predicted SQL template\n",
        "            sql_pred = torch.argmax(sql_logits, dim=-1)[0].item()\n",
        "\n",
        "        # Convert predictions back to human-readable form\n",
        "        predicted_tags = [self.dataset.decode_tag(tag) for tag in tag_preds[:len(tokens)]]\n",
        "        predicted_template = self.dataset.decode_sql_template(sql_pred)\n",
        "\n",
        "        # Extract identified variables\n",
        "        identified_variables = {}\n",
        "        for i, (token, tag) in enumerate(zip(tokens, predicted_tags)):\n",
        "            if tag != \"-\" or tag != 'no_var':\n",
        "                # Tag is a variable name\n",
        "                identified_variables[tag] = token\n",
        "\n",
        "        # Replace variables in the SQL template\n",
        "        final_sql = predicted_template\n",
        "        for var_name, var_value in identified_variables.items():\n",
        "            final_sql = final_sql.replace(var_name, var_value)\n",
        "\n",
        "        return {\n",
        "            \"tokens\": tokens,\n",
        "            \"predicted_tags\": predicted_tags,\n",
        "            \"identified_variables\": identified_variables,\n",
        "            \"predicted_template\": predicted_template,\n",
        "            \"final_sql\": final_sql\n",
        "        }\n",
        "\n",
        "    def batch_predict(self, dataloader):\n",
        "        \"\"\"\n",
        "        Generate SQL queries for a batch of inputs\n",
        "\n",
        "        Args:\n",
        "            dataloader: DataLoader with test data\n",
        "\n",
        "        Returns:\n",
        "            List of prediction results\n",
        "        \"\"\"\n",
        "        results = []\n",
        "\n",
        "        for batch in tqdm(dataloader, desc=\"Generating SQL queries\"):\n",
        "            # Move batch to device\n",
        "            token_vectors = batch[\"token_vectors\"].to(self.device)\n",
        "            attention_mask = batch[\"attention_mask\"].to(self.device)\n",
        "            tokens = batch[\"tokens\"]\n",
        "            raw_items = batch[\"raw_items\"]\n",
        "\n",
        "            # Forward pass\n",
        "            with torch.no_grad():\n",
        "                tag_logits, sql_logits = self.model(token_vectors, attention_mask)\n",
        "\n",
        "                # Get predictions\n",
        "                tag_preds = torch.argmax(tag_logits, dim=-1).cpu().numpy()\n",
        "                sql_preds = torch.argmax(sql_logits, dim=-1).cpu().numpy()\n",
        "\n",
        "            # Process each item in batch\n",
        "            for i in range(len(tokens)):\n",
        "                # Get token and tag sequences\n",
        "                item_tokens = tokens[i]\n",
        "                valid_tokens = [t for t in item_tokens if t] # Filter out padding\n",
        "\n",
        "                # Get predicted tags for valid tokens\n",
        "                item_tags = [self.dataset.decode_tag(tag) for tag in tag_preds[i, :len(valid_tokens)]]\n",
        "\n",
        "                # Get predicted SQL template\n",
        "                predicted_template = self.dataset.decode_sql_template(sql_preds[i])\n",
        "\n",
        "                # Extract identified variables\n",
        "                identified_variables = {}\n",
        "                for j, (token, tag) in enumerate(zip(valid_tokens, item_tags)):\n",
        "                    if tag != \"-\" or tag != 'no_var':\n",
        "                        identified_variables[tag] = token\n",
        "\n",
        "                # Replace variables in the SQL template\n",
        "                final_sql = predicted_template\n",
        "                for var_name, var_value in identified_variables.items():\n",
        "                    final_sql = final_sql.replace(var_name, var_value)\n",
        "\n",
        "                # Add to results\n",
        "                results.append({\n",
        "                    \"tokens\": valid_tokens,\n",
        "                    \"predicted_tags\": item_tags,\n",
        "                    \"identified_variables\": identified_variables,\n",
        "                    \"predicted_template\": predicted_template,\n",
        "                    \"final_sql\": final_sql,\n",
        "                    \"raw_item\": raw_items[i]\n",
        "                })\n",
        "\n",
        "        return results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "JkRJBHXB8qkV"
      },
      "outputs": [],
      "source": [
        "# Create datasets\n",
        "train_dataset = ATISClassificationDataset(\n",
        "    dataset_loc=\"atis.json\",\n",
        "    nlp=nlp,\n",
        "    split_type=\"question\",\n",
        "    split=[\"train\"]\n",
        ")\n",
        "\n",
        "val_dataset = ATISClassificationDataset(\n",
        "    dataset_loc=\"atis.json\",\n",
        "    nlp=nlp,\n",
        "    split_type=\"question\",\n",
        "    split=[\"dev\"]\n",
        ")\n",
        "\n",
        "test_dataset = ATISClassificationDataset(\n",
        "    dataset_loc=\"atis.json\",\n",
        "    nlp=nlp,\n",
        "    split_type=\"question\",\n",
        "    split=[\"test\"]\n",
        ")\n",
        "\n",
        "# Create dataloaders\n",
        "train_loader = train_dataset.get_dataloader(batch_size=32)\n",
        "val_loader = val_dataset.get_dataloader(batch_size=32)\n",
        "test_loader = test_dataset.get_dataloader(batch_size=32)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "tC_chljK87ZT"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "A module that was compiled using NumPy 1.x cannot be run in\n",
            "NumPy 2.2.4 as it may crash. To support both 1.x and 2.x\n",
            "versions of NumPy, modules must be compiled with NumPy 2.0.\n",
            "Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.\n",
            "\n",
            "If you are a user of the module, the easiest solution will be to\n",
            "downgrade to 'numpy<2' or try to upgrade the affected module.\n",
            "We expect that some modules will need time to support NumPy 2.\n",
            "\n",
            "Traceback (most recent call last):  File \"c:\\Users\\slh\\AppData\\Local\\Programs\\Python\\Python310\\lib\\runpy.py\", line 196, in _run_module_as_main\n",
            "    return _run_code(code, main_globals, None,\n",
            "  File \"c:\\Users\\slh\\AppData\\Local\\Programs\\Python\\Python310\\lib\\runpy.py\", line 86, in _run_code\n",
            "    exec(code, run_globals)\n",
            "  File \"C:\\Users\\slh\\AppData\\Roaming\\Python\\Python310\\site-packages\\ipykernel_launcher.py\", line 18, in <module>\n",
            "    app.launch_new_instance()\n",
            "  File \"C:\\Users\\slh\\AppData\\Roaming\\Python\\Python310\\site-packages\\traitlets\\config\\application.py\", line 1075, in launch_instance\n",
            "    app.start()\n",
            "  File \"C:\\Users\\slh\\AppData\\Roaming\\Python\\Python310\\site-packages\\ipykernel\\kernelapp.py\", line 739, in start\n",
            "    self.io_loop.start()\n",
            "  File \"C:\\Users\\slh\\AppData\\Roaming\\Python\\Python310\\site-packages\\tornado\\platform\\asyncio.py\", line 205, in start\n",
            "    self.asyncio_loop.run_forever()\n",
            "  File \"c:\\Users\\slh\\AppData\\Local\\Programs\\Python\\Python310\\lib\\asyncio\\base_events.py\", line 595, in run_forever\n",
            "    self._run_once()\n",
            "  File \"c:\\Users\\slh\\AppData\\Local\\Programs\\Python\\Python310\\lib\\asyncio\\base_events.py\", line 1881, in _run_once\n",
            "    handle._run()\n",
            "  File \"c:\\Users\\slh\\AppData\\Local\\Programs\\Python\\Python310\\lib\\asyncio\\events.py\", line 80, in _run\n",
            "    self._context.run(self._callback, *self._args)\n",
            "  File \"C:\\Users\\slh\\AppData\\Roaming\\Python\\Python310\\site-packages\\ipykernel\\kernelbase.py\", line 545, in dispatch_queue\n",
            "    await self.process_one()\n",
            "  File \"C:\\Users\\slh\\AppData\\Roaming\\Python\\Python310\\site-packages\\ipykernel\\kernelbase.py\", line 534, in process_one\n",
            "    await dispatch(*args)\n",
            "  File \"C:\\Users\\slh\\AppData\\Roaming\\Python\\Python310\\site-packages\\ipykernel\\kernelbase.py\", line 437, in dispatch_shell\n",
            "    await result\n",
            "  File \"C:\\Users\\slh\\AppData\\Roaming\\Python\\Python310\\site-packages\\ipykernel\\ipkernel.py\", line 362, in execute_request\n",
            "    await super().execute_request(stream, ident, parent)\n",
            "  File \"C:\\Users\\slh\\AppData\\Roaming\\Python\\Python310\\site-packages\\ipykernel\\kernelbase.py\", line 778, in execute_request\n",
            "    reply_content = await reply_content\n",
            "  File \"C:\\Users\\slh\\AppData\\Roaming\\Python\\Python310\\site-packages\\ipykernel\\ipkernel.py\", line 449, in do_execute\n",
            "    res = shell.run_cell(\n",
            "  File \"C:\\Users\\slh\\AppData\\Roaming\\Python\\Python310\\site-packages\\ipykernel\\zmqshell.py\", line 549, in run_cell\n",
            "    return super().run_cell(*args, **kwargs)\n",
            "  File \"C:\\Users\\slh\\AppData\\Roaming\\Python\\Python310\\site-packages\\IPython\\core\\interactiveshell.py\", line 3075, in run_cell\n",
            "    result = self._run_cell(\n",
            "  File \"C:\\Users\\slh\\AppData\\Roaming\\Python\\Python310\\site-packages\\IPython\\core\\interactiveshell.py\", line 3130, in _run_cell\n",
            "    result = runner(coro)\n",
            "  File \"C:\\Users\\slh\\AppData\\Roaming\\Python\\Python310\\site-packages\\IPython\\core\\async_helpers.py\", line 128, in _pseudo_sync_runner\n",
            "    coro.send(None)\n",
            "  File \"C:\\Users\\slh\\AppData\\Roaming\\Python\\Python310\\site-packages\\IPython\\core\\interactiveshell.py\", line 3334, in run_cell_async\n",
            "    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n",
            "  File \"C:\\Users\\slh\\AppData\\Roaming\\Python\\Python310\\site-packages\\IPython\\core\\interactiveshell.py\", line 3517, in run_ast_nodes\n",
            "    if await self.run_code(code, result, async_=asy):\n",
            "  File \"C:\\Users\\slh\\AppData\\Roaming\\Python\\Python310\\site-packages\\IPython\\core\\interactiveshell.py\", line 3577, in run_code\n",
            "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
            "  File \"D:\\Temp\\ipykernel_140184\\2630202541.py\", line 2, in <module>\n",
            "    model = LSTMTaggerClassifer(\n",
            "  File \"D:\\Temp\\ipykernel_140184\\3243235020.py\", line 23, in __init__\n",
            "    self.lstm = nn.LSTM(\n",
            "  File \"c:\\Users\\slh\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\rnn.py\", line 711, in __init__\n",
            "    super().__init__('LSTM', *args, **kwargs)\n",
            "  File \"c:\\Users\\slh\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\rnn.py\", line 98, in __init__\n",
            "    w_ih = Parameter(torch.empty((gate_size, layer_input_size), **factory_kwargs))\n",
            "c:\\Users\\slh\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\rnn.py:98: UserWarning: Failed to initialize NumPy: _ARRAY_API not found (Triggered internally at ..\\torch\\csrc\\utils\\tensor_numpy.cpp:84.)\n",
            "  w_ih = Parameter(torch.empty((gate_size, layer_input_size), **factory_kwargs))\n"
          ]
        }
      ],
      "source": [
        "# Create model\n",
        "model = LSTMTaggerClassifer(\n",
        "    input_dim=train_dataset.get_vector_dim(),\n",
        "    hidden_dim=256,\n",
        "    tag_vocab_size=train_dataset.get_tag_vocab_size(),\n",
        "    sql_vocab_size=train_dataset.get_sql_vocab_size(),\n",
        "    num_layers=3\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HNnGVbIy9zXT",
        "outputId": "1718f0f7-585b-4860-fc78-69689cb4d1da"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 1/50: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 136/136 [00:03<00:00, 38.40it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/50, Training Loss: 7.6358\n",
            "Validation - Tag Acc: 0.8616, SQL Acc: 0.2243\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 2/50: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 136/136 [00:03<00:00, 40.50it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 2/50, Training Loss: 6.1835\n",
            "Validation - Tag Acc: 0.8893, SQL Acc: 0.3292\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 3/50: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 136/136 [00:03<00:00, 39.80it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 3/50, Training Loss: 5.2792\n",
            "Validation - Tag Acc: 0.9123, SQL Acc: 0.3827\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 4/50: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 136/136 [00:03<00:00, 41.26it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 4/50, Training Loss: 4.5304\n",
            "Validation - Tag Acc: 0.9217, SQL Acc: 0.4403\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 5/50: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 136/136 [00:03<00:00, 41.00it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 5/50, Training Loss: 3.8765\n",
            "Validation - Tag Acc: 0.9366, SQL Acc: 0.4835\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 6/50: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 136/136 [00:03<00:00, 40.75it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 6/50, Training Loss: 3.3218\n",
            "Validation - Tag Acc: 0.9395, SQL Acc: 0.5247\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 7/50: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 136/136 [00:03<00:00, 41.37it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 7/50, Training Loss: 2.8018\n",
            "Validation - Tag Acc: 0.9447, SQL Acc: 0.5617\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 8/50: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 136/136 [00:03<00:00, 41.09it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 8/50, Training Loss: 2.3726\n",
            "Validation - Tag Acc: 0.9461, SQL Acc: 0.5782\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 9/50: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 136/136 [00:03<00:00, 40.90it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 9/50, Training Loss: 1.9984\n",
            "Validation - Tag Acc: 0.9465, SQL Acc: 0.6008\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 10/50: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 136/136 [00:03<00:00, 40.60it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 10/50, Training Loss: 1.6436\n",
            "Validation - Tag Acc: 0.9519, SQL Acc: 0.5967\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 11/50: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 136/136 [00:03<00:00, 41.61it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 11/50, Training Loss: 1.3707\n",
            "Validation - Tag Acc: 0.9523, SQL Acc: 0.6235\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 12/50: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 136/136 [00:03<00:00, 40.10it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 12/50, Training Loss: 1.1697\n",
            "Validation - Tag Acc: 0.9550, SQL Acc: 0.6564\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 13/50: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 136/136 [00:03<00:00, 39.87it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 13/50, Training Loss: 0.9829\n",
            "Validation - Tag Acc: 0.9581, SQL Acc: 0.6420\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 14/50: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 136/136 [00:03<00:00, 39.75it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 14/50, Training Loss: 0.8259\n",
            "Validation - Tag Acc: 0.9548, SQL Acc: 0.6543\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 15/50: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 136/136 [00:03<00:00, 39.29it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 15/50, Training Loss: 0.7249\n",
            "Validation - Tag Acc: 0.9588, SQL Acc: 0.6523\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 16/50: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 136/136 [00:03<00:00, 39.48it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 16/50, Training Loss: 0.6101\n",
            "Validation - Tag Acc: 0.9615, SQL Acc: 0.6667\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 17/50: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 136/136 [00:03<00:00, 39.34it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 17/50, Training Loss: 0.5471\n",
            "Validation - Tag Acc: 0.9621, SQL Acc: 0.6749\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 18/50: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 136/136 [00:03<00:00, 40.33it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 18/50, Training Loss: 0.4869\n",
            "Validation - Tag Acc: 0.9641, SQL Acc: 0.6584\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 19/50: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 136/136 [00:03<00:00, 40.25it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 19/50, Training Loss: 0.4303\n",
            "Validation - Tag Acc: 0.9656, SQL Acc: 0.6584\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 20/50: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 136/136 [00:03<00:00, 40.32it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 20/50, Training Loss: 0.3877\n",
            "Validation - Tag Acc: 0.9662, SQL Acc: 0.6708\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 21/50: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 136/136 [00:03<00:00, 40.08it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 21/50, Training Loss: 0.3467\n",
            "Validation - Tag Acc: 0.9648, SQL Acc: 0.6605\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 22/50: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 136/136 [00:03<00:00, 40.16it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 22/50, Training Loss: 0.3076\n",
            "Validation - Tag Acc: 0.9643, SQL Acc: 0.6728\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 23/50: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 136/136 [00:03<00:00, 41.03it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 23/50, Training Loss: 0.2649\n",
            "Validation - Tag Acc: 0.9679, SQL Acc: 0.6687\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 24/50: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 136/136 [00:03<00:00, 40.65it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 24/50, Training Loss: 0.2359\n",
            "Validation - Tag Acc: 0.9646, SQL Acc: 0.6872\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 25/50: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 136/136 [00:03<00:00, 39.43it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 25/50, Training Loss: 0.2114\n",
            "Validation - Tag Acc: 0.9652, SQL Acc: 0.6749\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 26/50: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 136/136 [00:03<00:00, 40.20it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 26/50, Training Loss: 0.2268\n",
            "Validation - Tag Acc: 0.9646, SQL Acc: 0.6728\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 27/50: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 136/136 [00:03<00:00, 39.39it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 27/50, Training Loss: 0.2082\n",
            "Validation - Tag Acc: 0.9660, SQL Acc: 0.6770\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 28/50: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 136/136 [00:03<00:00, 39.36it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 28/50, Training Loss: 0.1918\n",
            "Validation - Tag Acc: 0.9656, SQL Acc: 0.6770\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 29/50: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 136/136 [00:03<00:00, 38.11it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 29/50, Training Loss: 0.1945\n",
            "Validation - Tag Acc: 0.9658, SQL Acc: 0.6502\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 30/50: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 136/136 [00:03<00:00, 40.17it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 30/50, Training Loss: 0.2198\n",
            "Validation - Tag Acc: 0.9656, SQL Acc: 0.6543\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 31/50: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 136/136 [00:03<00:00, 39.76it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 31/50, Training Loss: 0.1722\n",
            "Validation - Tag Acc: 0.9633, SQL Acc: 0.6708\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 32/50: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 136/136 [00:03<00:00, 40.00it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 32/50, Training Loss: 0.1333\n",
            "Validation - Tag Acc: 0.9668, SQL Acc: 0.6667\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 33/50: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 136/136 [00:03<00:00, 39.96it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 33/50, Training Loss: 0.1152\n",
            "Validation - Tag Acc: 0.9679, SQL Acc: 0.6790\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 34/50: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 136/136 [00:03<00:00, 40.33it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 34/50, Training Loss: 0.0988\n",
            "Validation - Tag Acc: 0.9671, SQL Acc: 0.6934\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 35/50: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 136/136 [00:03<00:00, 39.84it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 35/50, Training Loss: 0.0902\n",
            "Validation - Tag Acc: 0.9702, SQL Acc: 0.6852\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 36/50: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 136/136 [00:03<00:00, 38.02it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 36/50, Training Loss: 0.0770\n",
            "Validation - Tag Acc: 0.9695, SQL Acc: 0.6934\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 37/50: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 136/136 [00:03<00:00, 39.41it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 37/50, Training Loss: 0.0648\n",
            "Validation - Tag Acc: 0.9714, SQL Acc: 0.6996\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 38/50: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 136/136 [00:03<00:00, 39.01it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 38/50, Training Loss: 0.0804\n",
            "Validation - Tag Acc: 0.9660, SQL Acc: 0.6872\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 39/50: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 136/136 [00:03<00:00, 39.63it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 39/50, Training Loss: 0.1193\n",
            "Validation - Tag Acc: 0.9675, SQL Acc: 0.6687\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 40/50: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 136/136 [00:03<00:00, 40.45it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 40/50, Training Loss: 0.1456\n",
            "Validation - Tag Acc: 0.9637, SQL Acc: 0.6749\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 41/50: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 136/136 [00:03<00:00, 40.04it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 41/50, Training Loss: 0.1213\n",
            "Validation - Tag Acc: 0.9687, SQL Acc: 0.6564\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 42/50: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 136/136 [00:03<00:00, 39.76it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 42/50, Training Loss: 0.1201\n",
            "Validation - Tag Acc: 0.9666, SQL Acc: 0.6831\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 43/50: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 136/136 [00:03<00:00, 40.22it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 43/50, Training Loss: 0.0781\n",
            "Validation - Tag Acc: 0.9668, SQL Acc: 0.6790\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 44/50: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 136/136 [00:03<00:00, 39.82it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 44/50, Training Loss: 0.0782\n",
            "Validation - Tag Acc: 0.9693, SQL Acc: 0.6893\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 45/50: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 136/136 [00:03<00:00, 40.30it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 45/50, Training Loss: 0.0554\n",
            "Validation - Tag Acc: 0.9714, SQL Acc: 0.6914\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 46/50: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 136/136 [00:03<00:00, 40.48it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 46/50, Training Loss: 0.0437\n",
            "Validation - Tag Acc: 0.9679, SQL Acc: 0.6893\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 47/50: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 136/136 [00:03<00:00, 40.15it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 47/50, Training Loss: 0.0357\n",
            "Validation - Tag Acc: 0.9720, SQL Acc: 0.6934\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 48/50: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 136/136 [00:03<00:00, 40.05it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 48/50, Training Loss: 0.0356\n",
            "Validation - Tag Acc: 0.9708, SQL Acc: 0.6975\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 49/50: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 136/136 [00:03<00:00, 40.95it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 49/50, Training Loss: 0.0304\n",
            "Validation - Tag Acc: 0.9689, SQL Acc: 0.6934\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 50/50: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 136/136 [00:03<00:00, 41.40it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 50/50, Training Loss: 0.0310\n",
            "Validation - Tag Acc: 0.9681, SQL Acc: 0.6934\n"
          ]
        }
      ],
      "source": [
        "# Train model\n",
        "trained_model = train_model(\n",
        "    model=model,\n",
        "    train_loader=train_loader,\n",
        "    val_loader=val_loader,\n",
        "    epochs=50,\n",
        "    lr=1e-3\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y_qCs7HB9fPz",
        "outputId": "e6f96c67-a416-4921-86d0-fc1ec464c84c"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'tag_loss': 0.17478251636892125,\n",
              " 'sql_loss': 3.2299980548183616,\n",
              " 'tag_acc': 0.9681159420289855,\n",
              " 'sql_acc': 0.6934156378600823,\n",
              " 'sql_match_acc': 0.448559670781893}"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Evaluating model on validation set\n",
        "evaluate_model(trained_model, val_loader,dataset=val_dataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b_tvTzl99j1J",
        "outputId": "c45144c9-64d4-4dbf-f093-a0256f0005d5"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'tag_loss': 0.24567377020023065,\n",
              " 'sql_loss': 7.839518092622693,\n",
              " 'tag_acc': 0.965818363273453,\n",
              " 'sql_acc': 0.465324384787472,\n",
              " 'sql_match_acc': 0.33557046979865773}"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Evaluating model on test set\n",
        "evaluate_model(trained_model, test_loader,dataset=test_dataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Lp7RKIfi-Fzc"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
