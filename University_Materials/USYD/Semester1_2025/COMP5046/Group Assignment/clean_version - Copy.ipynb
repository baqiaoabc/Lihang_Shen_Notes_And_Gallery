{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e600cb0c",
   "metadata": {},
   "source": [
    "# Step0: Set up Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1df3c4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.2 -> 25.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting en-core-web-lg==3.8.0\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_lg-3.8.0/en_core_web_lg-3.8.0-py3-none-any.whl (400.7 MB)\n",
      "     ---------------------------------------- 0.0/400.7 MB ? eta -:--:--\n",
      "      -------------------------------------- 5.8/400.7 MB 32.0 MB/s eta 0:00:13\n",
      "     - ------------------------------------ 13.1/400.7 MB 32.9 MB/s eta 0:00:12\n",
      "     - ------------------------------------ 20.7/400.7 MB 33.6 MB/s eta 0:00:12\n",
      "     -- ----------------------------------- 27.5/400.7 MB 32.9 MB/s eta 0:00:12\n",
      "     --- ---------------------------------- 34.9/400.7 MB 33.6 MB/s eta 0:00:11\n",
      "     ---- --------------------------------- 42.5/400.7 MB 33.8 MB/s eta 0:00:11\n",
      "     ---- --------------------------------- 49.8/400.7 MB 34.1 MB/s eta 0:00:11\n",
      "     ----- -------------------------------- 57.4/400.7 MB 34.2 MB/s eta 0:00:11\n",
      "     ------ ------------------------------- 64.7/400.7 MB 34.1 MB/s eta 0:00:10\n",
      "     ------ ------------------------------- 72.4/400.7 MB 34.4 MB/s eta 0:00:10\n",
      "     ------- ------------------------------ 79.7/400.7 MB 34.3 MB/s eta 0:00:10\n",
      "     -------- ----------------------------- 87.3/400.7 MB 34.4 MB/s eta 0:00:10\n",
      "     -------- ----------------------------- 94.6/400.7 MB 34.5 MB/s eta 0:00:09\n",
      "     --------- --------------------------- 102.2/400.7 MB 34.5 MB/s eta 0:00:09\n",
      "     ---------- -------------------------- 109.8/400.7 MB 34.5 MB/s eta 0:00:09\n",
      "     ---------- -------------------------- 117.2/400.7 MB 34.6 MB/s eta 0:00:09\n",
      "     ----------- ------------------------- 124.8/400.7 MB 34.6 MB/s eta 0:00:08\n",
      "     ------------ ------------------------ 132.1/400.7 MB 34.7 MB/s eta 0:00:08\n",
      "     ------------ ------------------------ 139.5/400.7 MB 34.7 MB/s eta 0:00:08\n",
      "     ------------- ----------------------- 147.1/400.7 MB 34.7 MB/s eta 0:00:08\n",
      "     -------------- ---------------------- 154.1/400.7 MB 34.7 MB/s eta 0:00:08\n",
      "     -------------- ---------------------- 161.5/400.7 MB 34.6 MB/s eta 0:00:07\n",
      "     --------------- --------------------- 169.1/400.7 MB 34.7 MB/s eta 0:00:07\n",
      "     ---------------- -------------------- 176.4/400.7 MB 34.7 MB/s eta 0:00:07\n",
      "     ---------------- -------------------- 183.8/400.7 MB 34.7 MB/s eta 0:00:07\n",
      "     ----------------- ------------------- 191.4/400.7 MB 34.7 MB/s eta 0:00:07\n",
      "     ------------------ ------------------ 198.7/400.7 MB 34.8 MB/s eta 0:00:06\n",
      "     ------------------- ----------------- 206.3/400.7 MB 34.8 MB/s eta 0:00:06\n",
      "     ------------------- ----------------- 213.9/400.7 MB 34.8 MB/s eta 0:00:06\n",
      "     -------------------- ---------------- 221.0/400.7 MB 34.7 MB/s eta 0:00:06\n",
      "     --------------------- --------------- 228.3/400.7 MB 34.8 MB/s eta 0:00:05\n",
      "     --------------------- --------------- 235.7/400.7 MB 34.7 MB/s eta 0:00:05\n",
      "     ---------------------- -------------- 243.3/400.7 MB 34.8 MB/s eta 0:00:05\n",
      "     ----------------------- ------------- 250.6/400.7 MB 34.8 MB/s eta 0:00:05\n",
      "     ----------------------- ------------- 258.2/400.7 MB 34.8 MB/s eta 0:00:05\n",
      "     ------------------------ ------------ 265.6/400.7 MB 34.8 MB/s eta 0:00:04\n",
      "     ------------------------- ----------- 272.9/400.7 MB 34.9 MB/s eta 0:00:04\n",
      "     ------------------------- ----------- 280.5/400.7 MB 34.9 MB/s eta 0:00:04\n",
      "     -------------------------- ---------- 288.1/400.7 MB 35.0 MB/s eta 0:00:04\n",
      "     --------------------------- --------- 295.4/400.7 MB 35.1 MB/s eta 0:00:04\n",
      "     --------------------------- --------- 302.5/400.7 MB 35.1 MB/s eta 0:00:03\n",
      "     ---------------------------- -------- 310.6/400.7 MB 35.0 MB/s eta 0:00:03\n",
      "     ----------------------------- ------- 318.0/400.7 MB 35.0 MB/s eta 0:00:03\n",
      "     ------------------------------ ------ 325.6/400.7 MB 35.1 MB/s eta 0:00:03\n",
      "     ------------------------------ ------ 332.9/400.7 MB 35.0 MB/s eta 0:00:02\n",
      "     ------------------------------- ----- 340.5/400.7 MB 35.0 MB/s eta 0:00:02\n",
      "     -------------------------------- ---- 347.1/400.7 MB 35.0 MB/s eta 0:00:02\n",
      "     -------------------------------- ---- 353.4/400.7 MB 34.7 MB/s eta 0:00:02\n",
      "     --------------------------------- --- 360.7/400.7 MB 34.8 MB/s eta 0:00:02\n",
      "     --------------------------------- --- 367.3/400.7 MB 34.8 MB/s eta 0:00:01\n",
      "     ---------------------------------- -- 372.8/400.7 MB 34.3 MB/s eta 0:00:01\n",
      "     ---------------------------------- -- 378.0/400.7 MB 34.3 MB/s eta 0:00:01\n",
      "     ----------------------------------- - 384.6/400.7 MB 34.0 MB/s eta 0:00:01\n",
      "     ----------------------------------- - 384.6/400.7 MB 34.0 MB/s eta 0:00:01\n",
      "     ----------------------------------- - 384.6/400.7 MB 34.0 MB/s eta 0:00:01\n",
      "     ------------------------------------  392.4/400.7 MB 32.3 MB/s eta 0:00:01\n",
      "     ------------------------------------  399.8/400.7 MB 32.3 MB/s eta 0:00:01\n",
      "     ------------------------------------  400.6/400.7 MB 32.2 MB/s eta 0:00:01\n",
      "     ------------------------------------  400.6/400.7 MB 32.2 MB/s eta 0:00:01\n",
      "     ------------------------------------- 400.7/400.7 MB 30.7 MB/s eta 0:00:00\n",
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('en_core_web_lg')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.2 -> 25.0.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'torchtext'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 20\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mcollections\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Counter\n\u001b[0;32m     19\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Dataset, DataLoader\n\u001b[1;32m---> 20\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorchtext\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mvocab\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m build_vocab_from_iterator\n\u001b[0;32m     21\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorchtext\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m get_tokenizer\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'torchtext'"
     ]
    }
   ],
   "source": [
    "!pip install spacy --quiet\n",
    "!pip install torchtext\n",
    "!python -m spacy download en_core_web_lg\n",
    "\n",
    "import json\n",
    "import re\n",
    "import unicodedata\n",
    "import spacy\n",
    "from spacy.language import Language\n",
    "from spacy.tokens import Doc\n",
    "from spacy.tokenizer import Tokenizer\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from collections import Counter\n",
    "import torch\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "import re\n",
    "import spacy\n",
    "from collections import Counter\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "\n",
    "#====================================================================================\n",
    "# 设置device\n",
    "device = 'cpu'\n",
    "if torch.cuda.is_available():\n",
    "    device = \"cuda\"\n",
    "elif torch.backends.mps.is_available():\n",
    "    pass\n",
    "    #device = \"mps\"\n",
    "print(f\"Using {device} device\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38a5d237",
   "metadata": {},
   "source": [
    "# Step1: Methods Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d013398",
   "metadata": {},
   "source": [
    "For this task, I need to implement LSTM for Encoder and Decoder. \n",
    "\n",
    "1. Data pre-processing\n",
    "    - Read data from a JSON file and split it into query and question datasets, each with train, dev, and test splits.\n",
    "\n",
    "2. Create a Spacy tokenizer to process the input and output sentences.\n",
    "    - maybe need to consider about vocab\n",
    "\n",
    "3. Data Loader:\n",
    "    - Use the tokenizer to tokenize each input sentence and its corresponding label, adding the following special tokens: `<sos>`, `<eos>`, and `<pad>`.\n",
    "    - Pad the tokenized sentences so that each batch has the same length.\n",
    "\n",
    "4. Define the LSTM Encoder and Decoder:\n",
    "    - Implement an Encoder and a Decoder using LSTM.\n",
    "    - Combine the Encoder and Decoder into a sequence-to-sequence (seq2seq) model.\n",
    "    - Optionally, use teacher forcing during training to enhance convergence.\n",
    "    - Optionally, use bidirectional or more layers.\n",
    "\n",
    "5. Define the training method:\n",
    "    - Perform the feedforward step.\n",
    "    - Calculate the loss between the predicted output and the target labels.\n",
    "    - Perform backpropagation to compute the gradients.\n",
    "    - Apply gradient clipping to prevent exploding gradients.\n",
    "    - Record and print the loss in the terminal during training.\n",
    "\n",
    "6. Define the testing method to evaluate the model's performance on the test dataset.\n",
    "\n",
    "7. Set the hyperparameters for the seq2seq model, such as:\n",
    "    - Learning rate\n",
    "    - Batch size\n",
    "    - Number of epochs\n",
    "    - Hidden dimensions\n",
    "\n",
    "8. Train the model using both the question and query training datasets.\n",
    "\n",
    "9. Test the model using both the question and query testing datasets.\n",
    "    - Remember to ignore `<sos>`, `<eos>`, and `<pad>`.\n",
    "    - Remember not only shortest sql query is valid."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23628e40",
   "metadata": {},
   "source": [
    "# Step 2: Pre-process Raw Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "196ad310",
   "metadata": {},
   "outputs": [],
   "source": [
    "def unicodeToAscii(s):\n",
    "    # Convert a Unicode string 's' to plain ASCII.\n",
    "    # This is done by first normalizing the string into its decomposed form using 'NFD',\n",
    "    # which separates characters from their accents. Then, it filters out all nonspacing marks (Mn).\n",
    "    return ''.join(\n",
    "        c for c in unicodedata.normalize('NFD', s)\n",
    "        if unicodedata.category(c) != 'Mn'\n",
    "    )\n",
    "\n",
    "# Does not allow sql to have multiple space between each word\n",
    "def normalize_whitespace(text):\n",
    "    return re.sub(r'\\s+', ' ', text).strip()\n",
    "\n",
    "def preprocess_sentence(s:str) -> str:\n",
    "    \"\"\"\n",
    "    Preprocesses sentence text for consistency\n",
    "    \"\"\"\n",
    "    s = s.strip()\n",
    "    s = normalize_whitespace(s)\n",
    "    s = unicodeToAscii(s)\n",
    "    s = s.strip()\n",
    "    return s\n",
    "\n",
    "def preprocess_dataset(dataset_loc = \"atis.json\",split_type=None, split=['dev', 'test', 'train']):\n",
    "    \n",
    "    # Read Dataset JSON file\n",
    "    with open(dataset_loc) as f:\n",
    "        dataset_json = json.load(f)\n",
    "\n",
    "    processed_dataset = []\n",
    "    variable_names = set()\n",
    "    sql_templates = set()\n",
    "\n",
    "    for sample in dataset_json:\n",
    "        processed_sample = {}\n",
    "\n",
    "        # Preprocess sql queries\n",
    "        sql = [preprocess_sentence(query) for query in sample['sql']]\n",
    "\n",
    "        # All valid sql queries for this examples sorted by their length\n",
    "        sql = sorted(sql, key=len)\n",
    "\n",
    "        # Adds shorests sql template to the set of sql templates\n",
    "        sql_templates.add(sql[0])\n",
    "        \n",
    "\n",
    "        # Dictionary for variables/placeholders metadata\n",
    "        variables_metadata = sample[\"variables\"]\n",
    "\n",
    "        # Delete 'location' key from variables dictionary\n",
    "        # variable_type_mapping = {var['name']:var['type'] for var in variables_metadata}\n",
    "        for var in variables_metadata:\n",
    "            # Add current variable to set of all possible variable names\n",
    "            variable_names.add(var.get(\"name\"))\n",
    "            var.pop('location', None)\n",
    "        # query split for this sample\n",
    "        query_split = sample['query-split']\n",
    "\n",
    "        # Skips sample if its not the specified split_type or split\n",
    "        if(split_type == \"query\"):\n",
    "            if(query_split not in split):\n",
    "                continue\n",
    "\n",
    "        # Process each sentence\n",
    "        for sentence in sample['sentences']:\n",
    "            # Skips sample if its not the specified split_type or split\n",
    "            if(split_type == \"question\"):\n",
    "                if(sentence['question-split'] not in split):\n",
    "                    continue\n",
    "            # variables/placeholder mapping dictionary\n",
    "            variables = sentence['variables']\n",
    "\n",
    "            # Sentence text with variables/placeholders\n",
    "            text_with_vars = preprocess_sentence(sentence['text'])\n",
    "\n",
    "            # Replacing variables/placeholders in current sentence and sql query with their values from the variables dictionary\n",
    "            text_with_vars_replaced = text_with_vars\n",
    "            sql_with_vars_replaced = sql\n",
    "\n",
    "            # Replace sentence and all sql variables with their values\n",
    "            for var in variables:\n",
    "                text_with_vars_replaced = text_with_vars_replaced.replace(var,variables[var])\n",
    "                sql_with_vars_replaced = [query.replace(var,variables[var]) for query in sql_with_vars_replaced]\n",
    "\n",
    "            # Taggingg expected output\n",
    "            sentence_var_tagging_labels = []\n",
    "            for word in text_with_vars.split():\n",
    "                if(word in variables):\n",
    "                    sentence_var_tagging_labels.append(word)\n",
    "                else:\n",
    "                    sentence_var_tagging_labels.append(\"-\")\n",
    "\n",
    "            # Appends preprocessed dictionary of current sentence to the processesed_dataset list\n",
    "            processed_dataset.append({\n",
    "                \"text_with_vars\":text_with_vars,\n",
    "                \"text_with_vars_replaced\":text_with_vars_replaced,\n",
    "                \"sentence_var_tagging_labels\":sentence_var_tagging_labels,\n",
    "                \"vars_metadata\":variables_metadata,\n",
    "                \"variables\":variables,\n",
    "                \"sql_with_vars\": sql,\n",
    "                \"shortest_sql_with_vars\":sql[0],\n",
    "                \"sql_with_vars_replaced\": sql_with_vars_replaced,\n",
    "                \"shortest_sql_with_vars_replaced\":sql_with_vars_replaced[0],\n",
    "                \"query_split\":sample['query-split'],\n",
    "                \"question_split\":sentence['question-split']\n",
    "            })\n",
    "    \n",
    "    return processed_dataset,variable_names,sql_templates"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "280e7b2c",
   "metadata": {},
   "source": [
    "# Step 3: Create spacy tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60185748",
   "metadata": {},
   "outputs": [],
   "source": [
    "#====================================================================================\n",
    "# Define tokenizer\n",
    "# 这会加载默认的 spaCy NLP 管道，包括 tokenizer、tagger、parser、ner 等\n",
    "nlp = spacy.load('en_core_web_lg')\n",
    "\n",
    "# Create a custom component to merge entities\n",
    "@Language.component(\"entity_merger\")\n",
    "def entity_merger(doc):\n",
    "    \"\"\"\n",
    "    Custom component of the spacy nlp pipeline which merges geographical location entity tokens into a single token\n",
    "    For example: 'New York' would noramlly be split into 2 tokens 'New' and 'York' but this will combine into a single 'New York' token\n",
    "    This is implemented because city_name type variables could have the value 'New York' and for effective tagging we aim to keep the tokenisation scheme consistent to the dataset\n",
    "    \"\"\"\n",
    "    # Iterate over the entities in reverse order (to avoid index issues when merging)\n",
    "    with doc.retokenize() as retokenizer:\n",
    "        for ent in reversed(list(doc.ents)):\n",
    "            # Merge the entity tokens into one token\n",
    "            if(ent.label_ in [\"GPE\"]):\n",
    "                attrs = {\"LEMMA\": ent.text}\n",
    "                retokenizer.merge(ent, attrs=attrs)\n",
    "    return doc\n",
    "\n",
    "# Add the custom component after NER\n",
    "nlp.add_pipe(\"entity_merger\", after=\"ner\")\n",
    "\n",
    "#====================================================================================\n",
    "# use embedded vocab for en_core_web_lg\n",
    "vector_size=300\n",
    "\n",
    "def sentence_to_tensor(sentence, vector_size=vector_size):\n",
    "    doc = nlp(sentence)\n",
    "    vectors = []\n",
    "    for token in doc:\n",
    "        if token.has_vector:\n",
    "            vectors.append(torch.tensor(token.vector))\n",
    "        else:\n",
    "            vectors.append(torch.zeros(vector_size))\n",
    "    return torch.stack(vectors)  # [seq_len, 300]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d5a57fe",
   "metadata": {},
   "source": [
    "# Step 4: Data Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "626a492d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Dataloader\n",
    "class generationDataset(Dataset):\n",
    "    def __init__(self, data, nlp, max_len = 200):\n",
    "        self.data = data\n",
    "        self.nlp = nlp\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        input_sentence = self.data[idx][\"text_with_vars_replaced\"]\n",
    "        target_sql_query = self.data[idx][\"shortest_sql_with_vars_replaced\"]\n",
    "        input_vectors = sentence_to_tensor(input_sentence)  # 不 pad\n",
    "        target_vectors = sentence_to_tensor(target_sql_query)  # 不 pad\n",
    "        return input_vectors, target_vectors\n",
    "\n",
    "sos_vector = torch.ones(vector_size)      # <sos> 向量，用 1 表示\n",
    "eos_vector = -torch.ones(vector_size)     # <eos> 向量，用 -1 表示\n",
    "pad_vector = torch.zeros(vector_size)  # padding 使用全零向量\n",
    "\n",
    "# Define collate function for each batch\n",
    "def collate_fn(batch):\n",
    "    input_seqs, target_seqs = zip(*batch)  # batch 是 [(input1, target1), (input2, target2), ...]\n",
    "    \n",
    "    # 计算输入和目标的长度\n",
    "    input_lengths = [seq.size(0) for seq in input_seqs]\n",
    "    target_lengths = [seq.size(0) for seq in target_seqs]\n",
    "\n",
    "    max_input_len = max(input_lengths)\n",
    "    max_target_len = max(target_lengths)\n",
    "\n",
    "    # 添加 sos 和 eos 到每个输入和目标序列\n",
    "    padded_inputs = []\n",
    "    padded_targets = []\n",
    "\n",
    "    for seq in input_seqs:\n",
    "        # 添加 sos 和 eos\n",
    "        padded_seq = torch.cat([sos_vector, seq, eos_vector])\n",
    "        padded_inputs.append(padded_seq)\n",
    "\n",
    "    for seq in target_seqs:\n",
    "        # 添加 sos 和 eos\n",
    "        padded_seq = torch.cat([sos_vector, seq, eos_vector])\n",
    "        padded_targets.append(padded_seq)\n",
    "\n",
    "    # 对输入和目标进行 padding\n",
    "    padded_inputs = [torch.cat([seq, pad_vector.repeat(max_input_len - len(seq), 1)]) for seq in padded_inputs]\n",
    "    padded_targets = [torch.cat([seq, pad_vector.repeat(max_target_len - len(seq), 1)]) for seq in padded_targets]\n",
    "\n",
    "    # 将列表转换为张量\n",
    "    inputs_tensor = torch.stack(padded_inputs)  # [batch_size, max_input_len, 300]\n",
    "    targets_tensor = torch.stack(padded_targets)  # [batch_size, max_target_len, 300]\n",
    "\n",
    "    return inputs_tensor, targets_tensor, input_lengths, target_lengths"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6734f62",
   "metadata": {},
   "source": [
    "# Step 5: Build LSTM Encoder and Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea544af6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, emb_dim, hid_dim, n_layers=1, dropout=0.5):\n",
    "        super().__init__()\n",
    "        self.lstm = nn.LSTM(emb_dim, hid_dim, n_layers, dropout=dropout, batch_first=True, bidirectional=True)\n",
    "\n",
    "    def forward(self, src):  # src: [batch size, seq len, emb dim]\n",
    "        outputs, (hidden, cell) = self.lstm(src)\n",
    "\n",
    "        hidden = hidden.view(hidden.size(0) // 2, 2, hidden.size(1), hidden.size(2))\n",
    "        hidden = torch.cat((hidden[:, 0, :, :], hidden[:, 1, :, :]), dim=2)\n",
    "\n",
    "        cell = cell.view(cell.size(0) // 2, 2, cell.size(1), cell.size(2))\n",
    "        cell = torch.cat((cell[:, 0, :, :], cell[:, 1, :, :]), dim=2)\n",
    "\n",
    "        return hidden, cell\n",
    "\n",
    "\n",
    "# Define the Decoder\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, emb_dim, hid_dim, output_dim, n_layers=1, dropout=0.5):\n",
    "        super().__init__()\n",
    "        self.lstm = nn.LSTM(emb_dim, hid_dim * 2, n_layers, dropout=dropout, batch_first=True)\n",
    "        self.fc_out = nn.Linear(hid_dim * 2, output_dim)  # 输出词向量空间维度\n",
    "\n",
    "    def forward(self, input_vec, hidden, cell):  # input_vec: [batch size, emb dim]\n",
    "        input_vec = input_vec.unsqueeze(1)  # [batch size, 1, emb dim]\n",
    "        output, (hidden, cell) = self.lstm(input_vec, (hidden, cell))\n",
    "        prediction = self.fc_out(output.squeeze(1))  # [batch size, output dim]\n",
    "        return prediction, hidden, cell\n",
    "\n",
    "\n",
    "# Define the Seq2Seq model\n",
    "class Seq2Seq(nn.Module):\n",
    "    def __init__(self, encoder, decoder, device):\n",
    "        super().__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.device = device\n",
    "\n",
    "        # 提前加载 spaCy 词汇和词向量，以提高效率\n",
    "        self.words_vectors = {}\n",
    "        for word in nlp.vocab:\n",
    "            if word.has_vector:  # 只考虑有词向量的词\n",
    "                self.words_vectors[word.text] = word.vector\n",
    "    \n",
    "    def token_to_vector(self, word):\n",
    "        \"\"\"\n",
    "        给定预测的词，返回对应的词向量。\n",
    "        \"\"\"\n",
    "        doc = nlp(word)  # 使用spaCy获取词的向量\n",
    "        return torch.tensor(doc[0].vector)  # 返回 spaCy 的词向量\n",
    "\n",
    "    def forward(self, src, trg, teacher_forcing_ratio=0.5, mode='train'):\n",
    "        \"\"\"\n",
    "        训练和推理的前向传播方法。包括Teacher Forcing的使用。\n",
    "        mode: 'train' or 'inference'\n",
    "        \"\"\"\n",
    "        batch_size = src.shape[0]\n",
    "        trg_len = trg.shape[1]\n",
    "        trg_vocab_size = len(nlp.vocab)  # 词汇大小，取决于 spaCy 的词汇表\n",
    "\n",
    "        outputs = torch.zeros(batch_size, trg_len, trg_vocab_size).to(self.device)\n",
    "        \n",
    "        hidden, cell = self.encoder(src)  # 获取编码器的隐藏状态和细胞状态\n",
    "        \n",
    "        # Decoder的输入是目标序列的第一个词嵌入向量（一般是<sos> token）\n",
    "        input_vec = trg[:, 0, :]  # [batch size, emb dim]\n",
    "        \n",
    "        for t in range(1, trg_len):\n",
    "            output, hidden, cell = self.decoder(input_vec, hidden, cell)\n",
    "            outputs[:, t, :] = output\n",
    "            \n",
    "            if mode == 'train':\n",
    "                # 使用 Teacher Forcing 或预测结果作为下一个时间步的输入\n",
    "                top1 = output.argmax(1)  # 获取输出的最大值索引，代表预测的词ID\n",
    "                if torch.rand(1).item() < teacher_forcing_ratio:\n",
    "                    # 使用真实的目标词作为输入\n",
    "                    input_vec = trg[:, t, :]\n",
    "                else:\n",
    "                    # 使用模型预测的词作为输入\n",
    "                    predicted_word = nlp.vocab.strings[top1.item()]  # 将预测索引转换为词\n",
    "                    input_vec = self.token_to_vector(predicted_word)  # 根据预测词获取词向量\n",
    "            elif mode == 'inference':\n",
    "                # 推理模式：不使用 Teacher Forcing，而是直接使用模型预测的词\n",
    "                top1 = output.argmax(1)  # 获取输出的最大值索引，代表预测的词ID\n",
    "                predicted_word = nlp.vocab.strings[top1.item()]  # 将预测索引转换为词\n",
    "                input_vec = self.token_to_vector(predicted_word)  # 根据预测词获取词向量\n",
    "        \n",
    "        return outputs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4145242e",
   "metadata": {},
   "source": [
    "# Step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b86ca9da",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
