{
    "cells": [
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Workshop 9\n",
                "\n",
                "Today, we'll explore Pinecone, a vector database. The activities are based on [two tutorials](https://docs.pinecone.io/examples/notebooks) from the Pinecone developers.\n",
                "\n",
                "A vector database works by storing each item with a vector. When you want to query the database, you provide a vector and the database returns items that have similar vectors to your query. This is a key ingredient for efficient RAG systems.\n",
                "\n",
                "# Pre-Workshop Preparation\n",
                "\n",
                "If you haven't already, set up accounts with Pinecone and OpenAI:\n",
                "\n",
                "Pinecone - https://app.pinecone.io\n",
                "\n",
                "Once you create an account they will ask you a few questions to get it set up. Choose Python as the language. The rest you can answer however you like.\n",
                "\n",
                "OpenAI API - https://platform.openai.com/overview\n",
                "\n",
                "Note, you will need to add a credit/debit card to your account in order to pay for some of the services we use. The expenses should be very small (assuming you just do the activities in this lab)."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 17,
            "metadata": {},
            "outputs": [],
            "source": [
                "# api key from app.pinecone.io\n",
                "pinecone_api_key = 'INSERT_YOUR_PINECONE_KEY_HERE'\n",
                "\n",
                "# api key from platform.openai.com\n",
                "openai_api_key = 'INSERT_YOUR_OPENAI_KEY_HERE'"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Now, let's install the libraries we need:"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 18,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "\n",
                        "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1\u001b[0m\n",
                        "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
                    ]
                }
            ],
            "source": [
                "!pip install -qU \\\n",
                "  openai==0.27.7 \\\n",
                "  pinecone==6.0.2 \\\n",
                "  pinecone-datasets==1.0.2 \\\n",
                "  sentence-transformers==3.4.1 \\\n",
                "  pinecone-notebooks==0.1.1 \\\n",
                "  pyarrow \\\n",
                "  hf_xet \\\n",
                "  tqdm"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Note, some Windows users have also found that they needed to isntall `pyarrow-11.0.0`\n",
                "\n",
                "# In Workshop Activities\n",
                "\n",
                "## Data Download"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "In this notebook we will use a pre-processed dataset from Pinecone Datasets.\n",
                "\n",
                "If you are curious about what pre-processing they did. see [this notebook](https://github.com/pinecone-io/examples/blob/master/learn/search/semantic-search/semantic-search.ipynb)."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 19,
            "metadata": {},
            "outputs": [
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "Loading documents parquet files: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 10/10 [02:02<00:00, 12.26s/it]\n"
                    ]
                },
                {
                    "data": {
                        "text/html": [
                            "<div>\n",
                            "<style scoped>\n",
                            "    .dataframe tbody tr th:only-of-type {\n",
                            "        vertical-align: middle;\n",
                            "    }\n",
                            "\n",
                            "    .dataframe tbody tr th {\n",
                            "        vertical-align: top;\n",
                            "    }\n",
                            "\n",
                            "    .dataframe thead th {\n",
                            "        text-align: right;\n",
                            "    }\n",
                            "</style>\n",
                            "<table border=\"1\" class=\"dataframe\">\n",
                            "  <thead>\n",
                            "    <tr style=\"text-align: right;\">\n",
                            "      <th></th>\n",
                            "      <th>id</th>\n",
                            "      <th>values</th>\n",
                            "      <th>sparse_values</th>\n",
                            "      <th>metadata</th>\n",
                            "    </tr>\n",
                            "  </thead>\n",
                            "  <tbody>\n",
                            "    <tr>\n",
                            "      <th>240000</th>\n",
                            "      <td>515997</td>\n",
                            "      <td>[-0.00531694, 0.06937869, -0.0092854, 0.003286...</td>\n",
                            "      <td>{'indices': [845, 1657, 13677, 20780, 27058, 2...</td>\n",
                            "      <td>{'text': ' Why is a \"law of sciences\" importan...</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <th>240001</th>\n",
                            "      <td>515998</td>\n",
                            "      <td>[-0.09243751, 0.065432355, -0.06946959, 0.0669...</td>\n",
                            "      <td>{'indices': [2110, 6324, 9754, 13677, 15207, 2...</td>\n",
                            "      <td>{'text': ' Is it possible to format a BitLocke...</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <th>240002</th>\n",
                            "      <td>515999</td>\n",
                            "      <td>[-0.021924071, 0.032280188, -0.020190848, 0.07...</td>\n",
                            "      <td>{'indices': [2110, 4949, 23579, 23758, 27058, ...</td>\n",
                            "      <td>{'text': ' Can formatting a hard drive stress ...</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <th>240003</th>\n",
                            "      <td>516000</td>\n",
                            "      <td>[-0.120020054, 0.024080949, 0.10693012, -0.018...</td>\n",
                            "      <td>{'indices': [22014, 24734, 24773, 25791, 25991...</td>\n",
                            "      <td>{'text': ' Are the new Samsung Galaxy J7 and J...</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <th>240004</th>\n",
                            "      <td>516001</td>\n",
                            "      <td>[-0.095293395, -0.048446465, -0.017618902, -0....</td>\n",
                            "      <td>{'indices': [307, 2110, 5785, 12969, 12971, 13...</td>\n",
                            "      <td>{'text': ' I just watched an add for Indonesia...</td>\n",
                            "    </tr>\n",
                            "  </tbody>\n",
                            "</table>\n",
                            "</div>"
                        ],
                        "text/plain": [
                            "            id                                             values  \\\n",
                            "240000  515997  [-0.00531694, 0.06937869, -0.0092854, 0.003286...   \n",
                            "240001  515998  [-0.09243751, 0.065432355, -0.06946959, 0.0669...   \n",
                            "240002  515999  [-0.021924071, 0.032280188, -0.020190848, 0.07...   \n",
                            "240003  516000  [-0.120020054, 0.024080949, 0.10693012, -0.018...   \n",
                            "240004  516001  [-0.095293395, -0.048446465, -0.017618902, -0....   \n",
                            "\n",
                            "                                            sparse_values  \\\n",
                            "240000  {'indices': [845, 1657, 13677, 20780, 27058, 2...   \n",
                            "240001  {'indices': [2110, 6324, 9754, 13677, 15207, 2...   \n",
                            "240002  {'indices': [2110, 4949, 23579, 23758, 27058, ...   \n",
                            "240003  {'indices': [22014, 24734, 24773, 25791, 25991...   \n",
                            "240004  {'indices': [307, 2110, 5785, 12969, 12971, 13...   \n",
                            "\n",
                            "                                                 metadata  \n",
                            "240000  {'text': ' Why is a \"law of sciences\" importan...  \n",
                            "240001  {'text': ' Is it possible to format a BitLocke...  \n",
                            "240002  {'text': ' Can formatting a hard drive stress ...  \n",
                            "240003  {'text': ' Are the new Samsung Galaxy J7 and J...  \n",
                            "240004  {'text': ' I just watched an add for Indonesia...  "
                        ]
                    },
                    "execution_count": 19,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "from pinecone_datasets import load_dataset\n",
                "\n",
                "dataset = load_dataset('quora_all-MiniLM-L6-bm25')\n",
                "\n",
                "# we drop metadata as will use blob column\n",
                "dataset.documents.drop(['metadata'], axis=1, inplace=True)\n",
                "dataset.documents.rename(columns={'blob': 'metadata'}, inplace=True)\n",
                "\n",
                "# we will use 80K rows of the dataset between rows 240K -> 320K\n",
                "dataset.documents.drop(dataset.documents.index[320_000:], inplace=True)\n",
                "dataset.documents.drop(dataset.documents.index[:240_000], inplace=True)\n",
                "\n",
                "# Print out a sample from the dataset to show what we are working with\n",
                "dataset.head()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 20,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "80000\n"
                    ]
                }
            ],
            "source": [
                "print(len(dataset))"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Creating an Index\n",
                "\n",
                "Now the data is ready, we can set up our index to store it.\n",
                "\n",
                "We begin by initializing our connection to Pinecone. This is where your API key is needed."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 22,
            "metadata": {},
            "outputs": [],
            "source": [
                "import os\n",
                "from pinecone import Pinecone\n",
                "\n",
                "# configure client\n",
                "pc = Pinecone(api_key=pinecone_api_key)"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Now we set up our index specification. This allows us to define the cloud provider and region where we want to deploy our index. You can find a list of all available providers and regions [here](https://docs.pinecone.io/docs/projects)."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 23,
            "metadata": {},
            "outputs": [],
            "source": [
                "from pinecone import ServerlessSpec\n",
                "\n",
                "cloud = os.environ.get('PINECONE_CLOUD') or 'aws'\n",
                "region = os.environ.get('PINECONE_REGION') or 'us-east-1'\n",
                "\n",
                "spec = ServerlessSpec(cloud=cloud, region=region)"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Now we create a new index called `semantic-search-fast`. It's important that we align the index `dimension` and `metric` parameters with those required by the `MiniLM-L6` model."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 24,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/plain": [
                            "{'dimension': 384,\n",
                            " 'index_fullness': 0.0,\n",
                            " 'metric': 'dotproduct',\n",
                            " 'namespaces': {},\n",
                            " 'total_vector_count': 0,\n",
                            " 'vector_type': 'dense'}"
                        ]
                    },
                    "execution_count": 24,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "import time\n",
                "\n",
                "index_name = 'semantic-search-fast'\n",
                "\n",
                "existing_indexes = [\n",
                "    index_info[\"name\"] for index_info in pc.list_indexes()\n",
                "]\n",
                "\n",
                "# check if index already exists (it shouldn't if this is first time)\n",
                "if index_name not in existing_indexes:\n",
                "    # if does not exist, create index\n",
                "    pc.create_index(\n",
                "        index_name,\n",
                "        dimension=384,  # dimensionality of minilm\n",
                "        metric='dotproduct',\n",
                "        spec=spec\n",
                "    )\n",
                "    # wait for index to be initialized\n",
                "    while not pc.describe_index(index_name).status['ready']:\n",
                "        time.sleep(1)\n",
                "\n",
                "# connect to index\n",
                "index = pc.Index(index_name)\n",
                "time.sleep(1)\n",
                "# view index stats\n",
                "index.describe_index_stats()"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Upsert the data to put it in the database (this can take 2-5 minutes):"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 25,
            "metadata": {},
            "outputs": [
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 160/160 [06:33<00:00,  2.46s/it]\n"
                    ]
                }
            ],
            "source": [
                "from tqdm.auto import tqdm\n",
                "\n",
                "for batch in tqdm(dataset.iter_documents(batch_size=500), total=160):\n",
                "    index.upsert(batch)"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Making Queries"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Now that our index is populated we can begin making queries. We are performing a semantic search for *similar questions*, so we should embed and search with another question.\n",
                "\n",
                "Note that we use the same model as the one used above. That's critical - otherwise the vector spaces will not be meaningfully comparable."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 26,
            "metadata": {},
            "outputs": [
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
                    ]
                },
                {
                    "data": {
                        "text/plain": [
                            "SentenceTransformer(\n",
                            "  (0): Transformer({'max_seq_length': 256, 'do_lower_case': False}) with Transformer model: BertModel \n",
                            "  (1): Pooling({'word_embedding_dimension': 384, 'pooling_mode_cls_token': False, 'pooling_mode_mean_tokens': True, 'pooling_mode_max_tokens': False, 'pooling_mode_mean_sqrt_len_tokens': False, 'pooling_mode_weightedmean_tokens': False, 'pooling_mode_lasttoken': False, 'include_prompt': True})\n",
                            "  (2): Normalize()\n",
                            ")"
                        ]
                    },
                    "execution_count": 26,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "from sentence_transformers import SentenceTransformer\n",
                "import torch\n",
                "\n",
                "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
                "\n",
                "model = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2', device=device)\n",
                "model"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Now let's query."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 27,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/plain": [
                            "{'matches': [{'id': '69331',\n",
                            "              'metadata': {'text': \" What's the world's largest city?\"},\n",
                            "              'score': 0.785789192,\n",
                            "              'values': []},\n",
                            "             {'id': '69332',\n",
                            "              'metadata': {'text': ' What is the biggest city?'},\n",
                            "              'score': 0.727474,\n",
                            "              'values': []},\n",
                            "             {'id': '84749',\n",
                            "              'metadata': {'text': \" What are the world's most advanced \"\n",
                            "                                   'cities?'},\n",
                            "              'score': 0.709189594,\n",
                            "              'values': []},\n",
                            "             {'id': '109231',\n",
                            "              'metadata': {'text': ' Where is the most beautiful city in the '\n",
                            "                                   'world?'},\n",
                            "              'score': 0.696054876,\n",
                            "              'values': []},\n",
                            "             {'id': '109230',\n",
                            "              'metadata': {'text': ' What is the greatest, most beautiful city '\n",
                            "                                   'in the world?'},\n",
                            "              'score': 0.657444537,\n",
                            "              'values': []}],\n",
                            " 'namespace': '',\n",
                            " 'usage': {'read_units': 6}}"
                        ]
                    },
                    "execution_count": 27,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "query = \"which city has the highest population in the world?\"\n",
                "\n",
                "# create the query vector, store it into the Pinecone vector\n",
                "xq = model.encode(query).tolist()\n",
                "\n",
                "# now query\n",
                "xc = index.query(vector=xq, top_k=5, include_metadata=True)\n",
                "xc"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "In the returned response `xc` we can see the most relevant questions to our particular query — we don't have any exact matches but we can see that the returned questions are similar in the topics they are asking about. We can reformat this response to be a little easier to read:"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 28,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "0.79:  What's the world's largest city?\n",
                        "0.73:  What is the biggest city?\n",
                        "0.71:  What are the world's most advanced cities?\n",
                        "0.7:  Where is the most beautiful city in the world?\n",
                        "0.66:  What is the greatest, most beautiful city in the world?\n"
                    ]
                }
            ],
            "source": [
                "for result in xc['matches']:\n",
                "    print(f\"{round(result['score'], 2)}: {result['metadata']['text']}\")"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "These are good results, let's try and modify the words being used to see if we still surface similar results."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 29,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "0.64:  What is the biggest city?\n",
                        "0.6:  What is the most dangerous city in USA?\n",
                        "0.59:  What's the world's largest city?\n",
                        "0.59:  What is the most dangerous city in USA? Why?\n",
                        "0.58:  What are the world's most advanced cities?\n"
                    ]
                }
            ],
            "source": [
                "query = \"which metropolis has the highest number of people?\"\n",
                "\n",
                "# create the query vector\n",
                "xq = model.encode(query).tolist()\n",
                "\n",
                "# now query\n",
                "xc = index.query(vector=xq, top_k=5, include_metadata=True)\n",
                "for result in xc['matches']:\n",
                "    print(f\"{round(result['score'], 2)}: {result['metadata']['text']}\")"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Here we used different terms in our query than that of the returned documents. We substituted **\"city\"** for **\"metropolis\"** and **\"populated\"** for **\"number of people\"**.\n",
                "\n",
                "Despite these very different terms and *lack* of term overlap between query and returned documents — we get highly relevant results — this is the power of *semantic search*.\n",
                "\n",
                "## Task 1\n",
                "\n",
                "Try changing the model and querying again. You can find alternative models [here](https://sbert.net/docs/pretrained_models.html). Note that you will need to choose one with the same dimensionality (384). Clicking on the \"info\" symbol next to the model names will tell you information including their dimensionality.\n",
                "\n",
                "Find a model that gives similar results and a model that gives different results."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 30,
            "metadata": {},
            "outputs": [
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Similar\n",
                        "0.47:  What is the biggest city?\n",
                        "0.47:  How do you measure the pollution rate in a city with a population of 6 million people?\n",
                        "0.46:  Which city in the world has the lowest crime rate and why?\n",
                        "0.46:  Which city in India has a large Parsi population?\n",
                        "0.46:  What are the world's most advanced cities?\n",
                        "\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Different\n",
                        "1.46:  How would people compare the various metropolitan cities in India on all the possible parameters?\n",
                        "1.38:  Are Zillow, Redfin, and Trulia competitors? Who is better-positioned?\n",
                        "1.36:  What are the world's most advanced cities?\n",
                        "1.36:  Is there a way to leverage Google Maps or other tools to determine highest volume commute destinations for a neighborhood?\n",
                        "1.34:  Does London have lower violent crime rates than New York?\n"
                    ]
                }
            ],
            "source": [
                "# TODO\n",
                "\n",
                "# Solution\n",
                "\n",
                "query = \"which metropolis has the highest number of people?\"\n",
                "\n",
                "# Similar\n",
                "model2 = SentenceTransformer('sentence-transformers/all-MiniLM-L12-v2', device=device)\n",
                "xq = model2.encode(query).tolist()\n",
                "xc = index.query(vector=xq, top_k=5, include_metadata=True)\n",
                "print(\"Similar\")\n",
                "for result in xc['matches']:\n",
                "    print(f\"{round(result['score'], 2)}: {result['metadata']['text']}\")\n",
                "print()\n",
                "\n",
                "# Different\n",
                "model3 = SentenceTransformer('sentence-transformers/paraphrase-MiniLM-L3-v2', device=device)\n",
                "xq = model3.encode(query).tolist()\n",
                "xc = index.query(vector=xq, top_k=5, include_metadata=True)\n",
                "print(\"Different\")\n",
                "for result in xc['matches']:\n",
                "    print(f\"{round(result['score'], 2)}: {result['metadata']['text']}\")"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Retrieval Enhanced Generative Question Answering\n",
                "\n",
                "Next, we will see how these queries can be used with an LLM to generate better outputs.\n",
                "\n",
                "We will again use data that has already been prepared (for details, see [this notebook](https://github.com/pinecone-io/examples/blob/master/learn/generation/openai/gen-qa-openai.ipynb))."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 31,
            "metadata": {},
            "outputs": [
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "Loading documents parquet files: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:39<00:00, 39.35s/it]\n"
                    ]
                },
                {
                    "data": {
                        "text/html": [
                            "<div>\n",
                            "<style scoped>\n",
                            "    .dataframe tbody tr th:only-of-type {\n",
                            "        vertical-align: middle;\n",
                            "    }\n",
                            "\n",
                            "    .dataframe tbody tr th {\n",
                            "        vertical-align: top;\n",
                            "    }\n",
                            "\n",
                            "    .dataframe thead th {\n",
                            "        text-align: right;\n",
                            "    }\n",
                            "</style>\n",
                            "<table border=\"1\" class=\"dataframe\">\n",
                            "  <thead>\n",
                            "    <tr style=\"text-align: right;\">\n",
                            "      <th></th>\n",
                            "      <th>id</th>\n",
                            "      <th>values</th>\n",
                            "      <th>sparse_values</th>\n",
                            "      <th>metadata</th>\n",
                            "    </tr>\n",
                            "  </thead>\n",
                            "  <tbody>\n",
                            "    <tr>\n",
                            "      <th>0</th>\n",
                            "      <td>35Pdoyi6ZoQ-t0.0</td>\n",
                            "      <td>[-0.010402066633105278, -0.018359748646616936,...</td>\n",
                            "      <td>None</td>\n",
                            "      <td>{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'en...</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <th>1</th>\n",
                            "      <td>35Pdoyi6ZoQ-t18.48</td>\n",
                            "      <td>[-0.011849376372992992, 0.0007984379190020263,...</td>\n",
                            "      <td>None</td>\n",
                            "      <td>{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'en...</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <th>2</th>\n",
                            "      <td>35Pdoyi6ZoQ-t32.36</td>\n",
                            "      <td>[-0.014534404501318932, -0.0003158661129418760...</td>\n",
                            "      <td>None</td>\n",
                            "      <td>{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'en...</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <th>3</th>\n",
                            "      <td>35Pdoyi6ZoQ-t51.519999999999996</td>\n",
                            "      <td>[-0.011597747914493084, -0.007550035137683153,...</td>\n",
                            "      <td>None</td>\n",
                            "      <td>{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'en...</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <th>4</th>\n",
                            "      <td>35Pdoyi6ZoQ-t67.28</td>\n",
                            "      <td>[-0.015879768878221512, 0.0030445053707808256,...</td>\n",
                            "      <td>None</td>\n",
                            "      <td>{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'en...</td>\n",
                            "    </tr>\n",
                            "  </tbody>\n",
                            "</table>\n",
                            "</div>"
                        ],
                        "text/plain": [
                            "                                id  \\\n",
                            "0                 35Pdoyi6ZoQ-t0.0   \n",
                            "1               35Pdoyi6ZoQ-t18.48   \n",
                            "2               35Pdoyi6ZoQ-t32.36   \n",
                            "3  35Pdoyi6ZoQ-t51.519999999999996   \n",
                            "4               35Pdoyi6ZoQ-t67.28   \n",
                            "\n",
                            "                                              values sparse_values  \\\n",
                            "0  [-0.010402066633105278, -0.018359748646616936,...          None   \n",
                            "1  [-0.011849376372992992, 0.0007984379190020263,...          None   \n",
                            "2  [-0.014534404501318932, -0.0003158661129418760...          None   \n",
                            "3  [-0.011597747914493084, -0.007550035137683153,...          None   \n",
                            "4  [-0.015879768878221512, 0.0030445053707808256,...          None   \n",
                            "\n",
                            "                                            metadata  \n",
                            "0  {'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'en...  \n",
                            "1  {'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'en...  \n",
                            "2  {'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'en...  \n",
                            "3  {'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'en...  \n",
                            "4  {'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'en...  "
                        ]
                    },
                    "execution_count": 31,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "from pinecone_datasets import load_dataset\n",
                "\n",
                "dataset = load_dataset('youtube-transcripts-text-embedding-ada-002')\n",
                "\n",
                "# we drop sparse_values as they are not needed for this example\n",
                "dataset.documents.drop(['metadata'], axis=1, inplace=True)\n",
                "dataset.documents.rename(columns={'blob': 'metadata'}, inplace=True)\n",
                "\n",
                "# Print a sample of the data\n",
                "dataset.head()"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Again, we will set up a pinecone database:"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 32,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/plain": [
                            "{'dimension': 1536,\n",
                            " 'index_fullness': 0.0,\n",
                            " 'metric': 'cosine',\n",
                            " 'namespaces': {},\n",
                            " 'total_vector_count': 0,\n",
                            " 'vector_type': 'dense'}"
                        ]
                    },
                    "execution_count": 32,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "index_name = 'gen-qa-openai-fast'\n",
                "# check if index already exists (it shouldn't if this is first time)\n",
                "if index_name not in pc.list_indexes().names():\n",
                "    # if does not exist, create index\n",
                "    pc.create_index(\n",
                "        index_name,\n",
                "        dimension=1536,  # dimensionality of text-embedding-ada-002\n",
                "        metric='cosine',\n",
                "        spec=spec\n",
                "    )\n",
                "# connect to index\n",
                "index = pc.Index(index_name)\n",
                "# view index stats\n",
                "index.describe_index_stats()"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "As in the previous section, we'll insert the data into the database (this can take 5-10 minutes):"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 33,
            "metadata": {},
            "outputs": [],
            "source": [
                "for batch in dataset.iter_documents(batch_size=100):\n",
                "    index.upsert(batch)"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Now we've added all of our langchain docs to the index. With that we can move on to retrieval and then answer generation.\n",
                "\n",
                "## Retrieval\n",
                "\n",
                "To search through our documents we first need to create a query vector `xq`. Using `xq` we will retrieve the most relevant chunks from the LangChain docs. To create that query vector we must initialize a `text-embedding-ada-002` embedding model with OpenAI. For this, you need an [OpenAI API key](https://platform.openai.com/)."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 34,
            "metadata": {},
            "outputs": [],
            "source": [
                "import openai\n",
                "\n",
                "openai.api_key = openai_api_key\n",
                "\n",
                "embed_model = \"text-embedding-ada-002\""
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 35,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/plain": [
                            "{'matches': [{'id': 'pNvujJ1XyeQ-t418.88',\n",
                            "              'metadata': {'channel_id': 'UCv83tO5cePwHMt1952IVVHw',\n",
                            "                           'end': 568.0,\n",
                            "                           'published': '2021-11-24 16:24:24 UTC',\n",
                            "                           'start': 418.0,\n",
                            "                           'text': 'pairs of related sentences you can go '\n",
                            "                                   'ahead and actually try training or '\n",
                            "                                   'fine-tuning using NLI with multiple '\n",
                            "                                   \"negative ranking loss. If you don't have \"\n",
                            "                                   'that fine. Another option is that you have '\n",
                            "                                   'a semantic textual similarity data set or '\n",
                            "                                   'STS and what this is is you have so you '\n",
                            "                                   'have sentence A here, sentence B here and '\n",
                            "                                   'then you have a score from from 0 to 1 '\n",
                            "                                   'that tells you the similarity between '\n",
                            "                                   'those two scores and you would train this '\n",
                            "                                   'using something like cosine similarity '\n",
                            "                                   \"loss. Now if that's not an option and your \"\n",
                            "                                   'focus or use case is on building a '\n",
                            "                                   'sentence transformer for another language '\n",
                            "                                   'where there is no current sentence '\n",
                            "                                   'transformer you can use multilingual '\n",
                            "                                   'parallel data. So what I mean by that is '\n",
                            "                                   'so parallel data just means translation '\n",
                            "                                   'pairs so if you have for example a English '\n",
                            "                                   'sentence and then you have another '\n",
                            "                                   'language here so it can it can be anything '\n",
                            "                                   \"I'm just going to put XX and that XX is \"\n",
                            "                                   'your target language you can fine-tune a '\n",
                            "                                   'model using something called multilingual '\n",
                            "                                   'knowledge distillation and what that does '\n",
                            "                                   'is takes a monolingual model for example '\n",
                            "                                   'in English and using those translation '\n",
                            "                                   'pairs it distills the knowledge the '\n",
                            "                                   'semantic similarity knowledge from that '\n",
                            "                                   'monolingual English model into a '\n",
                            "                                   'multilingual model which can handle both '\n",
                            "                                   'English and your target language. So '\n",
                            "                                   \"they're three options quite popular very \"\n",
                            "                                   'common that you can go for and as a '\n",
                            "                                   'supervised methods the chances are that '\n",
                            "                                   'probably going to outperform anything you '\n",
                            "                                   'do with unsupervised training at least for '\n",
                            "                                   'now. So if none of those sound like '\n",
                            "                                   'something',\n",
                            "                           'title': 'Today Unsupervised Sentence Transformers, '\n",
                            "                                    'Tomorrow Skynet (how TSDAE works)',\n",
                            "                           'url': 'https://youtu.be/pNvujJ1XyeQ'},\n",
                            "              'score': 0.865188,\n",
                            "              'values': []},\n",
                            "             {'id': 'WS1uVMGhlWQ-t747.92',\n",
                            "              'metadata': {'channel_id': 'UCv83tO5cePwHMt1952IVVHw',\n",
                            "                           'end': 906.0,\n",
                            "                           'published': '2021-10-20 17:06:20 UTC',\n",
                            "                           'start': 747.0,\n",
                            "                           'text': \"pooling approach. Or we can't use it in \"\n",
                            "                                   'its current form. Now the solution to this '\n",
                            "                                   'problem was introduced by two people in '\n",
                            "                                   '2019 Nils Reimers and Irenia Gurevich. '\n",
                            "                                   'They introduced what is the first sentence '\n",
                            "                                   'transformer or sentence BERT. And it was '\n",
                            "                                   'found that sentence BERT or S BERT '\n",
                            "                                   'outformed all of the previous Save the Art '\n",
                            "                                   'models on pretty much all benchmarks. Not '\n",
                            "                                   'all of them but most of them. And it did '\n",
                            "                                   'it in a very quick time. So if we compare '\n",
                            "                                   'it to BERT, if we wanted to find the most '\n",
                            "                                   'similar sentence pair from 10,000 '\n",
                            "                                   'sentences in that 2019 paper they found '\n",
                            "                                   'that with BERT that took 65 hours. With S '\n",
                            "                                   'BERT embeddings they could create all the '\n",
                            "                                   'embeddings in just around five seconds. '\n",
                            "                                   'And then they could compare all those with '\n",
                            "                                   \"cosine similarity in 0.01 seconds. So it's \"\n",
                            "                                   'a lot faster. We go from 65 hours to just '\n",
                            "                                   'over five seconds which is I think pretty '\n",
                            "                                   \"incredible. Now I think that's pretty much \"\n",
                            "                                   'all the context we need behind sentence '\n",
                            "                                   'transformers. And what we do now is dive '\n",
                            "                                   'into a little bit of how they actually '\n",
                            "                                   'work. Now we said before we have the core '\n",
                            "                                   'transform models and what S BERT does is '\n",
                            "                                   'fine tunes on sentence pairs using what is '\n",
                            "                                   'called a Siamese architecture or Siamese '\n",
                            "                                   'network. What we mean by a Siamese network '\n",
                            "                                   'is that we have what we can see, what can '\n",
                            "                                   'view as two BERT models that are identical '\n",
                            "                                   'and the weights between those two models '\n",
                            "                                   'are tied. Now in reality when implementing '\n",
                            "                                   'this we just use a single BERT model. And '\n",
                            "                                   'what we do is we process one sentence, a '\n",
                            "                                   'sentence A through the model and then we '\n",
                            "                                   'process another sentence, sentence B '\n",
                            "                                   \"through the model. And that's the sentence \"\n",
                            "                                   'pair. So with our cross-linked we were '\n",
                            "                                   'processing the sentence pair together. We '\n",
                            "                                   'were putting them both together, '\n",
                            "                                   'processing them all at once. This time we '\n",
                            "                                   'process them separately. And during '\n",
                            "                                   'training what happens is the weights '\n",
                            "                                   'within BERT are optimized to reduce the '\n",
                            "                                   'difference between two vector embeddings '\n",
                            "                                   'or two sentence',\n",
                            "                           'title': 'Intro to Sentence Embeddings with '\n",
                            "                                    'Transformers',\n",
                            "                           'url': 'https://youtu.be/WS1uVMGhlWQ'},\n",
                            "              'score': 0.863455892,\n",
                            "              'values': []}],\n",
                            " 'namespace': '',\n",
                            " 'usage': {'read_units': 6}}"
                        ]
                    },
                    "execution_count": 35,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "query = (\n",
                "    \"Which training method should I use for sentence transformers when \" +\n",
                "    \"I only have pairs of related sentences?\"\n",
                ")\n",
                "\n",
                "res = openai.Embedding.create(\n",
                "    input=[query],\n",
                "    engine=embed_model\n",
                ")\n",
                "\n",
                "# retrieve from Pinecone\n",
                "xq = res['data'][0]['embedding']\n",
                "\n",
                "# get relevant contexts (including the questions)\n",
                "res = index.query(vector=xq, top_k=2, include_metadata=True)\n",
                "\n",
                "res"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "We write some functions to handle the retrieval and completion steps:"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 39,
            "metadata": {},
            "outputs": [],
            "source": [
                "limit = 3750\n",
                "\n",
                "import time\n",
                "\n",
                "def retrieve(query):\n",
                "    res = openai.Embedding.create(\n",
                "        input=[query],\n",
                "        engine=embed_model\n",
                "    )\n",
                "\n",
                "    # retrieve from Pinecone\n",
                "    xq = res['data'][0]['embedding']\n",
                "\n",
                "    # get relevant contexts\n",
                "    contexts = []\n",
                "    time_waited = 0\n",
                "    while (len(contexts) < 3 and time_waited < 60 * 12):\n",
                "        res = index.query(vector=xq, top_k=3, include_metadata=True)\n",
                "        contexts = contexts + [\n",
                "            x['metadata']['text'] for x in res['matches']\n",
                "        ]\n",
                "        print(f\"Retrieved {len(contexts)} contexts, sleeping for 15 seconds...\")\n",
                "        time.sleep(15)\n",
                "        time_waited += 15\n",
                "\n",
                "    if time_waited >= 60 * 12:\n",
                "        print(\"Timed out waiting for contexts to be retrieved.\")\n",
                "        contexts = [\"No contexts retrieved. Try to answer the question yourself!\"]\n",
                "\n",
                "\n",
                "    # build our prompt with the retrieved contexts included\n",
                "    prompt_start = (\n",
                "        \"Answer the question based on the context below.\\n\\n\"+\n",
                "        \"Context:\\n\"\n",
                "    )\n",
                "    prompt_end = (\n",
                "        f\"\\n\\nQuestion: {query}\\nAnswer:\"\n",
                "    )\n",
                "    # append contexts until hitting limit\n",
                "    for i in range(1, len(contexts)):\n",
                "        if len(\"\\n\\n---\\n\\n\".join(contexts[:i])) >= limit:\n",
                "            prompt = (\n",
                "                prompt_start +\n",
                "                \"\\n\\n---\\n\\n\".join(contexts[:i-1]) +\n",
                "                prompt_end\n",
                "            )\n",
                "            break\n",
                "        elif i == len(contexts)-1:\n",
                "            prompt = (\n",
                "                prompt_start +\n",
                "                \"\\n\\n---\\n\\n\".join(contexts) +\n",
                "                prompt_end\n",
                "            )\n",
                "    return prompt\n",
                "\n",
                "\n",
                "def complete(prompt):\n",
                "    # instructions\n",
                "    sys_prompt = \"You are a helpful assistant that always answers questions.\"\n",
                "    # query text-davinci-003\n",
                "    res = openai.ChatCompletion.create(\n",
                "        model='gpt-4.1',\n",
                "        messages=[\n",
                "            {\"role\": \"system\", \"content\": sys_prompt},\n",
                "            {\"role\": \"user\", \"content\": prompt}\n",
                "        ],\n",
                "        temperature=0\n",
                "    )\n",
                "    return res['choices'][0]['message']['content'].strip()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 40,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Retrieved 3 contexts, sleeping for 15 seconds...\n"
                    ]
                },
                {
                    "data": {
                        "text/plain": [
                            "\"Answer the question based on the context below.\\n\\nContext:\\npairs of related sentences you can go ahead and actually try training or fine-tuning using NLI with multiple negative ranking loss. If you don't have that fine. Another option is that you have a semantic textual similarity data set or STS and what this is is you have so you have sentence A here, sentence B here and then you have a score from from 0 to 1 that tells you the similarity between those two scores and you would train this using something like cosine similarity loss. Now if that's not an option and your focus or use case is on building a sentence transformer for another language where there is no current sentence transformer you can use multilingual parallel data. So what I mean by that is so parallel data just means translation pairs so if you have for example a English sentence and then you have another language here so it can it can be anything I'm just going to put XX and that XX is your target language you can fine-tune a model using something called multilingual knowledge distillation and what that does is takes a monolingual model for example in English and using those translation pairs it distills the knowledge the semantic similarity knowledge from that monolingual English model into a multilingual model which can handle both English and your target language. So they're three options quite popular very common that you can go for and as a supervised methods the chances are that probably going to outperform anything you do with unsupervised training at least for now. So if none of those sound like something\\n\\n---\\n\\npooling approach. Or we can't use it in its current form. Now the solution to this problem was introduced by two people in 2019 Nils Reimers and Irenia Gurevich. They introduced what is the first sentence transformer or sentence BERT. And it was found that sentence BERT or S BERT outformed all of the previous Save the Art models on pretty much all benchmarks. Not all of them but most of them. And it did it in a very quick time. So if we compare it to BERT, if we wanted to find the most similar sentence pair from 10,000 sentences in that 2019 paper they found that with BERT that took 65 hours. With S BERT embeddings they could create all the embeddings in just around five seconds. And then they could compare all those with cosine similarity in 0.01 seconds. So it's a lot faster. We go from 65 hours to just over five seconds which is I think pretty incredible. Now I think that's pretty much all the context we need behind sentence transformers. And what we do now is dive into a little bit of how they actually work. Now we said before we have the core transform models and what S BERT does is fine tunes on sentence pairs using what is called a Siamese architecture or Siamese network. What we mean by a Siamese network is that we have what we can see, what can view as two BERT models that are identical and the weights between those two models are tied. Now in reality when implementing this we just use a single BERT model. And what we do is we process one sentence, a sentence A through the model and then we process another sentence, sentence B through the model. And that's the sentence pair. So with our cross-linked we were processing the sentence pair together. We were putting them both together, processing them all at once. This time we process them separately. And during training what happens is the weights within BERT are optimized to reduce the difference between two vector embeddings or two sentence\\n\\n---\\n\\nstraight into it and take a look at where we might want to use this training approach and and how we can actually implement it. So the first question we need to ask is do we really need to resort to unsupervised training? Now what we're going to do here is just have a look at a few of the most popular training approaches and what sort of data we need for that. So the first one we're looking at here is Natural Language Inference or NLI and NLI requires that we have pairs of sentences that are labeled as either contradictory, neutral which means they're not necessarily related or as entailing or as inferring each other. So you have pairs that entail each other so they are both very similar pairs that are neutral and also pairs that are contradictory. And this is the traditional NLI data. Now using another version of fine-tuning with NLI called a multiple negatives ranking loss you can get by with only entailment pairs so pairs that are related to each other or positive pairs and it can also use contradictory pairs to improve the performance of training as well but you don't need it. So if you have positive pairs of related sentences you can go ahead and actually try training or fine-tuning using NLI with multiple negative ranking loss. If you don't have that fine. Another option is that you have a semantic textual similarity data set or STS and what this is is you have so you have sentence A here, sentence B here and then you have a score from from 0 to 1 that tells you the similarity\\n\\nQuestion: Which training method should I use for sentence transformers when I only have pairs of related sentences?\\nAnswer:\""
                        ]
                    },
                    "execution_count": 40,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "# first we retrieve relevant items from Pinecone\n",
                "query_with_contexts = retrieve(query)\n",
                "query_with_contexts"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 41,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/plain": [
                            "\"If you only have pairs of related sentences (i.e., positive pairs), you should use the Natural Language Inference (NLI) training method with multiple negatives ranking loss. This approach allows you to train or fine-tune a sentence transformer using just the related (entailment) pairs, even if you don't have contradictory or neutral pairs. This method is effective and commonly used when only positive sentence pairs are available.\""
                        ]
                    },
                    "execution_count": 41,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "# then we complete the context-infused query\n",
                "complete(query_with_contexts)"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "And we get a pretty great answer straight away, specifying to use _multiple-rankings loss_ (also called _multiple negatives ranking loss_).\n",
                "\n"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Task 2\n",
                "\n",
                "Try adjusting the number of contexts down to 1, to see the impact on retrieval quality."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 42,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Retrieved 3 contexts, sleeping for 15 seconds...\n",
                        "Answer the question based on the context below.\n",
                        "\n",
                        "Context:\n",
                        "\n",
                        "\n",
                        "Question: Which training method should I use for sentence transformers when I only have pairs of related sentences?\n",
                        "Answer:\n"
                    ]
                },
                {
                    "data": {
                        "text/plain": [
                            "'If you only have pairs of related sentences (for example, sentence pairs that are paraphrases or semantically similar), the best training method for sentence transformers is **contrastive learning** using a **pairwise loss function**. The most common and effective loss functions in this scenario are:\\n\\n- **Cosine Similarity Loss** (also called CosineEmbeddingLoss)\\n- **Contrastive Loss**\\n- **Triplet Loss** (if you can generate negative pairs)\\n\\nFor sentence-transformers, the **Cosine Similarity Loss** is often used when you have pairs of sentences and a label indicating whether they are similar (1) or not (0). If you only have positive pairs (related sentences), you can use **MultipleNegativesRankingLoss**, which automatically treats other sentences in the batch as negatives.\\n\\n**In summary:**  \\nUse **Cosine Similarity Loss** or **MultipleNegativesRankingLoss** with your sentence pairs. These are specifically designed for training sentence transformers with pairs of related sentences.'"
                        ]
                    },
                    "execution_count": 42,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "# TODO\n",
                "\n",
                "# Solution\n",
                "def retrieve_1_context(query):\n",
                "    res = openai.Embedding.create(\n",
                "        input=[query],\n",
                "        engine=embed_model\n",
                "    )\n",
                "    xq = res['data'][0]['embedding']\n",
                "    contexts = []\n",
                "    time_waited = 0\n",
                "    while (len(contexts) < 1 and time_waited < 60 * 12): # change the 3 to 1\n",
                "        res = index.query(vector=xq, top_k=1, include_metadata=True) # using top_k=1\n",
                "        contexts = contexts + [\n",
                "            x['metadata']['text'] for x in res['matches']\n",
                "        ]\n",
                "        print(f\"Retrieved {len(contexts)} contexts, sleeping for 15 seconds...\")\n",
                "        time.sleep(15)\n",
                "        time_waited += 15\n",
                "\n",
                "    if time_waited >= 60 * 12:\n",
                "        print(\"Timed out waiting for contexts to be retrieved.\")\n",
                "        contexts = [\"No contexts retrieved. Try to answer the question yourself!\"]\n",
                "\n",
                "    prompt_start = (\n",
                "        \"Answer the question based on the context below.\\n\\n\"+\n",
                "        \"Context:\\n\"\n",
                "    )\n",
                "    prompt_end = (\n",
                "        f\"\\n\\nQuestion: {query}\\nAnswer:\"\n",
                "    )\n",
                "    joined_context = \"\\n\\n---\\n\\n\".join(contexts)\n",
                "    if len(joined_context) > limit:\n",
                "        joined_context = joined_context[:limit]  # truncate\n",
                "    prompt = prompt_start + joined_context + prompt_end\n",
                "    return prompt\n",
                "\n",
                "query_with_contexts = retrieve_1_context(query)\n",
                "print(query_with_contexts)\n",
                "complete(query_with_contexts)"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Pack up\n",
                "\n",
                "Once you're done with the workshop, delete the indices to save resources:"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 43,
            "metadata": {},
            "outputs": [],
            "source": [
                "pc.delete_index('gen-qa-openai-fast')\n",
                "pc.delete_index('semantic-search-fast')"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3 (ipykernel)",
            "language": "python",
            "name": "python3"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}
