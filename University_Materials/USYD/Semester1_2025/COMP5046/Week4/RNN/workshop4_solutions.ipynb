{
    "cells": [
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Workshop 4\n",
                "\n",
                "We will be building and training a basic character-level Recurrent Neural\n",
                "Network (RNN) to classify words. This lab is based on Based on [\"NLP From Scratch: Classifying Names with a Character-Level RNN\"](https://pytorch.org/tutorials/intermediate/char_rnn_classification_tutorial.html) by [Sean Robertson](https://github.com/spro).\n",
                "\n",
                "A character-level RNN reads words as a series of characters and for each one it (a) produces an output, and (b) updates a hidden state vector. The output hidden state from one step is in the input to the next step. In this lab, the final prediction will be made based on the last output.\n",
                "\n",
                "The task we'll consider is predicting the language of origin of a name.\n",
                "We'll train on a few thousand surnames from 18 languages\n",
                "of origin, and predict which language a name is from based on the\n",
                "spelling. For example:\n",
                "\n",
                "```sh\n",
                "$ python predict.py Hinton\n",
                "(-0.47) Scottish\n",
                "(-1.52) English\n",
                "(-3.57) Irish\n",
                "\n",
                "$ python predict.py Schmidhuber\n",
                "(-0.19) German\n",
                "(-2.48) Czech\n",
                "(-2.68) Dutch\n",
                "```\n",
                "\n",
                "# Pre-Work\n",
                "\n",
                "## Preparing the Data\n",
                "\n",
                "Included in the ``names.txt`` file are a list of names, one per line, and a language they are commonly used in.\n",
                "We have converted them to ASCII for convenience.\n",
                "\n",
                "We'll read them in and make a dictionary of lists of names per language, ``{language: [names ...]}``."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 28,
            "metadata": {},
            "outputs": [],
            "source": [
                "import string\n",
                "all_letters = string.ascii_letters + \" .,;’\"\n",
                "n_letters = len(all_letters)\n",
                "\n",
                "# Read the category_lines dictionary, a list of names per language\n",
                "category_lines = {}\n",
                "all_categories = set()\n",
                "\n",
                "with open(\"names.txt\") as src:\n",
                "    for line in src:\n",
                "        parts = line.strip().split()\n",
                "        category = parts[0]\n",
                "        name = ' '.join(parts[1:])\n",
                "        all_categories.add(category)\n",
                "        category_lines.setdefault(category, []).append(name)\n",
                "    \n",
                "all_categories = sorted(list(all_categories))\n",
                "n_categories = len(all_categories)"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Now we have ``category_lines``, a dictionary mapping each category\n",
                "(language) to a list of lines (names). We also kept track of\n",
                "``all_categories`` (just a list of languages) and ``n_categories`` for\n",
                "later reference.\n",
                "\n",
                "\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 29,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "['Abandonato', 'Abatangelo', 'Abatantuono', 'Abate', 'Abategiovanni']\n"
                    ]
                }
            ],
            "source": [
                "print(category_lines['Italian'][:5])"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Turning Names into Tensors\n",
                "\n",
                "Now that we have all the names organized, we need to turn them into\n",
                "Tensors to make any use of them.\n",
                "\n",
                "To represent a single letter, we use a one-hot vector of size\n",
                "`<1 x n_letters>`. A one-hot vector is filled with 0s except for a 1\n",
                "at index of the current letter, e.g. `\"b\" = <0 1 0 0 0 ...>`.\n",
                "In lecture 1, we noted that usually we use special data structures to avoid memory overhead (e.g., dictionaries or sparse vectors).\n",
                "In this lab, we'll use a normal vector for convenience.\n",
                "\n",
                "To make a word we join a bunch of those into a 2D matrix\n",
                "``<line_length x 1 x n_letters>``.\n",
                "\n",
                "That extra 1 dimension is because PyTorch assumes everything is in\n",
                "batches - we're just using a batch size of 1 here.\n",
                "\n",
                "\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 30,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "This is the tensor for 'J':\n",
                        "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
                        "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,\n",
                        "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
                        "         0., 0., 0.]])\n",
                        "\n",
                        "This is the dimensionality of the matrix for 'Jones':\n",
                        "torch.Size([5, 1, 57])\n"
                    ]
                }
            ],
            "source": [
                "import torch\n",
                "torch.random.manual_seed(0)\n",
                "\n",
                "# Find letter index from all_letters, e.g. \"a\" = 0\n",
                "def letterToIndex(letter):\n",
                "    return all_letters.find(letter)\n",
                "\n",
                "# Just for demonstration, turn a letter into a <1 x n_letters> Tensor, one-hot-code encoding\n",
                "def letterToTensor(letter):\n",
                "    tensor = torch.zeros(1, n_letters)\n",
                "    tensor[0][letterToIndex(letter)] = 1\n",
                "    return tensor\n",
                "\n",
                "# Turn a line into a <line_length x 1 x n_letters>,\n",
                "# or an array of one-hot letter vectors\n",
                "def lineToTensor(line):\n",
                "    tensor = torch.zeros(len(line), 1, n_letters) # 创建全零张量，形状为 (1, n_letters)\n",
                "    for li, letter in enumerate(line):\n",
                "        tensor[li][0][letterToIndex(letter)] = 1\n",
                "    return tensor\n",
                "\n",
                "print(\"This is the tensor for 'J':\")\n",
                "print(letterToTensor('J'))\n",
                "\n",
                "print(\"\\nThis is the dimensionality of the matrix for 'Jones':\")\n",
                "print(lineToTensor('Jones').size())"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Creating the Network\n",
                "\n",
                "Before autograd, creating a recurrent neural network in Torch involved\n",
                "cloning the parameters of a layer over several timesteps. The layers\n",
                "held hidden state and gradients which are now entirely handled by the\n",
                "graph itself. This means you can implement a RNN in a very \"pure\" way,\n",
                "as regular feed-forward layers.\n",
                "\n",
                "This RNN module is just 2 linear layers which operate on an input and hidden state, with\n",
                "a ``LogSoftmax`` layer after the output.\n",
                "\n",
                "\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 31,
            "metadata": {},
            "outputs": [],
            "source": [
                "import torch.nn as nn\n",
                "\n",
                "class RNN(nn.Module):\n",
                "    def __init__(self, input_size, hidden_size, output_size):\n",
                "        super(RNN, self).__init__()\n",
                "\n",
                "        self.hidden_size = hidden_size\n",
                "\n",
                "        # Define the layers of the model\n",
                "        # These also create the weights where needed\n",
                "\n",
                "        # input_size：表示当前时间步 t 的输入 x_t\n",
                "        # hidden_size：表示上一时间步 t-1 的隐藏状态 h_{t-1}\n",
                "        # 看forward里面的combined\n",
                "        self.i2h = nn.Linear(input_size + hidden_size, hidden_size)# i2h means input to hidden\n",
                "        # 这里没有使用任何的激活函数\n",
                "        self.h2o = nn.Linear(hidden_size, output_size) # means hidden to output, in this layer, we do not receive token\n",
                "        self.softmax = nn.LogSoftmax(dim=1)\n",
                "\n",
                "        # Set the weights to some initial values\n",
                "        self.init_weights()\n",
                "        \n",
                "    def init_weights(self):\n",
                "        # Initialise the weights to be random values in the matrices and zero for the biases\n",
                "        initrange = 0.1\n",
                "        self.i2h.weight.data.uniform_(-initrange, initrange) # 初始化为 [−0.1,0.1] 之间的均匀分布随机值。\n",
                "        self.i2h.bias.data.zero_() #  偏置向量 初始化为全 0。\n",
                "        self.h2o.weight.data.uniform_(-initrange, initrange)\n",
                "        self.h2o.bias.data.zero_()\n",
                "        \n",
                "    def initHidden(self):\n",
                "        # Define the initial hidden state\n",
                "        # Here we use an all zero vector\n",
                "        return torch.zeros(1, self.hidden_size)\n",
                "\n",
                "    def forward(self, input_tensor, hidden):\n",
                "        '''\n",
                "        在深度学习框架（例如 PyTorch）中，模型的权重（包括 i2h、h2h 和 h2o）\n",
                "        的更新并不是我们显式地去手动更新的，而是由框架自动处理的，具体的更新流程通过 \n",
                "        反向传播（backpropagation） 和 优化器（optimizer） 来完成。\n",
                "        '''\n",
                "        # Given an input, compute the steps defined by the model\n",
                "\n",
                "        # Concatenate the input and hidden vectors\n",
                "        combined = torch.cat((input_tensor, hidden), 1)\n",
                "        \n",
                "        # Apply a linear layer to get the new hidden vector\n",
                "        # 这里只是调用当前层来计算结果\n",
                "        hidden = self.i2h(combined)\n",
                "\n",
                "        # Apply a linear layer to get the output scores\n",
                "        output = self.h2o(hidden)\n",
                "\n",
                "        # Use softmax to turn the scores into probabilities\n",
                "        output = self.softmax(output)\n",
                "        return output, hidden\n",
                "\n",
                "n_hidden = 128\n",
                "# 创建model instance\n",
                "rnn = RNN(n_letters, n_hidden, n_categories)"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "To run a step of this network we need to pass an input (in our case, the\n",
                "Tensor for the current letter) and a previous hidden state (which we\n",
                "initialize as zeros at first). We'll get back the output (probability of\n",
                "each language) and a next hidden state (which we keep for the next\n",
                "step).\n",
                "\n",
                "Note - we haven't trained the model yet, so it's outputs will be random.\n",
                "\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 32,
            "metadata": {},
            "outputs": [],
            "source": [
                "input_tensor = letterToTensor('A')\n",
                "hidden = torch.zeros(1, n_hidden)\n",
                "\n",
                "'''\n",
                "在 PyTorch 中，当你执行 output, next_hidden = rnn(input_tensor, hidden) 时，\n",
                "forward 方法会自动被调用，这是因为 nn.Module 类已经为你实现了这个行为。\n",
                "\n",
                "\n",
                "一次 forward 只是在一个批次（或单个样本）上进行前向传播计算，\n",
                "而 一次 epoch 是训练过程中遍历整个训练集的过程，通常包含多个 forward 调用。\n",
                "\n",
                "因此，一次 forward 调用不能等同于一次 epoch，而是训练过程中多个 epoch 中的一个小部分。\n",
                "'''\n",
                "output, next_hidden = rnn(input_tensor, hidden)"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "For the sake of efficiency we don't want to be creating a new Tensor for\n",
                "every step, so we will use ``lineToTensor`` instead of\n",
                "``letterToTensor`` and use slices. This could be further optimized by\n",
                "precomputing batches of Tensors.\n",
                "\n",
                "\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 33,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Running 'Albert' through the RNN, Step-by-Step\n",
                        "\n",
                        "For given characters 'A'\n",
                        "\n",
                        "Next hidden state <torch.Size([1, 128])> is:\n",
                        "tensor([[ 0.0650, -0.0613,  0.0333, -0.0795,  0.0739,  0.0845,  0.0280, -0.0459,\n",
                        "         -0.0826,  0.0137,  0.0208, -0.0218, -0.0497, -0.0360,  0.0474,  0.0464,\n",
                        "          0.0277, -0.0369,  0.0146, -0.0415,  0.0855,  0.0203,  0.0645, -0.0328,\n",
                        "         -0.0948, -0.0276,  0.0791, -0.0833,  0.0843, -0.0786,  0.0086, -0.0290,\n",
                        "         -0.0446,  0.0827, -0.0840,  0.0705,  0.0398, -0.0939, -0.0959,  0.0506,\n",
                        "          0.0038, -0.0145,  0.0291, -0.0739, -0.0482, -0.0642,  0.0851,  0.0395,\n",
                        "          0.0447, -0.0785,  0.0191,  0.0034,  0.0516,  0.0238, -0.0216,  0.0633,\n",
                        "         -0.0726, -0.0362, -0.0191,  0.0849,  0.0474,  0.0374,  0.0538,  0.0343,\n",
                        "         -0.0930, -0.0039, -0.0937,  0.0177,  0.0566, -0.0995, -0.0558, -0.0280,\n",
                        "          0.0286, -0.0865, -0.0766, -0.0717,  0.0423,  0.0426,  0.0636, -0.0992,\n",
                        "          0.0112,  0.0612, -0.0171, -0.0686,  0.0979,  0.0423, -0.0275,  0.0208,\n",
                        "         -0.0246,  0.0719, -0.0425, -0.0098, -0.0434,  0.0651, -0.0911,  0.0011,\n",
                        "          0.0270, -0.0897, -0.0584, -0.0373, -0.0778,  0.0847, -0.0493, -0.0256,\n",
                        "         -0.0598,  0.0776, -0.0754, -0.0300,  0.0434, -0.0139,  0.0322,  0.0454,\n",
                        "         -0.0351, -0.0381,  0.0189, -0.0743, -0.0177, -0.0666,  0.0411, -0.0710,\n",
                        "         -0.0564, -0.0627, -0.0746, -0.0550, -0.0006, -0.0849,  0.0225,  0.0682]],\n",
                        "       grad_fn=<AddmmBackward0>)\n",
                        "\n",
                        "Likelihood <torch.Size([1, 18])> for each label is:\n",
                        "tensor([[-2.8697, -2.9320, -2.9049, -2.8794, -2.8750, -2.8573, -2.8949, -2.9308,\n",
                        "         -2.9500, -2.8278, -2.9299, -2.8864, -2.9097, -2.8594, -2.8778, -2.8594,\n",
                        "         -2.8350, -2.9597]], grad_fn=<LogSoftmaxBackward0>)\n",
                        "--------------------\n",
                        "For given characters 'A' and 'l'\n",
                        "\n",
                        "Next hidden state <torch.Size([1, 128])> is:\n",
                        "tensor([[ 0.0596,  0.0455, -0.0657, -0.1438, -0.0017, -0.0525, -0.0252, -0.0336,\n",
                        "         -0.0764, -0.0988,  0.0435,  0.0044, -0.0212,  0.0688, -0.0262, -0.0534,\n",
                        "          0.0406, -0.0407,  0.0147, -0.0285, -0.0287, -0.0223,  0.0567, -0.1454,\n",
                        "         -0.0233,  0.0305,  0.0607, -0.0747,  0.1286,  0.1062,  0.1034, -0.0441,\n",
                        "          0.0230,  0.0527,  0.0673,  0.0589, -0.0357, -0.0782,  0.0541, -0.0927,\n",
                        "         -0.0369, -0.0004, -0.0915,  0.0251,  0.0677,  0.0167,  0.0055, -0.0899,\n",
                        "         -0.0387, -0.0586, -0.1013,  0.0555, -0.0560, -0.0093, -0.0285,  0.0448,\n",
                        "          0.0012, -0.0570,  0.0476,  0.0285, -0.0132,  0.0024,  0.0575,  0.0727,\n",
                        "          0.0601,  0.0166,  0.0268, -0.0683, -0.0777,  0.0797, -0.0277,  0.0245,\n",
                        "         -0.0154,  0.0070, -0.0292, -0.0814,  0.0444, -0.0827, -0.0238,  0.0082,\n",
                        "         -0.0329,  0.0552,  0.0734, -0.0726, -0.0247,  0.1260,  0.0094,  0.0691,\n",
                        "         -0.0522,  0.1116, -0.0256,  0.0716, -0.0996, -0.0694,  0.1102,  0.1001,\n",
                        "          0.0417,  0.0321, -0.0245, -0.1076, -0.0417, -0.1067, -0.0131, -0.0418,\n",
                        "          0.0895, -0.0686, -0.0162,  0.0106, -0.0092, -0.0042, -0.0808,  0.0039,\n",
                        "          0.0189,  0.1547,  0.0877,  0.0558, -0.1007, -0.0334,  0.0812,  0.0630,\n",
                        "          0.0697,  0.0658, -0.0502,  0.1742,  0.0507,  0.0110,  0.0371,  0.0678]],\n",
                        "       grad_fn=<AddmmBackward0>)\n",
                        "\n",
                        "Likelihood <torch.Size([1, 18])> for each label is:\n",
                        "tensor([[-2.8814, -2.8669, -2.9169, -2.9613, -2.9232, -2.9371, -2.8936, -2.8348,\n",
                        "         -2.8394, -2.9019, -2.9463, -2.8853, -2.8238, -2.9127, -2.8741, -2.8780,\n",
                        "         -2.9394, -2.8258]], grad_fn=<LogSoftmaxBackward0>)\n",
                        "--------------------\n",
                        "\n",
                        "For given characters 'A', 'l', 'b', 'e', 'r', t'\n",
                        "Likelihood <torch.Size([1, 18])> for each label is:\n",
                        "tensor([[-2.9939, -2.7982, -2.9082, -2.9367, -2.9012, -2.8757, -2.9200, -2.8288,\n",
                        "         -2.8636, -2.8637, -2.9770, -2.8493, -2.9112, -2.8744, -2.8445, -2.8855,\n",
                        "         -2.8985, -2.9164]], grad_fn=<LogSoftmaxBackward0>)\n"
                    ]
                }
            ],
            "source": [
                "print(\"Running 'Albert' through the RNN, Step-by-Step\")\n",
                "line_tensor = lineToTensor('Albert')\n",
                "hidden = torch.zeros(1, n_hidden)\n",
                "\n",
                "# Pass character \"A\" (line_tensor[0]) into the rnn to get\n",
                "# - the \"output\" (nLikelihood for each label for given character 'A')\n",
                "# - the \"next_hidden\" state (hidden state for processing the next character 'l')\n",
                "output, next_hidden = rnn(line_tensor[0], hidden)\n",
                "\n",
                "print(\"\\nFor given characters 'A'\")\n",
                "print(f\"\\nNext hidden state <{next_hidden.size()}> is:\")\n",
                "print(next_hidden)\n",
                "\n",
                "print(f\"\\nLikelihood <{output.size()}> for each label is:\")\n",
                "print(output)\n",
                "print(\"-\"*20)\n",
                "# Pass character \"l\" (line_tensor[1]), and next_hidden into the rnn to get\n",
                "# - the output (nLikelihood for each label for given character 'A' and 'l')\n",
                "# - the next hidden state (hidden state for processing the next character 'b')\n",
                "output, next_hidden = rnn(line_tensor[1], next_hidden)\n",
                "\n",
                "print(\"For given characters 'A' and 'l'\")\n",
                "print(f\"\\nNext hidden state <{next_hidden.size()}> is:\")\n",
                "print(next_hidden)\n",
                "\n",
                "print(f\"\\nLikelihood <{output.size()}> for each label is:\")\n",
                "print(output)\n",
                "print(\"-\"*20)\n",
                "\n",
                "# Continue to iterate over \"b\", \"e\", \"r\", \"t\"\n",
                "output, next_hidden = rnn(line_tensor[2], next_hidden)\n",
                "output, next_hidden = rnn(line_tensor[3], next_hidden)\n",
                "output, next_hidden = rnn(line_tensor[4], next_hidden)\n",
                "output, next_hidden = rnn(line_tensor[5], next_hidden)\n",
                "# At the end, the output is nLikelihood for each label for given character 'A', 'l', 'b', 'e', 'r', t'\n",
                "# We will use this output for classification\n",
                "\n",
                "print(f\"\\nFor given characters 'A', 'l', 'b', 'e', 'r', t'\")\n",
                "print(f\"Likelihood <{output.size()}> for each label is:\")\n",
                "print(output)"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "As you can see the output is a ``<1 x n_categories>`` Tensor, where\n",
                "every item is the likelihood of that category (higher is more likely).\n",
                "\n",
                "# Workshop Tasks\n",
                "\n"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Task 1: Implement the above code using `for loop`"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 34,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Running 'Albert' through the RNN using `for loop`\n",
                        "\n",
                        "For given Albert, Likelihood <torch.Size([1, 18])> for each label is:\n",
                        "tensor([[-2.9939, -2.7982, -2.9082, -2.9367, -2.9012, -2.8757, -2.9200, -2.8288,\n",
                        "         -2.8636, -2.8637, -2.9770, -2.8493, -2.9112, -2.8744, -2.8445, -2.8855,\n",
                        "         -2.8985, -2.9164]], grad_fn=<LogSoftmaxBackward0>)\n",
                        "\n",
                        "Next hidden state <torch.Size([1, 128])> is:\n",
                        "tensor([[-5.1651e-02,  1.5536e-01,  8.5559e-02,  3.1779e-02,  1.0521e-02,\n",
                        "         -2.4438e-02, -5.5264e-02, -9.0539e-02, -6.4815e-02, -9.3608e-02,\n",
                        "         -4.1006e-02,  2.7128e-02,  1.0349e-01, -4.9309e-03, -4.1003e-02,\n",
                        "          1.3919e-01, -4.6296e-02, -8.5380e-02, -5.6727e-02,  1.1238e-01,\n",
                        "          8.7541e-02,  9.5509e-02,  1.5354e-02, -1.8413e-02, -8.7652e-02,\n",
                        "         -7.0610e-02,  7.9181e-02,  2.8588e-02,  6.4073e-02, -7.9719e-02,\n",
                        "         -5.5888e-03, -6.8839e-02,  3.1729e-02, -8.6611e-02,  1.1304e-01,\n",
                        "         -9.9707e-03,  3.2706e-03, -1.0931e-02, -2.9893e-02,  9.5714e-02,\n",
                        "          3.7780e-02, -1.3269e-01, -1.3981e-01, -6.4098e-02,  1.3737e-02,\n",
                        "         -5.4258e-02,  4.1192e-02, -5.6713e-02,  8.0066e-02,  9.0324e-02,\n",
                        "         -5.7490e-03,  3.3202e-02,  5.0138e-02,  1.0707e-01, -6.0602e-02,\n",
                        "         -5.3048e-05, -1.0216e-03, -2.5365e-02, -8.3129e-02, -7.0349e-02,\n",
                        "          1.0509e-02,  1.8776e-02,  4.3611e-02, -5.0243e-02,  1.5332e-01,\n",
                        "         -6.5630e-02,  2.0601e-02,  1.1442e-01, -8.0708e-02, -5.7423e-02,\n",
                        "         -8.4698e-03,  5.1924e-02, -6.7852e-02,  4.4180e-02,  9.0375e-03,\n",
                        "         -7.9157e-02,  6.5951e-03,  1.0532e-01, -3.6682e-02,  1.8774e-02,\n",
                        "         -6.0997e-02,  1.7083e-02, -1.9482e-02, -7.5050e-02, -4.5861e-02,\n",
                        "         -8.2214e-02,  3.1868e-02, -9.9403e-02,  7.6029e-02, -1.9947e-02,\n",
                        "          4.7414e-02,  5.9390e-02,  8.5217e-02,  6.2828e-02, -1.0185e-01,\n",
                        "         -1.2778e-01, -2.3216e-02, -1.7779e-02,  8.3110e-03,  5.1667e-02,\n",
                        "         -8.0113e-02, -1.2455e-01,  1.8225e-02,  7.4453e-03, -9.3959e-03,\n",
                        "         -2.4684e-03,  3.4084e-02,  1.3344e-01,  1.0852e-01,  1.0787e-01,\n",
                        "          2.7835e-03,  8.7090e-02,  7.0407e-02,  7.6126e-03, -4.5157e-02,\n",
                        "         -4.8361e-02, -7.4575e-02, -1.6588e-01, -1.3527e-01,  2.6093e-02,\n",
                        "         -4.0704e-02, -8.8869e-02, -1.2903e-01,  1.6210e-01, -1.1453e-01,\n",
                        "          4.4529e-02, -1.1808e-02,  7.5065e-02]], grad_fn=<AddmmBackward0>)\n"
                    ]
                }
            ],
            "source": [
                "print(\"Running 'Albert' through the RNN using `for loop`\")\n",
                "line_tensor = lineToTensor('Albert')\n",
                "hidden = torch.zeros(1, n_hidden)\n",
                "\n",
                "# Solution\n",
                "for i in range(line_tensor.size()[0]): # 是获取 PyTorch 张量（tensor）line_tensor 在第一个维度上的大小。\n",
                "    output, hidden = rnn(line_tensor[i], hidden)\n",
                "\n",
                "print(f\"\\nFor given Albert, Likelihood <{output.size()}> for each label is:\")\n",
                "print(output)\n",
                "\n",
                "print(f\"\\nNext hidden state <{hidden.size()}> is:\")\n",
                "print(hidden)"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Task 2\n",
                "\n",
                "Write code using pytorch operators to convert these to probabilities by exponentiating them (ie, f(x) = exp(x)). Print the result. You should find that they are all around 5% - 6%."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 35,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "tensor([[0.0501, 0.0609, 0.0546, 0.0530, 0.0550, 0.0564, 0.0539, 0.0591, 0.0571,\n",
                        "         0.0571, 0.0509, 0.0579, 0.0544, 0.0564, 0.0582, 0.0558, 0.0551, 0.0541]],\n",
                        "       grad_fn=<ExpBackward0>)\n"
                    ]
                }
            ],
            "source": [
                "# Solution\n",
                "probs = torch.exp(output)\n",
                "print(probs)"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Training\n",
                "\n",
                "### Preparing for Training\n",
                "\n",
                "Before going into training we should make a few helper functions. The\n",
                "first is to interpret the output of the network, which we know to be a\n",
                "likelihood of each category. We can use ``Tensor.topk`` to get the index\n",
                "of the greatest value:\n",
                "\n",
                "\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 36,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "('Chinese', 1)\n"
                    ]
                }
            ],
            "source": [
                "def categoryFromOutput(output):\n",
                "    # top_n：包含前 k 个最大值的张量。\n",
                "    # top_i：包含前 k 个最大值在张量中的索引，是一种tensor，类似于tensor([1，2，3])。\n",
                "    top_n, top_i = output.topk(1)\n",
                "    category_i = top_i[0].item() # 输出top_i中的第一个值，并提取出来\n",
                "    return all_categories[category_i], category_i\n",
                "\n",
                "print(categoryFromOutput(output))"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "We will also want a quick way to get a training example (a name and its\n",
                "language). We use randomness here as training on the same instances in the same order can lead to worse results as we overfit that particular sequence of samples.\n",
                "\n",
                "\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 37,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Here are 10 examples of randomly choosing data samples:\n",
                        "category = Polish / line = Szwarc\n",
                        "category = Czech / line = Quasninsky\n",
                        "category = Italian / line = Pontecorvo\n",
                        "category = French / line = Beringer\n",
                        "category = Spanish / line = Chavez\n",
                        "category = French / line = Lestrange\n",
                        "category = Japanese / line = Yasujiro\n",
                        "category = Russian / line = Tzeizik\n",
                        "category = Arabic / line = Maroun\n",
                        "category = Arabic / line = Haddad\n"
                    ]
                }
            ],
            "source": [
                "import random\n",
                "\n",
                "def randomChoice(l):\n",
                "    return l[random.randint(0, len(l) - 1)]\n",
                "\n",
                "def randomTrainingExample():\n",
                "    category = randomChoice(all_categories) # 从所有类别 all_categories 中随机选择一个类别。\n",
                "    line = randomChoice(category_lines[category]) # 从这个类别对应的训练数据（category_lines[category]）中随机选择一行（即一个句子）。\n",
                "    category_tensor = torch.tensor([all_categories.index(category)], dtype=torch.long)\n",
                "    line_tensor = lineToTensor(line)\n",
                "    return category, line, category_tensor, line_tensor\n",
                "\n",
                "print(\"Here are 10 examples of randomly choosing data samples:\")\n",
                "for i in range(10):\n",
                "    category, line, category_tensor, line_tensor = randomTrainingExample()\n",
                "    print('category =', category, '/ line =', line)"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Training the Network\n",
                "\n",
                "Now all it takes to train this network is show it a bunch of examples,\n",
                "have it make guesses, and tell it if it's wrong.\n",
                "\n",
                "For the loss function ``nn.NLLLoss`` is appropriate, since the last\n",
                "layer of the RNN is ``nn.LogSoftmax``.\n",
                "\n",
                "\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 38,
            "metadata": {},
            "outputs": [],
            "source": [
                "criterion = nn.NLLLoss()"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Each loop of training will:\n",
                "\n",
                "-  Create input and target tensors\n",
                "-  Create a zeroed initial hidden state\n",
                "-  Read each letter in and do the calculation, keeping the hidden state for the next letter\n",
                "-  Compare final output to target\n",
                "-  Back-propagate\n",
                "-  Return the output and loss\n",
                "\n",
                "\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 39,
            "metadata": {},
            "outputs": [],
            "source": [
                "learning_rate = 0.005 # If you set this too high, it might explode. If too low, it might not learn\n",
                "\n",
                "def train(category_tensor, line_tensor):\n",
                "    hidden = rnn.initHidden()\n",
                "\n",
                "    rnn.zero_grad()\n",
                "\n",
                "    for i in range(line_tensor.size()[0]):\n",
                "        output, hidden = rnn(line_tensor[i], hidden)\n",
                "\n",
                "    loss = criterion(output, category_tensor)\n",
                "    loss.backward()\n",
                "\n",
                "    # Add parameters' gradients to their values, multiplied by learning rate\n",
                "    for p in rnn.parameters():\n",
                "        p.data.add_(p.grad.data, alpha=-learning_rate)\n",
                "\n",
                "    return output, loss.item()"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Now we just have to run that with a bunch of examples. Since the\n",
                "``train`` function returns both the output and loss we can print its\n",
                "guesses and also keep track of loss for plotting. Since there are 1000s\n",
                "of examples we print only every ``print_every`` examples, and take an\n",
                "average of the loss.\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 40,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "2500 5% (0m 1s) 2.8517 Ko / Vietnamese ✗ (Korean)\n",
                        "5000 10% (0m 2s) 2.6615 Seif / Korean ✗ (Arabic)\n",
                        "7500 15% (0m 3s) 1.4088 Naser / Arabic ✓\n",
                        "10000 20% (0m 5s) 2.1775 Opova / Italian ✗ (Czech)\n",
                        "12500 25% (0m 6s) 2.0005 Kim / Korean ✗ (Vietnamese)\n",
                        "15000 30% (0m 7s) 2.2541 Aiza / Japanese ✗ (Spanish)\n",
                        "17500 35% (0m 9s) 3.4929 Olmos / Greek ✗ (Spanish)\n",
                        "20000 40% (0m 10s) 2.0028 Araullo / Spanish ✗ (Portuguese)\n",
                        "22500 45% (0m 11s) 0.1132 Hayashida / Japanese ✓\n",
                        "25000 50% (0m 13s) 0.5207 Paszek / Polish ✓\n",
                        "27500 55% (0m 14s) 0.0114 Haritopoulos / Greek ✓\n",
                        "30000 60% (0m 15s) 2.2480 Senft / English ✗ (German)\n",
                        "32500 65% (0m 16s) 0.8484 Cathasach / Irish ✓\n",
                        "35000 70% (0m 18s) 0.6753 Janick / Czech ✓\n",
                        "37500 75% (0m 19s) 1.3637 Sierra / Portuguese ✗ (Spanish)\n",
                        "40000 80% (0m 20s) 0.7659 Watson / Scottish ✓\n",
                        "42500 85% (0m 22s) 0.5092 Hung / Korean ✓\n",
                        "45000 90% (0m 23s) 0.4409 Messner / German ✓\n",
                        "47500 95% (0m 24s) 0.2713 Malouf / Arabic ✓\n",
                        "50000 100% (0m 26s) 1.4173 Ha / Korean ✗ (Vietnamese)\n"
                    ]
                }
            ],
            "source": [
                "import time\n",
                "import math\n",
                "\n",
                "n_iters = 50000\n",
                "print_every = 2500\n",
                "plot_every = 500\n",
                "\n",
                "# Keep track of losses for plotting\n",
                "current_loss = 0\n",
                "all_losses = []\n",
                "\n",
                "def timeSince(since):\n",
                "    now = time.time()\n",
                "    s = now - since\n",
                "    m = math.floor(s / 60)\n",
                "    s -= m * 60\n",
                "    return '%dm %ds' % (m, s)\n",
                "\n",
                "start = time.time()\n",
                "\n",
                "for iter in range(1, n_iters + 1):\n",
                "    category, line, category_tensor, line_tensor = randomTrainingExample()\n",
                "    output, loss = train(category_tensor, line_tensor)\n",
                "    current_loss += loss\n",
                "\n",
                "    # Print ``iter`` number, loss, name and guess\n",
                "    if iter % print_every == 0:\n",
                "        guess, guess_i = categoryFromOutput(output)\n",
                "        correct = '✓' if guess == category else '✗ (%s)' % category\n",
                "        print('%d %d%% (%s) %.4f %s / %s %s' % (iter, iter / n_iters * 100, timeSince(start), loss, line, guess, correct))\n",
                "\n",
                "    # Add current loss avg to list of losses\n",
                "    if iter % plot_every == 0:\n",
                "        all_losses.append(current_loss / plot_every) # 表示每500次迭代计算一次平均损失，并将结果绘制到图表中，以便跟踪训练过程中的损失变化。\n",
                "        current_loss = 0"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Plotting the Results\n",
                "\n",
                "Plotting the historical loss from ``all_losses`` shows the network\n",
                "learning.\n",
                "\n",
                "Note that learning is fast and fairly smooth at first, but then the improvements become smaller and more variable.\n",
                "\n",
                "\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 41,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/plain": [
                            "[<matplotlib.lines.Line2D at 0x7fa68272de50>]"
                        ]
                    },
                    "execution_count": 41,
                    "metadata": {},
                    "output_type": "execute_result"
                },
                {
                    "data": {
                        "image/png": "iVBORw0KGgoAAAANSUhEUgAAAiMAAAGdCAYAAADAAnMpAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjkuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8hTgPZAAAACXBIWXMAAA9hAAAPYQGoP6dpAABfR0lEQVR4nO3deXxU1d0/8M+dJZNtZkISsieQsEPYZF9ENrGIWJ9qtaLi+jxaWbRUW6l9tIst/mxrfdQWW6tSyyJFEam1KIoEURAIhLCGJQnZ92Rmsmdm7u+PmXszk2SSTDLJzfJ5v17zKjNz752TC3U+Oed7zhFEURRBREREpBCV0g0gIiKiwY1hhIiIiBTFMEJERESKYhghIiIiRTGMEBERkaIYRoiIiEhRDCNERESkKIYRIiIiUpRG6QZ0ht1uR0FBAfR6PQRBULo5RERE1AmiKMJisSAmJgYqlef+j34RRgoKChAfH690M4iIiKgLcnNzERcX5/H9fhFG9Ho9AMcPYzAYFG4NERERdYbZbEZ8fLz8Pe5Jvwgj0tCMwWBgGCEiIupnOiqxYAErERERKYphhIiIiBTFMEJERESKYhghIiIiRTGMEBERkaIYRoiIiEhRDCNERESkKIYRIiIiUhTDCBERESmKYYSIiIgUxTBCREREimIYISIiIkUN6jByMKMEG3amIbO0WummEBERDVr9YtfeniCKIl75/DLScquwJy0ft06OwdrFozAyIljpphEREQ0qg7ZnRBAE/Oq7E7B0XATsIrAnrQA3/jEF63ecQlZZjdLNIyIiGjQEURRFpRvREbPZDKPRCJPJBIPB4PPrn8034f++uIz954sBAMYALT770QJEGvx9/llERESDRWe/vwdtz4ir5Fgj3lw9HR+vm4+xUXqY6prw7Idn0Q9yGhERUb/HMOIiOdaI//vBVGjVAj6/UIx/pRcq3SQiIqIBj2GkhTFReqxbPAoA8PxHZ1FW3aBwi4iIiAY2hpE2/HDhCIyLNqCytgnP7z2ndHOIiIgGNIaRNmjVKvzujklQqwT8O70Q+85yuIaIiKinMIx4kBxrxGM3JAEAfr7nHKpqGxVuERER0cDEMNKOdc5F0MqqG/Dy/ktKN4eIiGhAYhhph79WjV/dOgEA8N6xXBRU1SncIiIiooGHYaQDc0eGY1ZiKBptdvzpyytKN4eIiGjA8SqMbNq0CTNmzIBer0dERARuu+02ZGRkdHjetm3bMHnyZAQGBiI6OhoPPvggysvLu9zo3vajG0cDAP55Ihd5lbUKt4aIiGhg8SqMpKSkYM2aNTh69Cj2798Pq9WKZcuWoabG814uhw8fxurVq/Hwww/j3Llz2LVrF44fP45HHnmk243vLbOTwjAnKQxNNhF/+vKq0s0hIiIaULq1N01paSkiIiKQkpKCBQsWtHnM73//e2zevBlXrzZ/ib/22mt46aWXkJub26nP6em9aTrjWFYF7vzLEWhUAg4+vRBxQwIVaQcREVF/0St705hMJgBAaGiox2Pmzp2LvLw8fPLJJxBFEcXFxXj//fexYsUKj+c0NDTAbDa7PZQ2MzEU80aGwWoXWTtCRETkQ10OI6IoYsOGDZg/fz6Sk5M9Hjd37lxs27YNd911F/z8/BAVFYWQkBC89tprHs/ZtGkTjEaj/IiPj+9qM33qR0sdtSO7TuQht4K1I0RERL7Q5TCydu1apKenY8eOHe0ed/78eaxfvx7PPfccUlNTsW/fPmRlZeGxxx7zeM7GjRthMpnkR2eHc3ra9OGhuH5UOKx2Ea8fYO8IERGRL3SpZmTdunXYs2cPDh06hMTExHaPve+++1BfX49du3bJrx0+fBjXX389CgoKEB0d3eHn9YWaEUnqtUrcvvkbqFUCjm5cgqF6naLtISIi6qt6pGZEFEWsXbsWu3fvxoEDBzoMIgBQW1sLlcr9Y9RqtXy9/mbasCGYFGeEzS7i8wvFSjeHiIio3/MqjKxZswZbt27F9u3bodfrUVRUhKKiItTVNa9MunHjRqxevVp+vnLlSuzevRubN29GZmYmvv76a6xfvx4zZ85ETEyM736SXnTThCgAwKfnihRuCRERUf/nVRjZvHkzTCYTFi5ciOjoaPmxc+dO+ZjCwkLk5OTIzx944AG8/PLLeP3115GcnIzvf//7GDNmDHbv3u27n6KX3TQhEgDwzZVyWOqbFG4NERFR/9atdUZ6S1+qGZEs/sNBZJbW4LW7p2Ll5P7Zw0NERNSTemWdkcFs2XgO1RAREfkCw0gXSUM1BzNK0WC1KdwaIiKi/othpIsmx4Ug0qBDdYMV31zpP5v+ERER9TUMI12kUgnyUM1n5zlUQ0RE1FUMI92wzDlUs/98MWz2Pl8HTERE1CcxjHTD7KQwGPw1KKtuxMmcSqWbQ0RE1C8xjHSDVq3CknGO3pHPOKuGiIioSxhGumnZeEcY+fRccb9c3p6IiEhpDCPddMOYodBpVMipqMXFIovSzSEiIup3GEa6KdBPg+tHDQUA/Ocsh2qIiIi8xTDiAysnRwMA/v5NNqpqGxVuDRERUf/CMOIDt0yKwdgoPUx1Tfi/Ly4r3RwiIqJ+hWHEB9QqAT9fMR4A8I8j15BZWq1wi4iIiPoPhhEfmT8qHIvHRsBqF7HpPxeVbg4REVG/wTDiQz+7eSzUKgH7zxfjm6tlSjeHiIioX2AY8aGREXrcMysBAPDCxxe4RDwREVEnMIz42JNLR0Pvr8H5QjM+OJmndHOIiIj6PIYRHwsN8sP6xaMAAL/7NAN1jTaFW0RERNS3MYz0gNVzhyHSoEOppQHHsyuUbg4REVGfxjDSA3QaNaYNGwIAuFBoVrg1REREfRvDSA8ZF2UAwDBCRETUEYaRHjIuWgoj3DyPiIioPQwjPWR8jCOMXC2tRoOVRaxERESeMIz0kGijP4wBWljtIi4Xc3l4IiIiTxhGeoggCBgXrQfAuhEiIqL2MIz0INaNEBERdYxhpAc1hxH2jBAREXnCMNKDxkthpMgMUeQ+NURERG1hGOlBIyOCoVYJqKptQpG5XunmEBER9UkMIz3IX6vGiKFBADhUQ0RE5AnDSA9jESsREVH7GEZ6mBRGzrNnhIiIqE1ehZFNmzZhxowZ0Ov1iIiIwG233YaMjIwOz2toaMCzzz6LYcOGQafTYcSIEXj77be73Oj+hDNqiIiI2qfx5uCUlBSsWbMGM2bMgNVqxbPPPotly5bh/PnzCAoK8njenXfeieLiYrz11lsYOXIkSkpKYLVau934/kBa+Cy7rAZ1jTYE+KkVbhEREVHf4lUY2bdvn9vzd955BxEREUhNTcWCBQs8npOSkoLMzEyEhoYCAIYPH9611vZDEXp/hAf7oay6ERnFFkyJD1G6SURERH1Kt2pGTCYTAMghoy179+7F9OnT8dJLLyE2NhajR4/GU089hbq6Oo/nNDQ0wGw2uz36Mw7VEBEReeZVz4grURSxYcMGzJ8/H8nJyR6Py8zMxOHDh+Hv748PP/wQZWVlePzxx1FRUeGxbmTTpk345S9/2dWm9Tnjow346nIZwwgREVEbutwzsnbtWqSnp2PHjh3tHme32yEIArZt24aZM2fi5ptvxssvv4wtW7Z47B3ZuHEjTCaT/MjNze1qM/sE9owQERF51qWekXXr1mHv3r04dOgQ4uLi2j02OjoasbGxMBqN8mvjxo2DKIrIy8vDqFGjWp2j0+mg0+m60rQ+SQojFwstEEURgiAo3CIiIqK+w6ueEVEUsXbtWuzevRsHDhxAYmJih+fMmzcPBQUFqK6ull+7dOkSVCpVh0FmoEgaGgQ/tQqWBivyKj3XyhAREQ1GXoWRNWvWYOvWrdi+fTv0ej2KiopQVFTkNtyyceNGrF69Wn6+atUqhIWF4cEHH8T58+dx6NAhPP3003jooYcQEBDgu5+kD9OqVRgVGQwAOFfAoRoiIiJXXoWRzZs3w2QyYeHChYiOjpYfO3fulI8pLCxETk6O/Dw4OBj79+9HVVUVpk+fjnvuuQcrV67Eq6++6rufoh9g3QgREVHbvKoZEUWxw2O2bNnS6rWxY8di//793nzUgMMwQkRE1DbuTdNLJsQ4wgiHaYiIiNwxjPQSKYzkV9WhvLpB4dYQERH1HQwjvUTvr0XSUMf+PWfyTQq3hoiIqO9gGOlFk2Ida62cyWMYISIikjCM9KJkKYywZ4SIiEjGMNKLJsWFAGAYISIicsUw0osmxBggCEChqR6lFhaxEhERAQwjvSpIp8GIoY6VWM+yd4SIiAgAw0ivk4pY01nESkREBIBhpNdNjJOKWKuUbQgREVEfwTDSyyayZ4SIiMgNw0gvGx9jgEoASiwNKDbXK90cIiIixTGM9LJAPw1GRjiKWLn4GREREcOIIibGhgAA0jmjhoiIiGFECZOcRayc3ktERMQwoohklyJWURQVbg0REZGyGEYUMD7aALVKQFl1A4pYxEpERIMcw4gCAvzUGOUsYuUUXyIiGuwYRhQirTfCuhEiIhrsGEYUIhWxsmeEiIgGO4YRhUyMCwEAnMlnESsREQ1uDCMKGRulh0YloKKmEQUmFrESEdHgxTCiEH+tGqMj9QCAtJwqZRtDRESkIIYRBc0YPgQAcDSzXOGWEBERKYdhREFzRoQBAI4wjBAR0SDGMKKgWYlhEATgSkk1Si0NSjeHiIhIEQwjChoS5IexUQYAHKohIqLBi2FEYbOTQgFwqIaIiAYvhhGFzUly1I2wZ4SIiAYrhhGFSXUjmaU1KOameURENAgxjCjMGKjFhBjWjRAR0eDFMNIHzE7kUA0REQ1eXoWRTZs2YcaMGdDr9YiIiMBtt92GjIyMTp//9ddfQ6PRYMqUKd62c0CT1xu5yjBCRESDj1dhJCUlBWvWrMHRo0exf/9+WK1WLFu2DDU1NR2eazKZsHr1aixZsqTLjR2oZiSGQiUA2eW1KDTVKd0cIiKiXqXx5uB9+/a5PX/nnXcQERGB1NRULFiwoN1zH330UaxatQpqtRp79uzxuqEDmcFfi+RYI9LzTDiaWY7/mhqndJOIiIh6TbdqRkwmEwAgNDS03ePeeecdXL16Fc8//3ynrtvQ0ACz2ez2GOikKb4cqiEiosGmy2FEFEVs2LAB8+fPR3JyssfjLl++jGeeeQbbtm2DRtO5jphNmzbBaDTKj/j4+K42s9+YzX1qiIhokOpyGFm7di3S09OxY8cOj8fYbDasWrUKv/zlLzF69OhOX3vjxo0wmUzyIzc3t6vN7DdmDA+FWiUgt6IOeZW1SjeHiIio13hVMyJZt24d9u7di0OHDiEuznN9g8ViwYkTJ3Dq1CmsXbsWAGC32yGKIjQaDT777DMsXry41Xk6nQ46na4rTeu3gnUaTIw1Ii23CkczK3DHtEClm0RERNQrvAojoihi3bp1+PDDD3Hw4EEkJia2e7zBYMCZM2fcXvvzn/+MAwcO4P333+/w/MFmzogwpOVW4cjVctwxjUWsREQ0OHgVRtasWYPt27fjo48+gl6vR1FREQDAaDQiICAAgGOIJT8/H++++y5UKlWrepKIiAj4+/u3W2cyWM1JCsPmg1fxbRbrRoiIaPDwqmZk8+bNMJlMWLhwIaKjo+XHzp075WMKCwuRk5Pj84YOBlMTQiAIQF5lHUotDUo3h4iIqFcIoiiKSjeiI2azGUajESaTCQaDQenm9Khlf0zBpeJqvLl6Om4cH6l0c4iIiLqss9/f3Jumj5kaPwQAkJZbqXBLiIiIegfDSB8zJSEEAHAqp0rRdhAREfUWhpE+ZqozjKTnmWCz9/kRNCIiom5jGOljRkXoEeSnRnWDFVdKqpVuDhERUY9jGOlj1CoBk+JCALBuhIiIBgeGkT6IdSNERDSYMIz0QVPiQwAAablViraDiIioNzCM9EFTnWEko9iC6garso0hIiLqYQwjfVCEwR+xIQEQRSA9r0rp5hAREfUohpE+inUjREQ0WDCM9FFTWTdCRESDBMNIHzXVpWekH2wfRERE1GUMI33UhBgjNCoBZdUNyK+qU7o5REREPYZhpI/y16oxPsaxwyHrRoiIaCBjGOnDuN4IERENBgwjfVhz3QiXhSciooGLYaQPmxI/BABwtsCMRqtd4dYQERH1DIaRPmx4WCBCArVotNpxodCsdHOIiIh6BMNIHyYIgrzeyFeXS5VtDBERUQ9hGOnjbpkUAwD4x9FrHKohIqIBiWGkj1s5OQYReh2KzQ341+kCpZtDRETkcwwjfZyfRoX75w4HALz5VSZXYyUiogGHYaQfuGdWAgK0alwssuDrK+VKN4eIiMinGEb6gZBAP9w5PQ4A8LfDmQq3hoiIyLcYRvqJh+YnQhCAgxmluFxsUbo5REREPsMw0k8MCwvCsvGRAIC/fZWlcGuIiIh8h2GkH/nv65MAAB+eykeppUHh1hAREfkGw0g/Mm3YEEyJD0GjzY5/HMlWujlEREQ+wTDSjwiCIPeObPs2B3Y7p/kSEVH/xzDSzyybEIkArRrlNY24UlqtdHOIiIi6jWGkn9GqVZji3K/m5LVKZRtDRETkAwwj/dC0YUMAAKkMI0RENAB4FUY2bdqEGTNmQK/XIyIiArfddhsyMjLaPWf37t248cYbMXToUBgMBsyZMweffvpptxo92MlhJIdhhIiI+j+vwkhKSgrWrFmDo0ePYv/+/bBarVi2bBlqamo8nnPo0CHceOON+OSTT5CamopFixZh5cqVOHXqVLcbP1hNTQgBAGSW1qCiplHZxhAREXWTIHZj57XS0lJEREQgJSUFCxYs6PR5EyZMwF133YXnnnuuU8ebzWYYjUaYTCYYDIauNndAWfKHg7haWoO37p+OJeMilW4OERFRK539/u5WzYjJZAIAhIaGdvocu90Oi8XS7jkNDQ0wm81uD3LHuhEiIhoouhxGRFHEhg0bMH/+fCQnJ3f6vD/84Q+oqanBnXfe6fGYTZs2wWg0yo/4+PiuNnPAYhghIqKBosthZO3atUhPT8eOHTs6fc6OHTvwi1/8Ajt37kRERITH4zZu3AiTySQ/cnNzu9rMAUsKI6fzqtBksyvcGiIioq7TdOWkdevWYe/evTh06BDi4uI6dc7OnTvx8MMPY9euXVi6dGm7x+p0Ouh0uq40bdBICg+GMUALU10TLhSaMSkuROkmERERdYlXPSOiKGLt2rXYvXs3Dhw4gMTExE6dt2PHDjzwwAPYvn07VqxY0aWGkjuVSsB1zlk1HKohIqL+zKswsmbNGmzduhXbt2+HXq9HUVERioqKUFdXJx+zceNGrF69Wn6+Y8cOrF69Gn/4wx8we/Zs+Ryp+JW6ThqqOZlTpWxDiIiIusGrMLJ582aYTCYsXLgQ0dHR8mPnzp3yMYWFhcjJyZGf/+Uvf4HVasWaNWvcznniiSd891MMUtdJYYQ9I0RE1I95VTPSmSVJtmzZ4vb84MGD3nwEeWFyXAjUKgH5VXUoNNUh2higdJOIiIi8xr1p+rEgnQbjovUAgJPXqpRtDBERURcxjPRz0xK43ggREfVvDCP93HXcNI+IiPo5hpF+TppRcy7fhPomm8KtISIi8h7DSD8XGxKACL0OVruI9DxOlyYiov6HYaSfEwRB7h05nl2hcGuIiIi8xzAyAMwZEQYAOJhRonBLiIiIvMcwMgAsGRcJwDGjpry6QeHWEBEReYdhZACIDQnAhBgD7CJw4CJ7R4iIqH9hGBkgbhzv6B3Zf75Y4ZYQERF5h2FkgJDCyFeXyzjFl4iI+hWGkQFifLQBsSEBqGuy4fDlMqWbQ0RE1GkMIwOEIAhYOi4CAIdqiIiof2EYGUBuHB8FAPjiYjFs9o53WCYiIuoLGEYGkFlJodD7a1BW3Yi03Cqlm0NERNQpDCMDiFatwqIxHKohIqL+hWFkgFkqT/EtUrglREREncMwMsAsHDMUWrWAq6U1yCytVro5REREHWIYGWAM/lrMTnLsVcOhGiIi6g8YRgYgrsZKRET9CcPIALRU2jgvpxKlFs8b5z296zRufDkF1Q3W3moaERFRKwwjA1BMSAAmxxkhisC+c20XsuZX1WFXah4ul1TjNKcBExGRghhGBqibJ0YDAD5JL2zz/X+nF8h/zq+q65U2ERERtYVhZICSwsi3WeVtDtX863RzSClgGCEiIgUxjAxQ8aGBmBxnhL2NoZrsshqcyTfJzxlGiIhISQwjA9iKSY7eEdchGQD42PlcoxIAAAVV9b3bMCIiIhcMIwPY8mRHGDmWVYESS3Pg+NhZR3LrlBgArBkhIiJlMYwMYPGhgZgcHwK7CHx61jFUc7nYgotFFmjVAh6alwjAEUZEkbv8EhGRMhhGBrhbnIWs/z7j6A35l7NXZMGooRgdqYcgAI1WO8prGhVrIxERDW4MIwPc8olRAIBvnUM1H5921IvcMjkafhoVIvQ6ACxiJSIi5TCMDHBxQwIxJT4Eogi8/NklZJbVQKdRyau0xoQEAGAYISIi5XgVRjZt2oQZM2ZAr9cjIiICt912GzIyMjo8LyUlBdOmTYO/vz+SkpLwxhtvdLnB5L0VzqGa947nAgAWjYmA3l8LoDmM5HNGDRERKcSrMJKSkoI1a9bg6NGj2L9/P6xWK5YtW4aamhqP52RlZeHmm2/G9ddfj1OnTuFnP/sZ1q9fjw8++KDbjafOkYZqJCsnx8h/ju2gZ+RgRgme/fAM968hIqIeo/Hm4H379rk9f+eddxAREYHU1FQsWLCgzXPeeOMNJCQk4JVXXgEAjBs3DidOnMDvf/973H777V1rNXlFGqpJy61CoJ8ai8dGyO/FGP0BeA4jL/7nIi4WWRBl8Me6JaN6pb1ERDS4dKtmxGRyrOIZGhrq8ZgjR45g2bJlbq/ddNNNOHHiBJqamrrz8eSF26+LBeBYJj7ATy2/HjskEEDbYaTRaseVkmoAwNZvr6HJZu+FlhIR0WDjVc+IK1EUsWHDBsyfPx/JyckejysqKkJkZKTba5GRkbBarSgrK0N0dHSrcxoaGtDQ0Lyfitls7mozyene2cMwLCwI04YNcXs9JsTRM9LWwmfZ5TWw2h3rjxSbG/DpuSLcMimm1XFERETd0eWekbVr1yI9PR07duzo8FhBENyeSwtstXxdsmnTJhiNRvkRHx/f1WaSkyAIWDB6KIJ07vlTqhkpq25EfZPN7b2MIovb879/k92jbSQiosGpS2Fk3bp12Lt3L7788kvExcW1e2xUVBSKitw3aispKYFGo0FYWFib52zcuBEmk0l+5ObmdqWZ1AnGAC0CncM2hSb3GTVSGFk8NgIalYDj2ZU467LBHhERkS94FUZEUcTatWuxe/duHDhwAImJiR2eM2fOHOzfv9/ttc8++wzTp0+HVqtt8xydTgeDweD2oJ4hCILHtUYyih1h5PpR4VjunB7M3hEiIvI1r8LImjVrsHXrVmzfvh16vR5FRUUoKipCXV3zl9jGjRuxevVq+fljjz2Ga9euYcOGDbhw4QLefvttvPXWW3jqqad891NQtzSvNeIeRi45w8iYKD0emDscAPDR6QJUcOl4IiLyIa/CyObNm2EymbBw4UJER0fLj507d8rHFBYWIicnR36emJiITz75BAcPHsSUKVPw61//Gq+++iqn9fYhsSGtp/fWNlqRU1ELABgTqcd1CSGYGGtEo9WOHcdy2rwOERFRV3g1m6YzO7tu2bKl1Ws33HADTp486c1HUS+KMbYeprlSUg1RBMKD/RAW7Ni/5v65w/HUrtPYdvQaHl2QBI2auwkQEVH38duEXGpGmgtYpeLV0ZF6+bVbJkUjLMgPBaZ67D9f3LuNJCKiAYthhNqsGWkrjPhr1bh7ZgIA4O2vs9q9ZnZZDYpM3O+GiIg6xjBCiBvSHEakobgMl+JVV/fOHgY/tQrHsytxLKuizetlllbjplcO4fbN38Bu73hoj4iIBjeGEUKkwR+C4Fj+vdw5U+aShzASZfTHHdMda8u8duBym9f7w/5LaLDakV9Vh2vOIlgiIiJPGEYIfhoVIvSOItWCqjpU1Tai2OxYjn9URHCr4394wwhoVAK+ulyGUzmVbu+dyTPh3+mF8nMukkZERB1hGCEAcFv47FKxY3O82JAA6P1bL0wXHxqI/5rq2HjvtQNX3N576dOLAACVc6X/swUMI0RE1D6GEQLgWsRa77FexNWaRSOhEoADF0vk3o+vr5Thq8tl0KoF/HDhCADAuXxuckhERO1jGCEAzRvmFVTVIaPIESBcZ9K0NDw8CLdOduzg+9qByxBFES/tc/SK3DNrGJYnO5aPP5Nv6tT6NERENHgxjBAAIMbYvArrpSLHMM2YqNb1Iq7WLh4JQQA+PVeM//viMk7nmRDop8aaRSMxKjIYWrUAU10T8irr2r0OERENbgwjBMB9rRF5mCay/Q0KR0bocbOzB+SVzx0zax6Zn4iheh10GrXcs3KOdSNERNQOhhEC0BxGLhZZYKprglolIGloUIfnrV08Uv7zkEAtHlmQJD9PjjECAM6yboSIiNrBMEIAmhc+a7TaAQDDwwLhr1V3eN64aANunhgFAFi3eBQMLrNvkuOcYYQ9I0RE1A6vNsqjgcsYoEWgnxq1jTYA7c+kaen335+M++cMx8zEULfXk2McwzxnnUWsgiD4rsFERDRgsGeEAACCIMhDNUD7M2laCvTTYFZSWKuwMS7aALVKQFl18yJqRERELTGMkMw1jIzxIox44q9Vyyu4ciVWIiLyhGGEZLEh/vKfR3sxTNOeCTGsGyEiovYxjJAsxujoGfHTqDA8rOOZNJ2RHCvVjXBGDRERtY1hhGSxzhk1oyKCoVb5ptg0OVaa3sueESIiahtn05BsybhI3DIpGrdNifXZNcdHGyAIQJG5HqWWBgx17g5MREQkYc8IyYwBWry+6josHR/ps2sG6TRICncM+XAlViIiagvDCPU4aajmXAHrRoiIqDWGEepx0rLwZ/LYM0JERK0xjFCPk4tYOUxDRERtYBihHjfeuSx8XmUdqmobFW4NERH1NQwj1OOMAVoMCwsEwLoRIiJqjWGEeoU0VJPOuhEiImqBYYR6xbSEIQCAry6XKtwSIiLqaxhGqFcsGRcBADiWVQFzfZPCrQFEUcSvPz6Pv6RcVbopRESDHsMI9YphYUEYGREMq13EoUvK945cKanGW4ez8OK+izDVKR+OiIgGM4YR6jVLxjp6R764UNLla9jtIj48lYfcitputSXHeb4oAqdyKrt1LSIi6h6GEeo1S8Y5lpn/MqMENrvYpWu8dzwXP9p5Grdv/gYFVXVdbkuOS5hJvcYwQkSkJIYR6jXXJYTAGKBFVW0TTnahN8JuF/G3w5kAgBJLAx7achyWLtaf5FY0B5kT2QwjRERK8jqMHDp0CCtXrkRMTAwEQcCePXs6PGfbtm2YPHkyAgMDER0djQcffBDl5eVdaS/1Yxq1CovGDAXQtaGalEulyCytgV6nQXiwDheLLFi7/RSsNrvX13LtGUnLrerSNYiIyDe8DiM1NTWYPHkyXn/99U4df/jwYaxevRoPP/wwzp07h127duH48eN45JFHvG4s9X+LnUM1X1wobvWeKIo4k2dCk4dg8NbhLADAXTPi8db90+GvVSHlUime33sOoujdsI9rzUldkw0XCi1enU9ERL6j8faE5cuXY/ny5Z0+/ujRoxg+fDjWr18PAEhMTMSjjz6Kl156yduPpgHghtFDoVYJuFxSjZzyWiQ4V2YFgNcOXMHL+y/hxvGR+Ot90yAIgvzexSIzDl8pg0oA7p87HPGhgXjlrqn44bZUbPs2B0MC/RA7JAAZRRZkFFmQVVaD26fF4umbxrZqgyiKcs/I8LBAZJfX4sS1CkyMM/b8DSAiolZ6vGZk7ty5yMvLwyeffAJRFFFcXIz3338fK1as8HhOQ0MDzGaz24MGBmOAFjOGOxZA++Jic+9I6rUKvPL5JQDA/vPFePvrbLfz3nb2inwnOQrxoYHyn5+9eRwA4PUvr2Dj7jPY8k02jmSWo8hcj38cudZmj0l5TSPqmmwQBODWyTEAgBMsYiUiUkyvhJFt27bhrrvugp+fH6KiohASEoLXXnvN4zmbNm2C0WiUH/Hx8T3dTOpFS8Y6hmoOXHTUjVjqm/DkzjTYRSBpaBAA4MX/XMDp3CoAQKmlAXvSCgAAD89PcrvWw/MT8dgNIxAbEoDrR4XjkfmJeOn2SVAJgLneilJLQ6vPl3pFog3+mD0iDACQml3p9VAPERH5Ro+HkfPnz2P9+vV47rnnkJqain379iErKwuPPfaYx3M2btwIk8kkP3Jzc3u6mdSLpNVYj2aWw1LfhF/sPY/cijrEhgRgz5p5uHliFJpsItZsPwlTXRO2Hr2GRqsdU+JDMG3YELdrCYKAZ5aPxdfPLMY/Hp6Fn98yHnfOiMfwMEeouVxS3erzpXqR+NBATIkPgVoloMhcjwJTfQ//5ERE1Bava0a8tWnTJsybNw9PP/00AGDSpEkICgrC9ddfjxdeeAHR0dGtztHpdNDpdD3dNFJI0tBgJIYHIausBs9+eBZ7TxdAJQCv/GAKDP5abPreJJzJNyG3og5P7TotL0r28PzETn/GyIhgZJbV4HKxBfNGhru9l1PuCCMJoYEI9NNgQowB6XkmnMiuQOyUWN/9oERE1Ck93jNSW1sLlcr9Y9RqNQCwW3wQk1Zj3XvaMfyyZtFIzBgeCsBRV/L63ddBqxaw/3wxyqobEWP0x/LkqE5ff1RkMAAPPSOVzT0jAOTeFi5+RkSkDK/DSHV1NdLS0pCWlgYAyMrKQlpaGnJycgA4hlhWr14tH79y5Urs3r0bmzdvRmZmJr7++musX78eM2fORExMjG9+Cup3FjuHagBgcnwI1i8Z5fb+5PgQPLN8nPz8/rnDoVF3/p/rqAg9gLbDiFQzktAijHDxMyIiZXg9THPixAksWrRIfr5hwwYAwP33348tW7agsLBQDiYA8MADD8BiseD111/Hj3/8Y4SEhGDx4sX4f//v//mg+dRfzRgeirghATDXNeH/7poCbRtB46F5w3G1tBpXiqtx96wEr64/MsLZM1JsgSiKbtOEpdVXpZ6R6cMcPTIXi8yobrAiWOf5/xY2u4gDF0swd0QYgto5joiIOk8Q+8FYidlshtFohMlkgsFgULo55COmuibY7CJCg/x8fu26RhvGP78Pogic+PlShAc7apAarXaM+d//QBSBY88uQYTeHwAw78UDyK+qwz8enonrRw31eN3t3+bgZx+ewT2zEvCb/5ro83YTEQ0knf3+5t40pBhjgLZHgggABPipET/E0fNxubh5qKagqg6iCPhrVRga3FwkPX145+pGjmdXAHAsZ98PcjwRUb/AMEID1mhnEeuVkual3l3rRVyHbqZ3soj1QqFjAb4icz0yy2p82l4iosGKYYQGrJFtFLG2LF6VXOcMI6dyqmCzt93j0WC14YrLtb65ys0eiYh8gWGEBqxRchFrc4CQFjyLG+IeRsZGGRCs06C6wYqLRW1vP3ClpBpWl6DyzZUyXzeZiGhQYhihAauttUakNUZa9oyoVQKmJoQA8DxUI+3sawzQAgCOZJbD7qEXpSWrzY6q2sbON56IaBBhGKEBa8RQRxgpq25AZY0jCHgapgE6Xm9Eqhe5dXIMgvzUqKptwvnCjjdxtNtFPPT3E5j+wudu9StEROTAMEIDVpBOg9iQAADAlVJH74i0FHx8G2FkpnMF2BPOGTMtSWFkYqwRs5IcG+x9c7XjoZodx3Nw6FIprHYRhy9zaIeIqCWGERrQ5KGa4mqYaptgrrcCAOJDA1odOyXBsWlegake+VV1bu+JoiiHkXHRBsx17vb79ZX2i1iLTPV48ZOL8vPO9KQQEQ02DCM0oMlFrCUWuV4kPFiHQL/Wq6cG+mmQHONYlKdl70ixuQGVtU1QqwSMigzG3BGOzfeOZVWg0Wpv87NFUcTP95yFxWVVV6nuZCC5UlKNYjN3PCairmMYoQFN2qPmSkm1S71I614RyXTnUM3xFmFE6hVJCg+Cv1aNsVF6hAb5oa7JhrTcqjav9cmZInx+oRhatYCX75wMAMgotsBqazu89Eellgbc/OpXuOsvR5RuChH1YwwjNKCNdBmmkcJIW/UikhnOlViPZ7kXsZ53GaIBAJVKwJwRnutGqmob8fzeswCAxxeOxNJxkQjyU6PRah9Qi6WlXnP0DGWX16Ku0aZ0c4ion2IYoQFN2jCvyFyPcwWOQNHWTBqJ1DOSUWyBqbZJfv1ikWN4RQojADDPOVTzTRt1Iy/8+wLKqhsxKiIYjy8aAZVKwFjnuecLBk7dyOk8k/znAlNdO0cSEXnGMEIDmsFfiyiDYzO8lIwSAO33jIQH65AUHgQASM1pHqppLl7Vy69JRayncitR22iVX3/vWA7eT82DIAAv3j4JOo0aADDeGUYuDKAi1vS8KvnPRSbWjRBR1zCM0IAnzaiRZ9IM8RxGgOZN84471xupb7Ih0zk1eLxLz8iwsEDEhgSgySbiWFYFRFHEnw9ewTO7zwAA/mdBkrx2CdDcqzJQZtTY7SLSXXtGqtgzQkRdwzBCA540VCNJCOsojLivN3Kp2AK7CIQF+WGovnmnX0EQXKb4luGFf1/AS/syAACPLxyBZ74z1u2642Oah2kGwo6/2eU1sNQ39wixZ4SIuqr1/EaiAUaaUQMAWrUgD9t4MsMZRk7nmlDfZHNbX8R1p18AmDcyHLtS8/D219nyBns/XzEOj1yf1Oq6YyL1UAlAeU0jSi0NiOigHX2da68IABQwjBBRFzGM0IA3OrK5ZyQ2JABqldDO0cDwsECEB/uhrLoRZ/NN8togrvUiEmlGjc0uQq0S8Ls7JuF718W1ed0APzUSw4NwtbQG5wvNrcLIh6fy8LevsvDHu6ZgdGTrzwIcQyNfZpQgo9iC7LIaZJXVILu8FhNiDHj7/hlQdfCz+dJpZ72IwV8Dc70VRSxgJaIu4jANDXiuwzTtFa9KBEGQe0eOZ1fKNR5jowytjo00+GN2UigC/dR4c/U0j0FEMk4uYnVf/EwURfzhs0s4V2DGrz8+7/H8X318Hg///QRe2peBf57Iw/HsSpRaGnAwo1Re1K23SD0jS8ZFAgAK2TNCRF3EMEIDXkhgc61He9N6XbkufnahxRojLW19eBaOPbsUi8dGdnhduW6kRRHr6TwT8iodPQtfXS7Dt5mtpwtfLrbg3SPZAIBbJkXjR0tH4/9+MAXDnTUwvbl+SZPNjnMFjjBy04QoACxgJaKuYxihQUFaFr4zPSNA8+Jnhy+XwVJvhVYttCqElWjUKnm59454mt77r9MFjms5h1n+8NmlVkWuv/3kAuwicNOESLy+6jo8sXQUvjslVg5JWaW9F0YuFVtQ32SHXqfB3JGOoSpzvRU1DdYOziQiao1hhAaFh+YlYk5SGFZMjO7U8eOjDQj0U6PRuXT7iKHB8NN0//8uUhjJLK1GfZNjxVK7XcTH6Y4w8vzK8fDTqHAsuwJfuezwe/hyGb7MKIVGJeCnLWbpJDrXRcnqxZ4RaYhmYpwRBn8t9M4wxqEaIuoKhhEaFJaOj8SO/5nd6Z4RjVqFqQkh8vPxHoZovDVUr0N4sB/sIpDhXNX1eHYFis0N0PtrcOeMeNw3exgA4A+fZUAURdjsIn7zyQUAwL2zhyFpqHsPzXBFwkgVAGBSXAgAIMroKMbl9F4i6gqGESIPpCJWwHO9iLcEQWi1+Nm/nL0i35kQBZ1GjR8uHIFAPzVO55mw/3wxdp/Mw4VCM/T+GqxfMqrVNZMUCCOncx09I5PjjACA6BDH5oNcEp6IuoJhhMiDnggjgHvdiNVmx3/OFAEAVk6OAeBYkv7BecMBAL//LAO//8yxkNraRSMRGuTX6nrSME1+VZ089NOT6ptsyCh29OpMig8BAEQb2DNCRF3HMELkwZT4EARo1fBTq+RZML4wzmXDvCOZ5SivaURokJ+8misA/M/1I6D31+BScTWKzQ2IDQnA/XOHt3m90CA/GPwdNRvZ5T3fO3KuwAybXUR4sA4xzuGZ6BDH/xayZ4SIuoBhhMiDIJ0GWx+Zib8/NLPNHomukoLNxSILPkpzDNEsT46CRt38f0djoBb/7bKK60++Mwb+WnWb1xMEAYnOOpLsXhiqkepFJscZ5RVpY4zOYZoq9owQkfe4AitRO6YNC+34IC8lhQfBT6NCdYMVe51hRBqicfXQ/EQcuFiCaKM/bm3j/ZbXPJ1b1StrjUgzaaTiVYAFrETUPQwjRL1Mo1ZhTKQeZ/JNaLTZEWnQudWnSIJ1GuxZM69T15Sn9/bCWiPSMvCT4o3yazHOYRoWsBJRV3CYhkgBrvvcrJgY0+F+OR3prbVGzPVNyHQGnsluPSOOYRpLvRXVXPiMiLzEMEKkANd1S26Z3LmF2NrTW2HkrHOIJm5IgFsdTbBOA72ziJYb5hGRtxhGiBRw3TDHcvPDwwIx1Tk9tjukhc/Kaxphqm3q9vU8OZ0nrS8S0uq9aKM0o4Z1I0TkHYYRIgVMigvBOw/MwJYHZ8ozUrojWKdBhHMzwKwenN77bZZjA79JccZW70U7h2oKOaOGiLzkdRg5dOgQVq5ciZiYGAiCgD179nR4TkNDA5599lkMGzYMOp0OI0aMwNtvv92V9hINGIvGRsg9Gr4gDdX01PTegqo6HLpUCsCxvH5LPVHEWmiqQ4O15xdyIyJleR1GampqMHnyZLz++uudPufOO+/EF198gbfeegsZGRnYsWMHxo4d2/GJRNRpSUMdYaSnpve+dzwXdhGYkxSGEUNb72AcZXD0jPhqeu/x7ArMffEAfvvvCz65HhH1XV5P7V2+fDmWL1/e6eP37duHlJQUZGZmIjTUMX1x+PDh3n4sEXWgJ4tYrTY7dh7PAQCsmpXQ5jHRcs+Ib8LIgYslEEUgxdkb0xZzfRMeeuc4bhwfiUdvGOGTzyWi3tfjNSN79+7F9OnT8dJLLyE2NhajR4/GU089hbo6z125DQ0NMJvNbg8ial9iuKO3Iqus2ufX/uJiCYrNDQgL8sNNE6LaPCZaXvjMN8M0Z5zFstcqaj3uuXPoUilOXKvEu0eu+eQziUgZPb7oWWZmJg4fPgx/f398+OGHKCsrw+OPP46KigqPdSObNm3CL3/5y55uGtGA4rrwmSiKPimMlWz/1tEr8v3p8fDTtP07THsFrP88kQubXcTdM9vuVWlJFEWcyTc5/wxcKalGcmzrotmLhY4N+4rM9bDZxW6v10JEyujxnhG73Q5BELBt2zbMnDkTN998M15++WVs2bLFY+/Ixo0bYTKZ5Edubm5PN5Oo30sIDYRKAGoabSi1NHh9/t7TBdiwM63VuTnltTh02TFUsqqdMCH1jFgarLDUN08vzimvxU/eT8fG3Wdw2bnbb0dyK+pgqmu+xiUP510scvSa2uxil35mIuobejyMREdHIzY2FkZj828148aNgyiKyMvLa/McnU4Hg8Hg9iCi9vlpVIgbEgjA+yLW6gYrnt19BrtP5eOevx1FeXXzF/uO4zkQReD6UeFICAv0eI0gnUbePdi1iPVf6QXyn/ek5XeqPen5VW7PLxW3PfR0sag5pHAp+v5j/Y5T+M4rh9wCJw1uPR5G5s2bh4KCAlRXN//H5NKlS1CpVIiLi+vpjycaVLpaxLrrRC4szmXcLxVX4963jqGqthGNVjt2nXD0TN4za1iH14kJce7e6xpGTjeHkY/SCmC3ix1eRxqiCXDuVNxWz4ilvgl5lc0BhOub9A9NNjv+lV6Ai0UWvHcsR+nmUB/hdRiprq5GWloa0tLSAABZWVlIS0tDTo7jH9XGjRuxevVq+fhVq1YhLCwMDz74IM6fP49Dhw7h6aefxkMPPYSAgADf/BREBKBra43Y7CK2fJMNAHh4fiLCg/1wodCM+946hg9O5qGsuhEReh2WjIvo8FpRLYpYLxVbcLHIAq1aQJCfGnmVdUjNqezwOlLx6vLkKPk6LbV8rZA9I/1CiaUBojOPbvkmG002u7INoj7B6zBy4sQJTJ06FVOnTgUAbNiwAVOnTsVzzz0HACgsLJSDCQAEBwdj//79qKqqwvTp03HPPfdg5cqVePXVV330IxCRpCtrjRy4WIJr5bUwBmjx42Wjse2R2QgN8sOZfBM27j4DAPjBjHho1R3/50IqYi1w9lLsTXP0itwwOgLLJzr24PnwVPtDNa7Fq9+7ztF7mldZh5oWG/BdKHQPIwXsGekXXGdbFZrq8cmZQgVbQ32F17NpFi5cCFH03M26ZcuWVq+NHTsW+/fv9/ajiMhLXRmmeftwFgDg7pkJCPTTYEyUHlsfnoW73zwKU10TVAJwVydnwTRP762HKIpyvcjKydEID9bh/dQ8/Du9EL9YOcHjrJxr5bWw1Fvhp1FhVlIowoN1KKtuwOWSakxx2cdHKl7V6zSwNFjZM9JPtNy76K3DWbh1coxPZ39R/8O9aYgGECmMXCuvga0TtRnnCkw4klkOtUrA6jnNNSHjYwzY+vAsxA0JwD2zhiE2pHNDqlIYKTDVIT3PhGvltQjQqnHj+EjMTgpDhF4HU10TDmaUeLxGurNXZFy0AVq1CqMjHeuntByWyXAWr14/Otz5mewZ6Q+k4ub5I8Oh06iQnmfC8eyOh+6otYwiC45lVSjdDJ9gGCEaQGKMAfDTqNBkE5Ff2XFPwTtfZwMAbp4YLRefSibGGXH4p4vx69uSO//5zmsUmuqx11m4unR8JAL9NFCrBHx3SgyA9mfVnHWGkUnOdUVGR+oBwG1asCiK8kyahWMctSyFVW3/vKIo4nefXsQ/jnJhtL5AGk4bH2PA7dMcw3B/+ypTySb1S3a7iHv+dhSr3jw6IKa1M4wQDSAqlYDhYdL03vZXYi21NMg1HQ/NG+6Tz5cKWAur6vCxNEQzKVp+/7apsQCAzy+UwFzf9rTO9LwqAMDEFmEkw2V6b4GpHpZ6K7RqAdePcvSMlFY3oNHauhjyYpEFf/ryKv53z1l5oz9STpHZERqjjf54aF4iAGD/heIe2cZgIMuvqkNZdSOsdnFA3DuGEaIBRhqq+fRcMbZ9ew1/+CwDT+06jY270/Hv9EJ5bYetR6+h0WbH1IQQTE0Y4pPPloZpahptKDY3wOCvwQ1jhsrvj482YHRkMBqtduw7U9TqfLtdxLl8Ry3IxDgpjDiGaVx7Ri4WOo4ZMTQYkXp/+KlVEEWg2Nx6qOZKSXOI2bj7jNuCbNT7pJqRaKM/RkYEY/HYCIgi8M7XWQq3rH9xHbbMq6xVsCW+0ePLwRNR73LsUVOMHW2s4bDjWC7UKgHTEobgconjP2YPz0/02WcH+mlgDNDKgec7yVHQadTy+4Ig4LtTYvG7TzPw4al83Dkj3u387PIaWBqs0GlUGBXhCCGjnD0jhaZ6mOqaYAzQykM0Y6P0UKkERBn9kVNRi0JTPeJD3Rdmyyxt/q0xv6oOm/5zEb/9r4k++5nJO1LNSJRz5tUj8xNx4GIJdp3Iw4YbRyMk0E/J5vUbGW5hpP8Xb7NnhGiA+e6UGIyN0iM51oCl4yJx7+wEPH3TGDwyPxEjI4Jhs4s4ll2BytomxBj98R0PG991ldQ7AgC3To5ts30AcDSrvNUMGGlK7/gYAzTOqcTGAC2iDI5rXnEGKDmMRBvcPrOtGTXScNUiZw/N9m9zcPhyWVd+NOomq80u915Jf2dzRoRhXLQBdU02bPuWi6B11iWX1Yc7Ux/W17FnhGiAGRdtwL4nF7T53s8B5FbU4mBGCU7mVOGOaXHyl76vRBv9cbHIgvBgP8xOCm31ftyQQMxMDMWxrArsOpGH9UtGye9Ji51NarEp3qjIYBSZ63GpuBrThoXKwzRjohy9JvLKr22sNSL1jNw1IwHxoYF498g1/PSDdHz6owUI1vE/gb2ptLoBdhHQqASEB+sAOHrLHpmfiB/vOo2tR6/h0QVJPv83ORC51lDlVfX/YRr+jRMNMvGhgbhvznD88a4pmDcy3OfXH+6sWbllUozHL5UfOIdnXv/yirxeCNA8rXdiXIjb8WOkItYiCxqsNnlRt3FRjp6RmJC2e0ZEUURmqeM/2iOGBuGn3xmLuCEByK+qw4v/udDln5G6RqoXiTT4u+2wvGJSNMKC/FBoqsfnFzxP+yYHq82Oqy61UBymISJq4Yc3jMAzy8fix8tGezzmv6bGYvHYCDRa7Vi3/RTqGm3O4lVnGGnRMyJP7y2x4EpJNWx2EcYALSINjt+uW678KimxNKCm0QaVACSEBSJIp8FLt08CAGw9moN/HMnu1F45zderl4eKyHvN9SL+bq/7a9W4yxlQ3z2S3dvN6neyy2vRaLNDWieuoKrOq3/HfRHDCBH5VITBH4/dMAJ6f63HYwRBwO/umISheh0ul1Tj1/8+j8yyGtQ02hCgVWOEc1l7yWjncMyl4mpcLGwuXpVW7fTUM3LV2SsSHxooF9LOHRkuL/D2vx+dwx1vfIMLhWZ05FyBCTe+fAg3/99h5HtY04TaV+ghjADAPbOHQSUA31wtZ+DrgDSzbHy0ARqVgCabiJJ+vtYIwwgRKSIsWIc/3jkFguAoKn15fwYAYIJL8apEmllTamnA0cxyAI7aGElzz4h7SJDqRZLC3cPNc7eMx//eMh5BfmqczKnCLa8dxm8/udBq/xvJpWIL7nvrGEx1TWi02bleSRdJC9NFG1qHkdiQACwdFwkAePcIF6hrT4ZLGJGCXX+f3sswQkSKmT8qHI8uGAEA+MS57khyiyEaAAjSaeQl6fedcxwnFa8CjpVnAaCytgl1jTb5dTmMDA12u55GrcLD8xPx+Y9vwPLkKNjsIv56KBOL/3AQ7x3LgdVlJ9nM0mqsevNbVNQ0ws8Zkr6+wtk4XVEozaTxsL3A6jnDAQAfpOZxPZh2SGuMjI7UI26I417297oRhhEiUtSPl43GZJcN8CbFtQ4jQHP4sNQ7ei/GuoQRQ4AGgX6OYRjXoZos57TepBbDPpJoYwA23zsN7zwwA/GhASg2N+CZ3Wdw0yuHsO9sEa6V12DVm9+irLoBY6P0eH2VY7fyI1fL+/0YvRKKTO7TeluaNzIMSUODUNNo63B358FM2pdpdJQecUMc6+r096FDhhEiUpRWrcKrP5iCYJ0GKgGYNqzt1WBHRTb3bghCc1Gr47ngstZIcxGrNOsmKdy9Z6SlRWMjsP9HN+DnK8ZhSKAWV0tr8NjWVNz48iEUmesxKiIY2x6ZhYVjIhCgVaO8ptFt0SlXhaY6fJlR4ta7Qg6eClglgiBg9WxHPc+7R661u0P8YFXfZEN2uWNIZkykXu4x5DANEVE3DQsLwp41c7HtkdkYFtZ2L8boiObwkRDqmBnjqnmtEcdviA1WG3IrHP+BblkQ2xZ/rRqPXJ+ElJ8swtpFIxGgVaPRZkdieBC2PTILYcE6+GlUmJnoWDulraEaURTx4DvH8eA7x3HLa4dxPNt3O6qKooiXP8vAq19c9tk1e5PNLqLI3H7PCADcPi0OQX5qXCmpxpGr5b3VvH4js9SxI7fBX4NIg47DNEREvjQyQo85I8I8vu9aI+I6RCNp2TOSU14LuwgE6zQYqtd1uh0Gfy2eumkMUp5eiN/8VzL++egcRLgUXM4b6WjjN218UabnmeTVYS8WWfD9N45gw840lFhaL8bmrSOZ5Xj1wBW8vP9Sv/wtuKy6ATa7CLVKQITecxjR+2vxX9c5Vu5lIWtrUr3IGOdsMnmYhmGEiKjnjRgaLK+rMDbK0Op9aUaNVDNyVS5eDZKnAHsjwuCPe2YNaxVk5o5wLBT3bWY5mloMxUh1DkvGRuDumQkQBGD3qXws+X0K/nki1+s2uPpLSqb859Rrld26lhKkkBih17kteNYWqZD1s/NFyCnvf8GrJ7kWrwJo7hnp5FojdruIf57IddtAsi9gGCGifiHAT43hziGccdGte0aktUakhc+kPWkSwzseovHG+GgDhgRqUdNoQ3pelfx6k82OvacLAAD3zhmGTd+biD2Pz8OkOCMsDVb85P10/OnLK12qg7hQaEaKy3Tik/0wjBQ5Q6KnehFXoyP1uH5UOOwi8OyeMwO+duT91DxMfP5TpF7reFjPtWcEcNxPlQA0Wu0oq+54rZFvrpbjJ++n48e7Tnev0T7GMEJE/cZzK8fjoXmJWOJcj8JVy56R5jVG2i9e9ZZKJcjDSV9faR6qSckoRUVNI8KDdbjeucz+5PgQ7Hl8HtYuGgkA+N2nGfjNvy94/eX610OOXpHwYMeOtif6YRgp7GAmTUu/+m4ydBoVvrpchl2peT3ZNMXtOpELS4NVnt7eHqlwepSzhkqrVsn/9vM6MaNGCuln802obWx7XR0lMIwQUb+xaEwEnls5Hto29ryRV2GVekZK25/W2x3Snj6uRazSEM13p7jvyaNSCXjqpjH431vGAwD+djgLP3k/vdOzbfIqa+Uel03fcyxlf6HQ7HGBtr6qOYy0vcZIS4nhQdhwo2NLgRc+Po8Sc/frbvoiu13EuQLHCsDSisGe1DRYkVvhCByjXWaXxXpRxCpNAbbZRaQ7N6bsCxhGiGhAkL7kLA1WWOqbmqf19kQYcdaNnMqpQl2jDaa6Juy/UAzAse9OWx6en4jff38y1CoBu1Lz8Pi2k2i0dhxI3j6cDZtdxLyRYbhxfCRiQwJgF4HTuVU++3l6g7c9I4Djnk2KM8Jcb8XP95wdkMM1WeU1qHYGy47CyGVnnUd4sA5hwc21THFeTO913b/pZE7f6WFjGCGiASFIp4HB3zHd93yBGVW1jhU8fV0zAgDDwgIRGxKARpsdx7Mr8MmZQjRa7RgdGYwJMa2LayV3TIvD5nuug59Ghc/OF2NXavtFrVW1jXjveA4AyCvVXudch6W/DdV4UzMi0ahVeOmOSdCqBXx2vhj/PlPYU83rkqOZ5fjTl1dQ32Tr+GAPzrj0TuRV1rV7rUtFUr2I+9CjN9N7XbdMOJVT5U1TexTDCBENGNJaI9LwSYzRH4F+mvZO6RJBEDBXqhu5WoYPTzqGaL53XVyHM3eWTYjCj53DDx+dKmj32K1Hr6G20Ybx0QZcP8rRGzPdGUY8zaiprGlstUdPX9CVnhHAMXPq8YWOmpvnPzqHippGn7etq57adRq/+zQDD//9eJfrL1yHSkQRyHL26LUlo8VMGok303vdw0hln+ltYhghogFDCiOHnWGk5Z40viTVjfwrrQDHsisgCI56kc747pRYCAJwLLvC4zLe9U02bPkmGwDw6A1JcsiRVqg9mVPZaiqn1WbH7Zu/wY0vp/SpGgu7XUSxWVp9tXM1I67WLBqJ0ZHBKK9pxIv/ueDr5nVJkale7on4+ko5Vr91DOYu7KdzJr/K7Xl7U27lmTQtwkhzzUj7wzRWm13+ewCAsurGPrNYGsMIEQ0Y0m/dp52/bfZEvYhE6hkpcP7GP29EeKeLM6OM/pjlXMn1X6fb7h15PzUPZdWNiA0JwIqJ0fLrY6P0CPRTw1JvlWsIJF9mlCKzrAY1jTY5kPUFZTUNaLKJUAmOdUa85adR4dffTQYAfJxeiAZr14dFfCUt19EzFWXwh8FfgxPXKnGPc0PFzrK5FK9Oce7P1F7diLzGSFTLnhHHv7v8qrp2ezqKLQ2wi4BWLch7QPWVuhGGESIaMKSeEZuzxyCpB+pFJBEGf4yKaO558VS46sl3pziO35vWOow02ex4I+UqAOC/r090m52jUavkL64TLdal2HEsR/7z0cy+s5S6tCfNUL2uzZlQnTEzMRSRBh1qG204mum7Zfa7Sqq3WDwuAjv+ZzbCgvxwJt+EH/z1SKdX3M0srUZtow2Bfmosm+CYri4t1tdSVW0jis2OdURc/90BjuJtQQDqm+wobycMSUM00cYAXJcwxO3nUBrDCBENGC3rEXpymAZoHqoJ0KrxneQor85dnhwFrVrA+UIzLrfYdG9vWgHyKusQHuyHu2YktDp3Wht1IwVVdTiYUSI/P+LDMFJW3YCX9l3E5F9+hoe2HPe6zqDQ1PUhGokgCFg8NgIAcMA5c0lJ0pf41PgQTIgxYuejcxBp0OFScTWe+eBMp64h1YskxxjlvZeuehimuVTseD02JAB6f63be34aFSKdS+y3N+wihZGYEH+5EJo9I0REPtZymKQnh2kA4NYpMVCrBKyaldBq476OhAT64YbRQwFAXkcEcPTq/PngFQDAw/OTEOCnbnWuXDfiEkb+eSIXdtGx0JpaJSC3oq7be9jkVdbi+Y/OYt6LB/Dng1dhqmvCgYsl8jBYZ0k9I9EG74pXW1oy1tF78MXFkh4vvPwgNQ9/Sbna5uc02exId9Z6SF/qIyOC8Zf7pgMAjmVVdGpp9jP5zjASa8QIZ29HZll1m+c2F6+2HbDjOlE3ki+HkQBMdfaunS8wd2s2kK8wjBDRgCEtfAYA/loVYrrxm3hnXJcwBGnP3Yif3TyuS+evnOwoeN17ukD+0tt3tghXS2tgDNDi3tmte0UAYGrCEAgCkF1ei1KLYwO6nccd04QfmjccE2Md9QDfdmM446+HrmLh7w7i70euocFqx+T4EHnH4u3fereBXYFzWm90SPfCyLyR4dBpVMirrGtVL+NLp3Iq8dT7p7HpPxdxqo31XC4WWlDfZIcxQItEl12mJ8QY4KdWobrB2qnCUCmMTIozIn5IALRqAfVNdvl+uUp3tmNcdNtTx+W6kU70jMSGBCBuSACG6nWw2kW5HUpiGCGiAcN1DYvhYUFQdbAhmy/o/bUdbvzmyY3jIxGgVeNaeS1O55kgiiJe/9LRK/LA3OGtuuMlxgCt3K1/MqcSKZdKUGiqx5BALW6aEIXZSY7i2q4O1XyUlo/ffnIRVudia9semYU9j8/FT24aAwD41+lCr2aOFHVxWm9LAX5quXD4iwslHRzdNTa7iP/96CykDpHPz7ceEjrlLF6dEh/i9m9Mq1ZhpLOH43yhud3PsdrsOFfgCAET44zQqFXy3ktt1Y0cy3YEyxnOQNiSNL23/WEax99DTEgABEGQe0dO9YGhGoYRIhowdBq1vH/LiB6uF/GFQD8NbhzvGHr4KC0fBy6W4EKhGUF+ajw4b3i7517nUjey45ijV+T26+Lgr1XLe+d0pYj1ZE4lnn4/HQDw6IIkbHtkNuaNDIcgCJg2bAhGRQSjrsmGPc7l7zvDFzUjksXOfYkOXOyZupHt317D2fzmILG/rTAi1YskhLR6b7xz0bsLHYSRK6XVqG+yI1inkXtXpH+zLaf3Fpnqca28FiqheYiupc5M7y1wGaYBmv8NnbxW1W5bewPDCBENKFLdSE/Xi/iKtDbJx+mFePWAo1fk3jnDEBLo1+550uJn+88X48BFRy/BD2YmyO+pVQLyKuuQW9H5upG8ylr8z7sn0Gi148bxkfjJd8a6vS8IjvoYANj+bU6n6zZ81TMCQC5iTb1WiUofL4BWVt2A332aAQB4atloaFQCLpdUI7vFQmRS0efUhNbBQBpG6SiMSCuvTogxyL0rUq9Ky+m9Uq/I+BgDDB56y1yn93qSLw/TOP4epJ6Rk31g8TOvw8ihQ4ewcuVKxMTEQBAE7Nmzp9Pnfv3119BoNJgyZYq3H0tE1CnjnV8G0vTXvu76UUMREqhFqaUBp3OroNOo8Mj8pA7Pk35Dziqrgc0uYubwUPnLLEinkdeR6GzviKW+CQ9vOYGy6kaMizbglbumtDn89L2pcdBpVLhYZGmznqIlu130aRiJDQnA2Cg97CKQcqm029dz9eJ/LsJcb8WEGAN+uHCkPNzl2jtSXt2Aa+WOgDclLqTVNcZFO4bPLhR1EEZc6kUkIyKcwzQtekaOZTn+DmcOD/N4PddhmraChbm+CZZ6xyqxUmCfFOcodi6xNMjr5SjF6zBSU1ODyZMn4/XXX/fqPJPJhNWrV2PJkiXefiQRUac9f+t47FkzT/4Nuq/z06iwPLl5UbO7ZyZgaCcWBhsWFoiwoObek7tnxbu9PydJGqrpuIjVZhexfscpZBRbMFSvw1v3T/c4O8gYqMUtkxy9Odu/zWnzGFcVtY1otNkhCECEvvthBACWjHP83X5x0Xd1IyeyK/B+ah4A4Ne3JUOtErDU+Tn7XaYSpzkD2MiIYBgDW/dSSGE4t6IOlnbqauRpvbEuYWSo1DPi3hNzLMvxdzjTQ70I0Bz0ahttqKxt/bnSbtYhgVr57zbATy2HJ6XrRrwOI8uXL8cLL7yA733ve16d9+ijj2LVqlWYM2eOtx9JRNRpgX4aTIkP6XCPmL5EGqrRqgX8z4KOe0UAyDUcgKOg1TXQAJB/qz+aWd5hF/xvP7mALzNKodOo8LfV0+WaAk9WOYPPx+kFMNW1X8gq9YqEB+vgp/FNZcBi5xTfgxklaLI173xcWdOIJ987hWc/PNPhDriurDY7fr7nLADgrunx8oJgS531PCeyK+SVVV3XF2lLSKCfHAwuFlnaPKbJZpeHcSa59K5I6+KUVTfA5AwUFTWN8hojM4a3XS8CAP5atby6bVt1I3K9SIu6HelnVbpupFdqRt555x1cvXoVzz//fKeOb2hogNlsdnsQEQ1UsxJD8YuV4/Ha3dd1GARcSV+W985OgL/WfT2SacOGQKMSkF9V1+4Mix3HcvDW4SwAwMt3TsHkTgxvXZcwBGMi9ahvsndYyNrVDfLaMyU+BGFBfrDUW3Ei2/EbfX5VHe544xvsSSvAtm9zsPTlFPz3uydwIrv9nqGKmkY88V4aLhZZEBKoxU+XN9fJxA0JxPhoA+wi5LocaSZNW/Uiko7qRi4XV6PBaofeX4NhoYHy68E6DaKca7FcLXMEkOPO9o+KCEZYcPs9Zu1N781vUbwqkYpwlV78rMfDyOXLl/HMM89g27Zt0Gg6tyjQpk2bYDQa5Ud8fHzHJxER9VOCIOCBeYler+L6/Wlx+M8T1+PHN45p9V6QTiMHC09TfI9cLcf/OnsENtw4GismRbd5XFvt7Wwha5G0xogPw4haJWDhGOdqrBeLkVFkwe1//gZXS2sQbfTH0nGREEVHrccdbxzB9/78Nf55PLdVL86+s0VY9scU/PtMIdQqAb+8dQJCg9wLh6XA9/n5YtjsItLamUkjketGPIQRaXO85Bhjq+nnLetGOjNEI4ltZ3pvQYviVYnUM3K+wKzonj89GkZsNhtWrVqFX/7ylxg9enSnz9u4cSNMJpP8yM3N7cFWEhH1T4IgYFy0weN6KrOTHF9gR6+2DiPZZTX44bZUWO0iVk6OwbrFI7367NumxsJfq0JGsaXd36o/cxZ/DvfxPkFS3chHaQX4/hvfoMhcj1ERwfjgh3Pxt/un4/MNN+AHM+Lhp1bhZE4VfvJBOmb85nM89o9UfJxegPU7TuGxrakoq27EqIhgfPj4XHm/IFfLnGHk0OVSnM03oca5l8zoFjvnupJ6Rs4Xtj1M01bxqmSkNL231Psw0t4qrC2n9UoSQgMRGuSHRpvdbUpzb+vRMGKxWHDixAmsXbsWGo0GGo0Gv/rVr3D69GloNBocOHCgzfN0Oh0MBoPbg4iIvOOpbsRU14SH/34cVbVNmBwfgt/dMcnrGhtjgBYrnYWsf9x/uc3ekdRrFfjqchk0KgH3zhrWjZ+ktetHhUPjnAlirrdi2rAh2PXYHPnLdmREMF68fRIO/3QRnr5pDEZFBKPRase+c0VYu/0U9p4ugEoAHl84Ah+vn+9Wu+FqQowBMUZ/1Dba5AXpJjtnoXgihZGMIrO8aaOrM20Ur0qkZeGvltTAUt8kL4zWmTCS4BzyaWt1WtcFz1wJgoDrnL08aZ2YHdVTvNtMwUsGgwFnzrhvGPTnP/8ZBw4cwPvvv4/ExMSe/HgiokFt2rAh0KoFFJjqkVtRhyFBWnyQmoct32Qju7wW0UZ/vHnftFb1Jp21bvEofJRWgMNXyvBlRolcWCp55fPLAIA7psUh3qU2whf0/lrMHxWOgxmlWDI2Aq+vuq7NfXwiDP5Ys2gkHl84AucLzdibVoD/nC1CaJAffnHrhA6ngAuCgKXjI/HukWvyFN/rhrV/zvCwIPhrVahvsiO7vMZtAb5Gqx0XnD0mbfWMSMdmllYj9Vol7KIjZLTcd6kt0102v2uw2qDTNN8PaYn5mDaW5H9y6WhsuHEMxkR57u3paV6Hkerqaly5ckV+npWVhbS0NISGhiIhIQEbN25Efn4+3n33XahUKiQnJ7udHxERAX9//1avExGRbwX6aTA5LgQnrlViwz/TcKHQjJpGR13AkEAt/nb/dER0Y/O6hLBAPDh/OP6SkokX/n0B148aCq3a0eF+Iru5V2TNIu+GgDrrpdsn4WROFZaOi4BG3X5HvyAImBBjxIQYIzZ6uZfQ0nGOMCKZGu+5eBVw1LSMiTLgdG4VLhSa3cLIiewKNNoc+9oktBHQpGOvVdTi6ytlADrXKwI4eoPCg/1QVt2I07km+Tyby1ovbRVIt9VD09u8HqY5ceIEpk6diqlTpwIANmzYgKlTp+K5554DABQWFiInp+O550RE1POkoZoT1ypR02jDqIhg/Pq2ZHz108WYENP9L6E1i0YiLMgPmaU1buuOSL0i35/u+14RSYTBH99JjuowiHTX7KQw6F3WXZnSTvGqZLyHItZtznu0YlJ0m0NjkQYdgnUa2OwiPnTOVOpsGBEEAbOSWm8FUGppgNUuQq0SfLbWi695/Te4cOFCiKLY6rFlyxYAwJYtW3Dw4EGP5//iF79AWlpaF5tLRETeuHN6PMZG6fGdCVHY/t+z8NmPFuC+2cMQ7GFRM28Z/LX40Y2OCQp//PwSTLVNOJ5dgcNXHL0ijy/smV6R3uSnUeGGMUMBOIZMwjuYYgu4Tu9tLmItNtfj03NFAOCxhkYQBIxwbmVQVu1Y22RWJ8MI0Bw+j7gULUvTeqMM/l3e1LGn9WjNCBERKSshLBD7nlzQo5/xgxnxePdINi4VV+PVA5eR4Vzsqyd7RXrbHdPi8HF6obyxYUfaWmvkvWO5sNpFTB82RN5Qry0jhgbjtLPINdKga3M4xxNp5d2TOZWob7LBX6t2mdbb/Y0Kewo3yiMiom7RqFV4dsV4AMA7X2fJvSI9VSuihIVjIvD1M4vx0xabB3oy1lkMWmiqR1VtI6w2O3YccwzR3Den/ZlF0owaAJiZGObVTKcRQ4MwVK9Dg9Uuz45pntbbN4doAIYRIiLygRtGD8XCMUMhzWT9/vR4efO2gSI2JKDTS9rr/bWID3X0RJwvNOPzCyUoMtcjLMivw8XtRrjsON3ZehGJIAhuU7oBz2uM9CUMI0RE5BM/XzEOGpUAP7UKaxaNULo5ihsX1Vw3svWoYzbOnTPi3abctsV19o039SKSOS3qRvI9rDHSl7BmhIiIfGJkhB7v/3AuVAIGXK9IV4yLNuCz88X45EwhUq9VQhCAVTMTOjwvMTwI04YNgZ9aJa/I6g1p5d1TuVWob7L1i5oRhhEiIvKZjhYRG0ykItbUa47l8hePiehUQa9GrcIHP5zb5c9NDA9CpEGHYnMDTuZUuix41nfDCIdpiIiIesD4aPcZM/d2ULjqK651IwculKCq1rFBIAtYiYiIBpm4IQHyei7xoQG4YdTQXvtsqW5kT1oBAEDvr4HeX9trn+8thhEiIqIeoFIJSI519I7cM2uYx92Ve4LUM1JW3QCgb9eLAKwZISIi6jG/uHUCDmaU4qF5vbsx7LCwQEQb/VHYzp40fQl7RoiIiHrI2CgDHrthRKfXJ/EV17oRoG/XiwAMI0RERAPSHLcwwp4RIiIi6mVzRjSHkb5eM8IwQkRENADFDQnA8DDHuiYjurB4Wm9iASsREdEAJAgC3rhvGq6UVCM51qh0c9rFMEJERDRAjY0yYGyUoeMDFcZhGiIiIlIUwwgREREpimGEiIiIFMUwQkRERIpiGCEiIiJFMYwQERGRohhGiIiISFEMI0RERKQohhEiIiJSFMMIERERKYphhIiIiBTFMEJERESKYhghIiIiRfWLXXtFUQQAmM1mhVtCREREnSV9b0vf4570izBisVgAAPHx8Qq3hIiIiLxlsVhgNBo9vi+IHcWVPsBut6OgoAB6vR6CIPjsumazGfHx8cjNzYXBYPDZdak13uvexfvde3ivew/vde/x1b0WRREWiwUxMTFQqTxXhvSLnhGVSoW4uLgeu77BYOA/7F7Ce927eL97D+917+G97j2+uNft9YhIWMBKREREimIYISIiIkUN6jCi0+nw/PPPQ6fTKd2UAY/3unfxfvce3uvew3vde3r7XveLAlYiIiIauAZ1zwgREREpj2GEiIiIFMUwQkRERIpiGCEiIiJFDeow8uc//xmJiYnw9/fHtGnT8NVXXyndpH5v06ZNmDFjBvR6PSIiInDbbbchIyPD7RhRFPGLX/wCMTExCAgIwMKFC3Hu3DmFWjwwbNq0CYIg4Mknn5Rf4332rfz8fNx7770ICwtDYGAgpkyZgtTUVPl93m/fsFqt+PnPf47ExEQEBAQgKSkJv/rVr2C32+VjeK+75tChQ1i5ciViYmIgCAL27Nnj9n5n7mtDQwPWrVuH8PBwBAUF4dZbb0VeXl73GycOUu+9956o1WrFN998Uzx//rz4xBNPiEFBQeK1a9eUblq/dtNNN4nvvPOOePbsWTEtLU1csWKFmJCQIFZXV8vHvPjii6Jerxc/+OAD8cyZM+Jdd90lRkdHi2azWcGW91/Hjh0Thw8fLk6aNEl84okn5Nd5n32noqJCHDZsmPjAAw+I3377rZiVlSV+/vnn4pUrV+RjeL9944UXXhDDwsLEjz/+WMzKyhJ37dolBgcHi6+88op8DO9113zyySfis88+K37wwQciAPHDDz90e78z9/Wxxx4TY2Njxf3794snT54UFy1aJE6ePFm0Wq3datugDSMzZ84UH3vsMbfXxo4dKz7zzDMKtWhgKikpEQGIKSkpoiiKot1uF6OiosQXX3xRPqa+vl40Go3iG2+8oVQz+y2LxSKOGjVK3L9/v3jDDTfIYYT32bd++tOfivPnz/f4Pu+376xYsUJ86KGH3F773ve+J957772iKPJe+0rLMNKZ+1pVVSVqtVrxvffek4/Jz88XVSqVuG/fvm61Z1AO0zQ2NiI1NRXLli1ze33ZsmX45ptvFGrVwGQymQAAoaGhAICsrCwUFRW53XudTocbbriB974L1qxZgxUrVmDp0qVur/M++9bevXsxffp0fP/730dERASmTp2KN998U36f99t35s+fjy+++AKXLl0CAJw+fRqHDx/GzTffDID3uqd05r6mpqaiqanJ7ZiYmBgkJyd3+973i43yfK2srAw2mw2RkZFur0dGRqKoqEihVg08oihiw4YNmD9/PpKTkwFAvr9t3ftr1671ehv7s/feew8nT57E8ePHW73H++xbmZmZ2Lx5MzZs2ICf/exnOHbsGNavXw+dTofVq1fzfvvQT3/6U5hMJowdOxZqtRo2mw2/+c1vcPfddwPgv+2e0pn7WlRUBD8/PwwZMqTVMd397hyUYUQiCILbc1EUW71GXbd27Vqkp6fj8OHDrd7jve+e3NxcPPHEE/jss8/g7+/v8TjeZ9+w2+2YPn06fvvb3wIApk6dinPnzmHz5s1YvXq1fBzvd/ft3LkTW7duxfbt2zFhwgSkpaXhySefRExMDO6//375ON7rntGV++qLez8oh2nCw8OhVqtbJbmSkpJWqZC6Zt26ddi7dy++/PJLxMXFya9HRUUBAO99N6WmpqKkpATTpk2DRqOBRqNBSkoKXn31VWg0Gvle8j77RnR0NMaPH+/22rhx45CTkwOA/6596emnn8YzzzyDH/zgB5g4cSLuu+8+/OhHP8KmTZsA8F73lM7c16ioKDQ2NqKystLjMV01KMOIn58fpk2bhv3797u9vn//fsydO1ehVg0Moihi7dq12L17Nw4cOIDExES39xMTExEVFeV27xsbG5GSksJ774UlS5bgzJkzSEtLkx/Tp0/HPffcg7S0NCQlJfE++9C8efNaTVG/dOkShg0bBoD/rn2ptrYWKpX7V5NarZan9vJe94zO3Ndp06ZBq9W6HVNYWIizZ892/953q/y1H5Om9r711lvi+fPnxSeffFIMCgoSs7OzlW5av/bDH/5QNBqN4sGDB8XCwkL5UVtbKx/z4osvikajUdy9e7d45swZ8e677+a0PB9wnU0jirzPvnTs2DFRo9GIv/nNb8TLly+L27ZtEwMDA8WtW7fKx/B++8b9998vxsbGylN7d+/eLYaHh4s/+clP5GN4r7vGYrGIp06dEk+dOiUCEF9++WXx1KlT8pIWnbmvjz32mBgXFyd+/vnn4smTJ8XFixdzam93/elPfxKHDRsm+vn5idddd508/ZS6DkCbj3feeUc+xm63i88//7wYFRUl6nQ6ccGCBeKZM2eUa/QA0TKM8D771r/+9S8xOTlZ1Ol04tixY8W//vWvbu/zfvuG2WwWn3jiCTEhIUH09/cXk5KSxGeffVZsaGiQj+G97povv/yyzf8+33///aIodu6+1tXViWvXrhVDQ0PFgIAA8ZZbbhFzcnK63TZBFEWxe30rRERERF03KGtGiIiIqO9gGCEiIiJFMYwQERGRohhGiIiISFEMI0RERKQohhEiIiJSFMMIERERKYphhIiIiBTFMEJERESKYhghIiIiRTGMEBERkaIYRoiIiEhR/x8j6Zp63IJ58AAAAABJRU5ErkJggg==",
                        "text/plain": [
                            "<Figure size 640x480 with 1 Axes>"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                }
            ],
            "source": [
                "import matplotlib.pyplot as plt\n",
                "import matplotlib.ticker as ticker\n",
                "\n",
                "plt.figure()\n",
                "plt.plot(all_losses)"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Evaluating the Results\n",
                "\n",
                "Let's see how well the model is doing on the training data. We can get a reasonable estimate with just part of the data, so we'll run 1000 samples through the network with `evaluate()`, which is the same as `train()` minus the backpropagation.\n",
                "\n",
                "\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 42,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "56.8%\n"
                    ]
                }
            ],
            "source": [
                "# Just return an output given a line\n",
                "def evaluate(line_tensor):\n",
                "    hidden = rnn.initHidden()\n",
                "\n",
                "    for i in range(line_tensor.size()[0]):\n",
                "        output, hidden = rnn(line_tensor[i], hidden)\n",
                "\n",
                "    return output\n",
                "\n",
                "total = 1000\n",
                "correct = 0\n",
                "# Go through a bunch of examples and record which are correctly guessed\n",
                "for i in range(total):\n",
                "    category, line, category_tensor, line_tensor = randomTrainingExample()\n",
                "    output = evaluate(line_tensor)\n",
                "    guess, guess_i = categoryFromOutput(output)\n",
                "    category_i = all_categories.index(category)\n",
                "    if category_i == guess_i:\n",
                "        correct += 1\n",
                "print(\"{}%\".format(100 * correct / total))"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "The score should be around 50-60%, which may seem low, but consider how tricky this task can be!\n",
                "\n",
                "\n"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Running on User Input\n",
                "\n",
                "This function shows the output for a sample input you can provide."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 43,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "\n",
                        "> Kummerfeld\n",
                        "(-1.04) English\n",
                        "(-1.85) German\n",
                        "(-1.93) Polish\n",
                        "\n",
                        "> Kay\n",
                        "(-1.41) Chinese\n",
                        "(-1.46) Vietnamese\n",
                        "(-2.23) Scottish\n"
                    ]
                }
            ],
            "source": [
                "def predict(input_line, n_predictions=3):\n",
                "    print('\\n> %s' % input_line)\n",
                "    with torch.no_grad():\n",
                "        output = evaluate(lineToTensor(input_line))\n",
                "\n",
                "        # Get top N categories\n",
                "        topv, topi = output.topk(n_predictions, 1, True)\n",
                "        predictions = []\n",
                "\n",
                "        for i in range(n_predictions):\n",
                "            value = topv[0][i].item()\n",
                "            category_index = topi[0][i].item()\n",
                "            print('(%.2f) %s' % (value, all_categories[category_index]))\n",
                "            predictions.append([value, all_categories[category_index]])\n",
                "\n",
                "predict('Kummerfeld')\n",
                "predict('Kay')"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Task 3\n",
                "\n",
                "Above, we trained and tested on the same data. That is misleading, because the model saw those examples during training.\n",
                "\n",
                "In this task:\n",
                "1. Modify the data reading process to split the data randomly into a test set (\\~10% of the data) and train set (\\~90% of the data). Train and test again.\n",
                "2. Modify `randomTrainingExample` to sample from your training data. Implement a `randomTestExample` to sample from your test data.\n",
                "3. Create a new instance of the model.\n",
                "4. Train that instance with the training data you created.\n",
                "5. Test it with the test data you created."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 44,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "2500 5% (0m 1s) 0.6186 Chu / Vietnamese ✓\n",
                        "5000 10% (0m 2s) 1.1531 Klerken / Dutch ✓\n",
                        "7500 15% (0m 4s) 1.8692 Smith / Arabic ✗ (Czech)\n",
                        "10000 20% (0m 5s) 0.6689 Cong / Chinese ✓\n",
                        "12500 25% (0m 6s) 0.0182 O'Mooney / Irish ✓\n",
                        "15000 30% (0m 8s) 3.2611 Bonnay / English ✗ (French)\n",
                        "17500 35% (0m 9s) 3.9193 Schneider / German ✗ (Dutch)\n",
                        "20000 40% (0m 10s) 3.5229 Whitaker / German ✗ (English)\n",
                        "22500 45% (0m 12s) 1.8959 Mclaughlin / Irish ✗ (Scottish)\n",
                        "25000 50% (0m 13s) 0.4411 Mai / Chinese ✓\n",
                        "27500 55% (0m 15s) 2.3142 Bainbridge / Dutch ✗ (English)\n",
                        "30000 60% (0m 16s) 0.0365 Ratti / Italian ✓\n",
                        "32500 65% (0m 17s) 2.4556 Desjardins / English ✗ (French)\n",
                        "35000 70% (0m 19s) 1.0512 Vieth / German ✓\n",
                        "37500 75% (0m 20s) 0.1790 Janvier / French ✓\n",
                        "40000 80% (0m 21s) 0.8677 Palmeiro / Portuguese ✓\n",
                        "42500 85% (0m 23s) 1.5210 Lafrenz / Spanish ✗ (German)\n",
                        "45000 90% (0m 24s) 2.7747 Schneijder / Polish ✗ (Dutch)\n",
                        "47500 95% (0m 25s) 4.2677 Giles / Portuguese ✗ (French)\n",
                        "50000 100% (0m 26s) 3.2235 Betlach / Irish ✗ (Czech)\n",
                        "46.2%\n"
                    ]
                }
            ],
            "source": [
                "import string\n",
                "import random\n",
                "\n",
                "# 定义字符集，包括所有小写字母、大写字母、以及一些常见的标点符号\n",
                "all_letters = string.ascii_letters + \" .,;’\"\n",
                "n_letters = len(all_letters)  # 字符集的长度\n",
                "\n",
                "# 定义类别和名字的字典，分别保存训练集和测试集的名字\n",
                "category_lines = {}\n",
                "all_categories = set()  # 用于保存所有类别\n",
                "category_lines_train = {}  # 训练集\n",
                "category_lines_test = {}   # 测试集\n",
                "\n",
                "# 从文件中读取类别和对应的名字，并填充字典\n",
                "with open(\"names.txt\") as src:\n",
                "    for line in src:\n",
                "        parts = line.strip().split()  # 分割每一行\n",
                "        category = parts[0]  # 类别是每一行的第一个部分\n",
                "        name = ' '.join(parts[1:])  # 名字是后面的部分\n",
                "        category_lines.setdefault(category, []).append(name)  # 将名字添加到对应类别的列表中\n",
                "        all_categories.add(category)  # 添加类别到所有类别的集合中\n",
                "\n",
                "# 将数据集分为训练集和测试集，10%的数据用于测试\n",
                "for cat in all_categories:\n",
                "    random.shuffle(category_lines[cat])  # 打乱每个类别的顺序\n",
                "    split_point = int(len(category_lines[cat]) * 0.1)  # 取10%的数据作为测试集\n",
                "    category_lines_train[cat] = category_lines[cat][split_point:]  # 剩下的作为训练集\n",
                "    category_lines_test[cat] = category_lines[cat][:split_point]  # 10%作为测试集\n",
                "\n",
                "all_categories = list(all_categories)  # 将所有类别转为列表\n",
                "\n",
                "# 定义生成一个随机训练样本的函数\n",
                "def randomTrainingExample():\n",
                "    category = randomChoice(all_categories)  # 随机选择一个类别\n",
                "    line = randomChoice(category_lines_train[category])  # 随机选择一个名字\n",
                "    category_tensor = torch.tensor([all_categories.index(category)], dtype=torch.long)  # 类别转换为 tensor\n",
                "    line_tensor = lineToTensor(line)  # 将名字转换为 tensor\n",
                "    return category, line, category_tensor, line_tensor  # 返回训练样本\n",
                "\n",
                "# 定义生成一个随机测试样本的函数\n",
                "def randomTestExample():\n",
                "    category = randomChoice(all_categories)  # 随机选择一个类别\n",
                "    line = randomChoice(category_lines_test[category])  # 随机选择一个名字\n",
                "    category_tensor = torch.tensor([all_categories.index(category)], dtype=torch.long)  # 类别转换为 tensor\n",
                "    line_tensor = lineToTensor(line)  # 将名字转换为 tensor\n",
                "    return category, line, category_tensor, line_tensor  # 返回测试样本\n",
                "\n",
                "# 初始化 RNN 模型\n",
                "new_rnn = RNN(n_letters, n_hidden, n_categories)\n",
                "\n",
                "# 定义训练过程中的迭代次数和打印频率\n",
                "n_iters = 50000\n",
                "print_every = 2500\n",
                "plot_every = 500\n",
                "\n",
                "# 用来跟踪损失值以便后续绘图\n",
                "current_loss = 0\n",
                "all_losses = []  # 用来保存每隔一段时间计算的平均损失\n",
                "start = time.time()  # 记录开始时间\n",
                "\n",
                "# 训练过程\n",
                "for iter in range(1, n_iters + 1):\n",
                "    category, line, category_tensor, line_tensor = randomTrainingExample()  # 获取一个随机训练样本\n",
                "    output, loss = train(category_tensor, line_tensor)  # 训练并计算损失\n",
                "    current_loss += loss  # 累积当前的损失\n",
                "\n",
                "    # 每隔一定的迭代次数打印一次训练信息\n",
                "    if iter % print_every == 0:\n",
                "        guess, guess_i = categoryFromOutput(output)  # 获取模型的预测\n",
                "        correct = '✓' if guess == category else '✗ (%s)' % category  # 判断预测是否正确\n",
                "        # 打印迭代次数、损失、名字以及预测结果\n",
                "        print('%d %d%% (%s) %.4f %s / %s %s' % (iter, iter / n_iters * 100, timeSince(start), loss, line, guess, correct))\n",
                "\n",
                "    # 每隔一定的迭代次数，计算并保存平均损失\n",
                "    if iter % plot_every == 0:\n",
                "        all_losses.append(current_loss / plot_every)  # 记录平均损失\n",
                "        current_loss = 0  # 重置当前损失\n",
                "\n",
                "# 在测试集上评估模型的性能\n",
                "total = 1000\n",
                "correct = 0\n",
                "# 遍历多个测试样本并记录正确的预测数量\n",
                "for i in range(total):\n",
                "    category, line, category_tensor, line_tensor = randomTestExample()  # 获取一个随机测试样本\n",
                "    output = evaluate(line_tensor)  # 进行预测\n",
                "    guess, guess_i = categoryFromOutput(output)  # 获取预测类别\n",
                "    category_i = all_categories.index(category)  # 获取实际类别的索引\n",
                "    if category_i == guess_i:  # 判断预测是否正确\n",
                "        correct += 1\n",
                "\n",
                "# 打印准确率\n",
                "print(\"{}%\".format(100 * correct / total))"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Task 4\n",
                "\n",
                "The model is entirely linear so far. Modify it to be the basic RNN introduced in lecture 4.\n",
                "\n",
                "Note - you can do this task without doing Task 2. It is fine to report results on the training set (as the code below does). If you want to combine task 3 and 4 that's okay too.\n",
                "\n",
                "In the process, also change the weight initialisation to set them to be random values uniformly distributed in the range (-sqrt(k), sqrt(k)) where k is 1/hidden_size.\n",
                "\n",
                "The cells below contains all the key code from above for easier manipulation."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 45,
            "metadata": {},
            "outputs": [],
            "source": [
                "import string\n",
                "all_letters = string.ascii_letters + \" .,;’\"\n",
                "n_letters = len(all_letters)\n",
                "\n",
                "# Read the category_lines dictionary, a list of names per language\n",
                "category_lines = {}\n",
                "all_categories = set()\n",
                "\n",
                "with open(\"names.txt\") as src:\n",
                "    for line in src:\n",
                "        parts = line.strip().split()\n",
                "        category = parts[0]\n",
                "        name = ' '.join(parts[1:])\n",
                "        all_categories.add(category)\n",
                "        category_lines.setdefault(category, []).append(name)\n",
                "    \n",
                "all_categories = sorted(list(all_categories))\n",
                "n_categories = len(all_categories)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 46,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Solution\n",
                "\n",
                "# Model and Inference\n",
                "\n",
                "# Find letter index from all_letters, e.g. \"a\" = 0\n",
                "def letterToIndex(letter):\n",
                "    return all_letters.find(letter)\n",
                "\n",
                "# Just for demonstration, turn a letter into a <1 x n_letters> Tensor\n",
                "def letterToTensor(letter):\n",
                "    tensor = torch.zeros(1, n_letters)\n",
                "    tensor[0][letterToIndex(letter)] = 1\n",
                "    return tensor\n",
                "\n",
                "# Turn a line into a <line_length x 1 x n_letters>,\n",
                "# or an array of one-hot letter vectors\n",
                "def lineToTensor(line):\n",
                "    tensor = torch.zeros(len(line), 1, n_letters)\n",
                "    for li, letter in enumerate(line):\n",
                "        tensor[li][0][letterToIndex(letter)] = 1\n",
                "    return tensor\n",
                "\n",
                "class RNN(nn.Module):\n",
                "    def __init__(self, input_size, hidden_size, output_size):\n",
                "        super(RNN, self).__init__()\n",
                "\n",
                "        self.hidden_size = hidden_size\n",
                "\n",
                "        # Define the structure of the model\n",
                "        # Note that nn.Linear creates a weight matrix and a bias vector\n",
                "        self.i2h = nn.Linear(input_size, hidden_size)\n",
                "        self.h2h = nn.Linear(hidden_size, hidden_size)\n",
                "        self.h2o = nn.Linear(hidden_size, output_size)\n",
                "\n",
                "        self.softmax = nn.LogSoftmax(dim=1)\n",
                "\n",
                "        self.init_weights()\n",
                "\n",
                "    def init_weights(self):\n",
                "        initrange = math.sqrt(1 / self.hidden_size)\n",
                "        self.i2h.weight.data.uniform_(-initrange, initrange)\n",
                "        self.i2h.bias.data.zero_()\n",
                "        self.h2o.weight.data.uniform_(-initrange, initrange)\n",
                "        self.h2o.bias.data.zero_()\n",
                "        self.h2h.weight.data.uniform_(-initrange, initrange)\n",
                "        self.h2h.bias.data.zero_()\n",
                "\n",
                "    def initHidden(self):\n",
                "        # Define the initial hidden state for an input as all zeros\n",
                "        return torch.zeros(1, self.hidden_size)\n",
                "\n",
                "    def forward(self, input_tensor, hidden):\n",
                "        # Given an input, compute the steps defined by the model\n",
                "        # check RNN graph in lecture slide, you can easily understand below\n",
                "        # calculate h_t \n",
                "        new_hidden = torch.tanh(self.i2h(input_tensor) + self.h2h(hidden))\n",
                "        # calculate y_t\n",
                "        output = torch.tanh(self.h2o(new_hidden))\n",
                "        output = self.softmax(output)\n",
                "        return output, new_hidden"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 47,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "2500 5% (0m 1s) 2.8604 Makino / Portuguese ✗ (Japanese)\n",
                        "5000 10% (0m 3s) 2.8606 Pullen / Scottish ✗ (English)\n",
                        "7500 15% (0m 5s) 3.3969 Hout / Korean ✗ (Dutch)\n",
                        "10000 20% (0m 7s) 2.3925 Lindsay / Irish ✗ (Scottish)\n",
                        "12500 25% (0m 9s) 1.6434 Bilias / Greek ✓\n",
                        "15000 30% (0m 11s) 1.8353 Joe / Korean ✗ (Chinese)\n",
                        "17500 35% (0m 12s) 1.3575 Agalakov / Russian ✓\n",
                        "20000 40% (0m 15s) 1.3112 Alexandropoulos / Greek ✓\n",
                        "22500 45% (0m 17s) 2.2019 Ironmonger / German ✗ (English)\n",
                        "25000 50% (0m 19s) 2.0372 Martin / Russian ✗ (Scottish)\n",
                        "27500 55% (0m 21s) 1.9844 Medina / Spanish ✓\n",
                        "30000 60% (0m 23s) 1.6291 Nam / Korean ✓\n",
                        "32500 65% (0m 25s) 1.3878 Assaf / Arabic ✓\n",
                        "35000 70% (0m 27s) 1.5358 Koo / Korean ✓\n",
                        "37500 75% (0m 28s) 1.3105 Kakinomoto / Japanese ✓\n",
                        "40000 80% (0m 30s) 1.7933 Zhen / Vietnamese ✗ (Chinese)\n",
                        "42500 85% (0m 32s) 2.1725 Dziedzic / Scottish ✗ (Polish)\n",
                        "45000 90% (0m 34s) 1.6558 Salcedo / Portuguese ✗ (Spanish)\n",
                        "47500 95% (0m 36s) 1.9936 O'Connell / Irish ✓\n",
                        "50000 100% (0m 37s) 1.6240 Paredes / Portuguese ✓\n"
                    ]
                }
            ],
            "source": [
                "n_hidden = 128\n",
                "rnn = RNN(n_letters, n_hidden, n_categories)\n",
                "\n",
                "# Training\n",
                "\n",
                "n_iters = 50000\n",
                "print_every = 2500 # note, we decreased this just to get more frequent updates, leaving it at 5000 is fine\n",
                "plot_every = 500\n",
                "\n",
                "criterion = nn.NLLLoss()\n",
                "\n",
                "learning_rate = 0.005 # If you set this too high, it might explode. If too low, it might not learn\n",
                "\n",
                "def categoryFromOutput(output):\n",
                "    top_n, top_i = output.topk(1)\n",
                "    category_i = top_i[0].item()\n",
                "    return all_categories[category_i], category_i\n",
                "\n",
                "def randomChoice(l):\n",
                "    return l[random.randint(0, len(l) - 1)]\n",
                "\n",
                "def randomTrainingExample():\n",
                "    category = randomChoice(all_categories)\n",
                "    line = randomChoice(category_lines[category])\n",
                "    category_tensor = torch.tensor([all_categories.index(category)], dtype=torch.long)\n",
                "    line_tensor = lineToTensor(line)\n",
                "    return category, line, category_tensor, line_tensor\n",
                "\n",
                "def train(category_tensor, line_tensor):\n",
                "    hidden = rnn.initHidden() # 初始化 hidden state\n",
                "\n",
                "    rnn.zero_grad() # 梯度归零\n",
                "\n",
                "    for i in range(line_tensor.size()[0]):\n",
                "        output, hidden = rnn(line_tensor[i], hidden)\n",
                "\n",
                "    loss = criterion(output, category_tensor)\n",
                "    loss.backward()\n",
                "\n",
                "    # Add parameters' gradients to their values, multiplied by learning rate\n",
                "    for p in rnn.parameters():\n",
                "        p.data.add_(p.grad.data, alpha=-learning_rate)\n",
                "\n",
                "    return output, loss.item()\n",
                "\n",
                "# Keep track of losses for plotting\n",
                "current_loss = 0\n",
                "all_losses = []\n",
                "\n",
                "def timeSince(since):\n",
                "    now = time.time()\n",
                "    s = now - since\n",
                "    m = math.floor(s / 60)\n",
                "    s -= m * 60\n",
                "    return '%dm %ds' % (m, s)\n",
                "\n",
                "start = time.time()\n",
                "\n",
                "for iter in range(1, n_iters + 1):\n",
                "    category, line, category_tensor, line_tensor = randomTrainingExample()\n",
                "    output, loss = train(category_tensor, line_tensor)\n",
                "    current_loss += loss\n",
                "\n",
                "    # Print ``iter`` number, loss, name and guess\n",
                "    if iter % print_every == 0:\n",
                "        guess, guess_i = categoryFromOutput(output)\n",
                "        correct = '✓' if guess == category else '✗ (%s)' % category\n",
                "        print('%d %d%% (%s) %.4f %s / %s %s' % (iter, iter / n_iters * 100, timeSince(start), loss, line, guess, correct))\n",
                "\n",
                "    # Add current loss avg to list of losses\n",
                "    if iter % plot_every == 0:\n",
                "        all_losses.append(current_loss / plot_every)\n",
                "        current_loss = 0"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 48,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "48.0%\n"
                    ]
                }
            ],
            "source": [
                "# Just return an output given a line\n",
                "def evaluate(line_tensor):\n",
                "    hidden = rnn.initHidden()\n",
                "\n",
                "    for i in range(line_tensor.size()[0]):\n",
                "        output, hidden = rnn(line_tensor[i], hidden)\n",
                "\n",
                "    return output\n",
                "\n",
                "total = 1000\n",
                "correct = 0\n",
                "# Go through a bunch of examples and record which are correctly guessed\n",
                "for i in range(total):\n",
                "    category, line, category_tensor, line_tensor = randomTrainingExample()\n",
                "    output = evaluate(line_tensor)\n",
                "    guess, guess_i = categoryFromOutput(output)\n",
                "    category_i = all_categories.index(category)\n",
                "    if category_i == guess_i:\n",
                "        correct += 1\n",
                "print(\"{}%\".format(100 * correct / total))"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 49,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "2500 5% (0m 1s) 3.2037 Vederman / Scottish ✗ (Russian)\n",
                        "5000 10% (0m 3s) 1.8144 Berg / Dutch ✓\n",
                        "7500 15% (0m 5s) 3.2092 Gomez / Czech ✗ (Spanish)\n",
                        "10000 20% (0m 7s) 2.9055 Vuong / Korean ✗ (Vietnamese)\n",
                        "12500 25% (0m 9s) 1.3373 Lambert / French ✓\n",
                        "15000 30% (0m 10s) 1.4612 Fonseca / Portuguese ✓\n",
                        "17500 35% (0m 12s) 3.0682 Esser / Dutch ✗ (German)\n",
                        "20000 40% (0m 14s) 2.9608 Garofalis / French ✗ (Greek)\n",
                        "22500 45% (0m 16s) 1.3073 Holmogortsev / Russian ✓\n",
                        "25000 50% (0m 18s) 2.8715 Mackenzie / French ✗ (Scottish)\n",
                        "27500 55% (0m 20s) 3.0872 Sokolowski / Russian ✗ (Polish)\n",
                        "30000 60% (0m 22s) 2.9753 Schulze / Dutch ✗ (German)\n",
                        "32500 65% (0m 24s) 3.4042 Kumiega / Japanese ✗ (Polish)\n",
                        "35000 70% (0m 26s) 1.2212 Leveque / French ✓\n",
                        "37500 75% (0m 28s) 1.3840 Si / Korean ✓\n",
                        "40000 80% (0m 30s) 3.3155 Abana / Japanese ✗ (Spanish)\n",
                        "42500 85% (0m 32s) 1.5660 Emile / Scottish ✗ (French)\n",
                        "45000 90% (0m 33s) 2.9100 Belesis / Portuguese ✗ (Greek)\n",
                        "47500 95% (0m 35s) 3.3686 Aodh / Arabic ✗ (Irish)\n",
                        "50000 100% (0m 37s) 3.1417 Puig / Chinese ✗ (Spanish)\n",
                        "34.3%\n"
                    ]
                }
            ],
            "source": [
                "# code for train/test splits\n",
                "\n",
                "import string\n",
                "import random\n",
                "\n",
                "all_letters = string.ascii_letters + \" .,;’\"\n",
                "n_letters = len(all_letters)\n",
                "\n",
                "# Read the category_lines dictionary, a list of names per language\n",
                "category_lines = {}\n",
                "all_categories = set()\n",
                "category_lines_train = {}\n",
                "category_lines_test = {}\n",
                "\n",
                "with open(\"names.txt\") as src:\n",
                "    for line in src:\n",
                "        parts = line.strip().split()\n",
                "        category = parts[0]\n",
                "        name = ' '.join(parts[1:])\n",
                "        category_lines.setdefault(category, []).append(name)\n",
                "        all_categories.add(category)\n",
                "for cat in all_categories:\n",
                "    random.shuffle(category_lines[cat])\n",
                "    split_point = int(len(category_lines[cat]) * 0.1)\n",
                "    category_lines_train[cat] = category_lines[cat][split_point:]\n",
                "    category_lines_test[cat] = category_lines[cat][:split_point]\n",
                "all_categories = list(all_categories)\n",
                "\n",
                "def randomTrainingExample():\n",
                "    category = randomChoice(all_categories)\n",
                "    line = randomChoice(category_lines_train[category])\n",
                "    category_tensor = torch.tensor([all_categories.index(category)], dtype=torch.long)\n",
                "    line_tensor = lineToTensor(line)\n",
                "    return category, line, category_tensor, line_tensor\n",
                "\n",
                "def randomTestExample():\n",
                "    category = randomChoice(all_categories)\n",
                "    line = randomChoice(category_lines_test[category])\n",
                "    category_tensor = torch.tensor([all_categories.index(category)], dtype=torch.long)\n",
                "    line_tensor = lineToTensor(line)\n",
                "    return category, line, category_tensor, line_tensor\n",
                "\n",
                "new_rnn = RNN(n_letters, n_hidden, n_categories)\n",
                "\n",
                "n_iters = 50000\n",
                "print_every = 2500\n",
                "plot_every = 500\n",
                "\n",
                "# Keep track of losses for plotting\n",
                "current_loss = 0\n",
                "all_losses = []\n",
                "start = time.time()\n",
                "for iter in range(1, n_iters + 1):\n",
                "    category, line, category_tensor, line_tensor = randomTrainingExample()\n",
                "    output, loss = train(category_tensor, line_tensor)\n",
                "    current_loss += loss\n",
                "\n",
                "    # Print ``iter`` number, loss, name and guess\n",
                "    if iter % print_every == 0:\n",
                "        guess, guess_i = categoryFromOutput(output)\n",
                "        correct = '✓' if guess == category else '✗ (%s)' % category\n",
                "        print('%d %d%% (%s) %.4f %s / %s %s' % (iter, iter / n_iters * 100, timeSince(start), loss, line, guess, correct))\n",
                "\n",
                "    # Add current loss avg to list of losses\n",
                "    if iter % plot_every == 0:\n",
                "        all_losses.append(current_loss / plot_every)\n",
                "        current_loss = 0\n",
                "\n",
                "total = 1000\n",
                "correct = 0\n",
                "# Go through a bunch of examples and record which are correctly guessed\n",
                "for i in range(total):\n",
                "    category, line, category_tensor, line_tensor = randomTestExample()\n",
                "    output = evaluate(line_tensor)\n",
                "    guess, guess_i = categoryFromOutput(output)\n",
                "    category_i = all_categories.index(category)\n",
                "    if category_i == guess_i:\n",
                "        correct += 1\n",
                "print(\"{}%\".format(100 * correct / total))"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Resources\n",
                "\n",
                "For more on RNNs, see:\n",
                "\n",
                "- [Chapter 9 of J+M](https://web.stanford.edu/~jurafsky/slp3/9.pdf)\n",
                "- [Chapter 7, section 6 of E](https://github.com/jacobeisenstein/gt-nlp-class/blob/master/notes/eisenstein-nlp-notes.pdf)\n",
                "-  [The Unreasonable Effectiveness of Recurrent Neural\n",
                "   Networks](https://karpathy.github.io/2015/05/21/rnn-effectiveness/) shows a bunch of real life examples\n",
                "-  [Understanding LSTM\n",
                "   Networks](https://colah.github.io/posts/2015-08-Understanding-LSTMs/) is about LSTMs specifically but also informative about RNNs in general"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3 (ipykernel)",
            "language": "python",
            "name": "python3"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}
