{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wIEN4FYH06K1"
      },
      "source": [
        "# Workshop 8 - HuggingFace\n",
        "\n",
        "This is based on the [HuggingFace NLP Course](https://huggingface.co/learn/nlp-course/chapter1/1), which is written by many people (see the page for details) and released under an [Apache 2 license](https://www.apache.org/licenses/LICENSE-2.0.html). I have made changes to (1) focus on the aspects relevant to this unit of study, (2) have it fit within a single Notebook, and (3) add some exercises.\n",
        "\n",
        "Note: Mac users may experience some issues. We also recommend using a virtual environment that is new (not one you have used for previous workshops)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5d7O_MGw1C2Z"
      },
      "source": [
        "The [ü§ó Transformers library](https://github.com/huggingface/transformers) is one of the most widely used libraries in ML today. It provides nice interfaces to many of the best models you can run yourself.\n",
        "\n",
        "The company that created it has also developed a range of other tools and resources:\n",
        "- [Models](https://huggingface.co/models)\n",
        "- [Datasets](https://huggingface.co/datasets)\n",
        "- [Example Apps](https://huggingface.co/spaces)\n",
        "- [Discussion Forums](https://huggingface.co/posts)\n",
        "- [Documentation and Explainers](https://huggingface.co/docs)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q8yVKdFZ06K2"
      },
      "source": [
        "# Set up\n",
        "\n",
        "Install the Transformers, Datasets, and Evaluate libraries to run this notebook."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "piV0jd0006K2"
      },
      "outputs": [],
      "source": [
        "!pip install datasets evaluate transformers sentencepiece -q"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "coUZ-IL13Hk8"
      },
      "source": [
        "# Example Off-the-Shelf Processing\n",
        "\n",
        "The most basic object in the ü§ó Transformers library is the `pipeline()` function. It connects a model with its necessary preprocessing and postprocessing steps, allowing us to directly input any text and get an  answer."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MoHIs4s9Brkr"
      },
      "source": [
        "\n",
        "## Sentiment Analysis\n",
        "\n",
        "Here is an example of a sentiment analysis pipeline provided by HuggingFace:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W_ztYuMr06K3",
        "outputId": "492f1ae2-51f9-46b1-e46a-7f6fbb56e968"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "No model was supplied, defaulted to distilbert/distilbert-base-uncased-finetuned-sst-2-english and revision 714eb0f (https://huggingface.co/distilbert/distilbert-base-uncased-finetuned-sst-2-english).\n",
            "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
            "Device set to use cpu\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "[{'label': 'NEGATIVE', 'score': 0.9828835129737854},\n",
              " {'label': 'NEGATIVE', 'score': 0.995841920375824}]"
            ]
          },
          "execution_count": 29,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "classifier = pipeline(\"sentiment-analysis\")\n",
        "\n",
        "# Run on one sentence\n",
        "classifier(\"I've been waiting for a HuggingFace lab my whole life.\")\n",
        "\n",
        "# Run on multiple sentences\n",
        "classifier(\n",
        "    [\"I've been waiting for a HuggingFace lab my whole life.\", \"I hate blue cheese so much!\"]\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "liGaEIpj3VXZ"
      },
      "source": [
        "By default, this pipeline selects a particular pretrained model that has been fine-tuned for sentiment analysis in English. The model is downloaded and cached when you create the `classifier` object. If you rerun the command, the cached model will be used instead and there is no need to download the model again.\n",
        "\n",
        "There are three main steps involved when you pass some text to a pipeline:\n",
        "\n",
        "1. The text is preprocessed into a format the model can understand.\n",
        "2. The preprocessed inputs are passed to the model.\n",
        "3. The predictions of the model are post-processed, so you can make sense of them.\n",
        "\n",
        "Some of the [pipelines currently provided by HuggingFace](https://huggingface.co/transformers/main_classes/pipelines) are:\n",
        "\n",
        "- `feature-extraction`\n",
        "- `fill-mask`\n",
        "- `ner`\n",
        "- `question-answering`\n",
        "- `sentiment-analysis`\n",
        "- `summarization`\n",
        "- `text-generation`\n",
        "- `translation`\n",
        "- `zero-shot-classification`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IpkCJNrm3u3x"
      },
      "source": [
        "## Text generation\n",
        "\n",
        "Now let‚Äôs see how to use a pipeline to generate some text. The main idea here is that you provide a prompt and the model will auto-complete it by generating the remaining text. This is similar to systems like ChatGPT, except without being trained to be conversational. This pipeline is set up to do sampling, so the output will vary each time you run it."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FjuJb1zP06K4",
        "outputId": "cb8b04ba-19e6-4fbe-d01a-5b2c56e7d993"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "No model was supplied, defaulted to openai-community/gpt2 and revision 607a30d (https://huggingface.co/openai-community/gpt2).\n",
            "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
            "Device set to use cpu\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "[{'generated_text': 'In this course, we will teach you how to build a database, write an object, and run and test that app in the background. There will be a very simple set of tests for your application - you can download an image here to see them'}]"
            ]
          },
          "execution_count": 30,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "generator = pipeline(\"text-generation\")\n",
        "generator(\"In this course, we will teach you how to\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iHQJDBrw30Ad"
      },
      "source": [
        "You can control how many different sequences are generated with the argument `num_return_sequences` and the total length of the output text with the argument `max_length`.\n",
        "\n",
        "### Task 1\n",
        "\n",
        "Try using these argments to generate two sentences of 15 words each:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R379J3YZ36Pa",
        "outputId": "619ab95d-0f86-4a06-a7ed-0a4d2e182362"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "[{'generated_text': 'In this course, we will teach you how to play with multiple types of'},\n",
              " {'generated_text': 'In this course, we will teach you how to become a master of the'}]"
            ]
          },
          "execution_count": 31,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# TODO\n",
        "# Code should call the generator once.\n",
        "# Output should be two sentences, each with 15 words.\n",
        "\n",
        "\n",
        "generator(\"In this course, we will teach you how to\", num_return_sequences=2, max_length=15)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9k-wstyr4Asp"
      },
      "source": [
        "## Using any model from the Hub in a pipeline\n",
        "\n",
        "The previous examples used the default model for the task at hand, but you can also choose a particular model from HuggingFace's [Model Hub](https://huggingface.co/models). If you go to that page and click on a tag on the left side, you can filter to models for a sepcific task. For example, here is what you get by clicking on ['text generation'](https://huggingface.co/models?pipeline_tag=text-generation).\n",
        "\n",
        "Let‚Äôs try the [distilgpt2](https://huggingface.co/distilgpt2) model. Here‚Äôs how to load it in the same pipeline as before:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jlWR6orP06K5",
        "outputId": "98f57e59-2f17-48c4-f29b-b9320c467559"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Device set to use cpu\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "[{'generated_text': 'In this course, we will teach you how to code in our projects from scratch, all at the same time, while we take your practice into account'},\n",
              " {'generated_text': \"In this course, we will teach you how to implement your code and what we'll teach you about using and using JavaScript. This course offers you as\"}]"
            ]
          },
          "execution_count": 32,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "generator = pipeline(\"text-generation\", model=\"distilgpt2\")\n",
        "generator(\n",
        "    \"In this course, we will teach you how to\",\n",
        "    max_length=30,\n",
        "    num_return_sequences=2,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GGF7f9Xh4R3_"
      },
      "source": [
        "You can refine your search for a model by clicking on the language tags, and pick a model that will generate text in another language.\n",
        "\n",
        "## Mask filling\n",
        "\n",
        "The next pipeline you‚Äôll try is fill-mask. The idea of this task is to replace occurrences of `<mask>` in a given text:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FmjVxVTd06K5",
        "outputId": "3eba6180-adec-486d-8d6f-629a8d0b94e9"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "No model was supplied, defaulted to distilbert/distilroberta-base and revision fb53ab8 (https://huggingface.co/distilbert/distilroberta-base).\n",
            "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
            "Some weights of the model checkpoint at distilbert/distilroberta-base were not used when initializing RobertaForMaskedLM: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "- This IS expected if you are initializing RobertaForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing RobertaForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Device set to use cpu\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "[{'score': 0.26102203130722046,\n",
              "  'token': 30412,\n",
              "  'token_str': ' mathematical',\n",
              "  'sequence': 'This lab will teach you all about mathematical models.'},\n",
              " {'score': 0.0715222880244255,\n",
              "  'token': 38163,\n",
              "  'token_str': ' computational',\n",
              "  'sequence': 'This lab will teach you all about computational models.'}]"
            ]
          },
          "execution_count": 33,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "unmasker = pipeline(\"fill-mask\")\n",
        "unmasker(\"This lab will teach you all about <mask> models.\", top_k=2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8mca04PP4p36"
      },
      "source": [
        "The `top_k` argument controls how many possibilities you want to be displayed. Note the choice of `<mask>` is specific to the model used by this pipeline. If you change the model you should check what token it looks for to replace. This is shown on the HuggingFace page.\n",
        "\n",
        "## Named Entity Recognition\n",
        "\n",
        "Let's try HuggingFace's NER pipeline - a task we've done before with spaCy:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PuVhGtIG06K5",
        "outputId": "b3294861-e8ea-4b01-ef7b-3ceb5d3efe0a"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "No model was supplied, defaulted to dbmdz/bert-large-cased-finetuned-conll03-english and revision 4c53496 (https://huggingface.co/dbmdz/bert-large-cased-finetuned-conll03-english).\n",
            "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
            "Some weights of the model checkpoint at dbmdz/bert-large-cased-finetuned-conll03-english were not used when initializing BertForTokenClassification: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
            "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Device set to use cpu\n",
            "/usr/local/lib/python3.11/dist-packages/transformers/pipelines/token_classification.py:170: UserWarning: `grouped_entities` is deprecated and will be removed in version v5.0.0, defaulted to `aggregation_strategy=\"AggregationStrategy.SIMPLE\"` instead.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "[{'entity_group': 'PER',\n",
              "  'score': np.float32(0.9961088),\n",
              "  'word': 'Jonathan K. Kummerfeld',\n",
              "  'start': 11,\n",
              "  'end': 33},\n",
              " {'entity_group': 'MISC',\n",
              "  'score': np.float32(0.42561412),\n",
              "  'word': 'NL',\n",
              "  'start': 46,\n",
              "  'end': 48},\n",
              " {'entity_group': 'ORG',\n",
              "  'score': np.float32(0.97828877),\n",
              "  'word': 'The University of Sydney',\n",
              "  'start': 53,\n",
              "  'end': 77},\n",
              " {'entity_group': 'LOC',\n",
              "  'score': np.float32(0.9995468),\n",
              "  'word': 'Australia',\n",
              "  'start': 81,\n",
              "  'end': 90}]"
            ]
          },
          "execution_count": 34,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "ner = pipeline(\"ner\", grouped_entities=True)\n",
        "# Ë°®Á§∫ÂØπËØÜÂà´Âá∫ÁöÑÂÆû‰ΩìËøõË°åÂêàÂπ∂ÔºåÊääÂ±û‰∫éÂêå‰∏Ä‰∏™ÂÆû‰ΩìÁöÑÂ§ö‰∏™ËØçÂêàÊàê‰∏Ä‰∏™ÂÆåÊï¥ÁöÑÂÆû‰Ωì„ÄÇ‰æãÂ¶ÇÔºå‚ÄúNew‚Äù Âíå ‚ÄúYork‚Äù ‰ºöÂêàÂπ∂Êàê ‚ÄúNew York‚Äù\n",
        "ner(\"My name is Jonathan K. Kummerfeld and I teach NLP at The University of Sydney in Australia.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yBkgkvxO5D8-"
      },
      "source": [
        "We pass the option `grouped_entities=True` in the pipeline creation function to tell the pipeline to regroup together the parts of the sentence that correspond to the same entity.\n",
        "\n",
        "Try running an NER pipeline without grouped entities, and see if you can find an input that is processed differently by it:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M2qus0YBExRF",
        "outputId": "1d30545d-ab7e-40e9-e45b-35ce8ea3a47e"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "No model was supplied, defaulted to dbmdz/bert-large-cased-finetuned-conll03-english and revision 4c53496 (https://huggingface.co/dbmdz/bert-large-cased-finetuned-conll03-english).\n",
            "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
            "Some weights of the model checkpoint at dbmdz/bert-large-cased-finetuned-conll03-english were not used when initializing BertForTokenClassification: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
            "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Device set to use cpu\n",
            "/usr/local/lib/python3.11/dist-packages/transformers/pipelines/token_classification.py:170: UserWarning: `grouped_entities` is deprecated and will be removed in version v5.0.0, defaulted to `aggregation_strategy=\"AggregationStrategy.NONE\"` instead.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "[{'entity': 'I-PER',\n",
              "  'score': np.float32(0.9995222),\n",
              "  'index': 4,\n",
              "  'word': 'Jonathan',\n",
              "  'start': 11,\n",
              "  'end': 19},\n",
              " {'entity': 'I-PER',\n",
              "  'score': np.float32(0.9996123),\n",
              "  'index': 5,\n",
              "  'word': 'K',\n",
              "  'start': 20,\n",
              "  'end': 21},\n",
              " {'entity': 'I-PER',\n",
              "  'score': np.float32(0.99331266),\n",
              "  'index': 6,\n",
              "  'word': '.',\n",
              "  'start': 21,\n",
              "  'end': 22},\n",
              " {'entity': 'I-PER',\n",
              "  'score': np.float32(0.9996099),\n",
              "  'index': 7,\n",
              "  'word': 'Ku',\n",
              "  'start': 23,\n",
              "  'end': 25},\n",
              " {'entity': 'I-PER',\n",
              "  'score': np.float32(0.99298507),\n",
              "  'index': 8,\n",
              "  'word': '##mmer',\n",
              "  'start': 25,\n",
              "  'end': 29},\n",
              " {'entity': 'I-PER',\n",
              "  'score': np.float32(0.99161047),\n",
              "  'index': 9,\n",
              "  'word': '##feld',\n",
              "  'start': 29,\n",
              "  'end': 33},\n",
              " {'entity': 'I-MISC',\n",
              "  'score': np.float32(0.42561412),\n",
              "  'index': 13,\n",
              "  'word': 'NL',\n",
              "  'start': 46,\n",
              "  'end': 48},\n",
              " {'entity': 'I-ORG',\n",
              "  'score': np.float32(0.929053),\n",
              "  'index': 16,\n",
              "  'word': 'The',\n",
              "  'start': 53,\n",
              "  'end': 56},\n",
              " {'entity': 'I-ORG',\n",
              "  'score': np.float32(0.9956611),\n",
              "  'index': 17,\n",
              "  'word': 'University',\n",
              "  'start': 57,\n",
              "  'end': 67},\n",
              " {'entity': 'I-ORG',\n",
              "  'score': np.float32(0.9946185),\n",
              "  'index': 18,\n",
              "  'word': 'of',\n",
              "  'start': 68,\n",
              "  'end': 70},\n",
              " {'entity': 'I-ORG',\n",
              "  'score': np.float32(0.99382246),\n",
              "  'index': 19,\n",
              "  'word': 'Sydney',\n",
              "  'start': 71,\n",
              "  'end': 77},\n",
              " {'entity': 'I-LOC',\n",
              "  'score': np.float32(0.9995468),\n",
              "  'index': 21,\n",
              "  'word': 'Australia',\n",
              "  'start': 81,\n",
              "  'end': 90}]"
            ]
          },
          "execution_count": 35,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# TODO\n",
        "ner = pipeline(\"ner\", grouped_entities=False)\n",
        "ner(\"My name is Jonathan K. Kummerfeld and I teach NLP at The University of Sydney in Australia.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dxdX4K58ExYs"
      },
      "source": [
        "## Question answering\n",
        "\n",
        "The question-answering pipeline answers questions using information from a given context:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c8a4-M0d06K6",
        "outputId": "f0a17a87-534c-465b-ce13-08950708c5ca"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "No model was supplied, defaulted to distilbert/distilbert-base-cased-distilled-squad and revision 564e9b5 (https://huggingface.co/distilbert/distilbert-base-cased-distilled-squad).\n",
            "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
            "Device set to use cpu\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "{'score': 0.2765628397464752,\n",
              " 'start': 53,\n",
              " 'end': 77,\n",
              " 'answer': 'The University of Sydney'}"
            ]
          },
          "execution_count": 36,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "question_answerer = pipeline(\"question-answering\")\n",
        "question_answerer(\n",
        "    question=\"Where do I work?\",\n",
        "    context=\"My name is Jonathan K. Kummerfeld and I teach NLP at The University of Sydney in Australia.\",\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QdBhDvbT5Td7"
      },
      "source": [
        "\n",
        "Note that this pipeline works by ***extracting*** information from the provided context; it does not generate the answer.\n",
        "\n",
        "## Summarization\n",
        "\n",
        "Summarization is the task of reducing a text into a shorter text while keeping all (or most) of the important aspects referenced in the text. Here‚Äôs an example:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HmDVogfs06K6",
        "outputId": "5a616079-cfa2-4625-df0a-054545739960"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "No model was supplied, defaulted to sshleifer/distilbart-cnn-12-6 and revision a4f8f3e (https://huggingface.co/sshleifer/distilbart-cnn-12-6).\n",
            "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
            "Device set to use cpu\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "[{'summary_text': ' The number of engineering graduates in the United States has declined in recent years . China and India graduate six and eight times as many traditional engineers as the U.S. does . Rapidly developing economies such as China continue to encourage and advance the teaching of engineering . There are declining offerings in engineering subjects dealing with infrastructure, infrastructure, the environment, and related issues .'}]"
            ]
          },
          "execution_count": 37,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "summarizer = pipeline(\"summarization\")\n",
        "summarizer(\n",
        "    \"\"\"\n",
        "    America has changed dramatically during recent years. Not only has the number of\n",
        "    graduates in traditional engineering disciplines such as mechanical, civil,\n",
        "    electrical, chemical, and aeronautical engineering declined, but in most of\n",
        "    the premier American universities engineering curricula now concentrate on\n",
        "    and encourage largely the study of engineering science. As a result, there\n",
        "    are declining offerings in engineering subjects dealing with infrastructure,\n",
        "    the environment, and related issues, and greater concentration on high\n",
        "    technology subjects, largely supporting increasingly complex scientific\n",
        "    developments. While the latter is important, it should not be at the expense\n",
        "    of more traditional engineering.\n",
        "\n",
        "    Rapidly developing economies such as China and India, as well as other\n",
        "    industrial countries in Europe and Asia, continue to encourage and advance\n",
        "    the teaching of engineering. Both China and India, respectively, graduate\n",
        "    six and eight times as many traditional engineers as does the United States.\n",
        "    Other industrial countries at minimum maintain their output, while America\n",
        "    suffers an increasingly serious decline in the number of engineering graduates\n",
        "    and a lack of well-educated engineers.\n",
        "\"\"\"\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PlSJVp8T5XEZ"
      },
      "source": [
        "## Translation\n",
        "\n",
        "For translation, you can use a default model if you provide a language pair in the task name (such as \"`translation_en_to_fr`\"), but the easiest way is to pick the model you want to use on the [Model Hub](https://huggingface.co/models). Here we‚Äôll try translating from French to English:\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "66pGUVMn06K6",
        "outputId": "a638b2d0-c4e7-489c-a290-704a06a1765f"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/transformers/models/marian/tokenization_marian.py:175: UserWarning: Recommended: pip install sacremoses.\n",
            "  warnings.warn(\"Recommended: pip install sacremoses.\")\n",
            "Device set to use cpu\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "[{'translation_text': 'Chocolate is always as delicious!'}]"
            ]
          },
          "execution_count": 38,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "translator = pipeline(\"translation\", model=\"Helsinki-NLP/opus-mt-fr-en\")\n",
        "translator(\"Le chocolat est toujours aussi d√©licieux!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K5pCgNrQ5p8r"
      },
      "source": [
        "Like with text generation and summarization, you can specify a max_length or a min_length for the result.\n",
        "\n",
        "The pipelines shown so far are mostly for demonstrative purposes. They were programmed for specific tasks and cannot perform variations of them.\n",
        "\n",
        "# Bias and Limitations\n",
        "\n",
        "If your intent is to use a pretrained model or a fine-tuned version in production, be aware that, while these models are powerful tools, they come with limitations. The biggest of these is that, to enable pretraining on large amounts of data, researchers often scrape all the content they can find, taking the best as well as the worst of what is available on the internet.\n",
        "\n",
        "To give a quick illustration, let‚Äôs go back the example of a `fill-mask` pipeline with the BERT model:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2sVMn8cxgYJn",
        "outputId": "42038a5c-17e9-44c8-9601-da9ebb5bec37"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
            "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Device set to use cpu\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['carpenter', 'lawyer', 'farmer', 'businessman']\n",
            "['nurse', 'maid', 'teacher', 'waitress']\n"
          ]
        }
      ],
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "unmasker = pipeline(\"fill-mask\", model=\"bert-base-uncased\")\n",
        "result = unmasker(\"This man works as a [MASK].\")\n",
        "print([r[\"token_str\"] for r in result[:4]])\n",
        "\n",
        "result = unmasker(\"This woman works as a [MASK].\")\n",
        "print([r[\"token_str\"] for r in result[:4]])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TFVAdvhdg5_K"
      },
      "source": [
        "When asked to fill in the missing word in these two sentences, the model gives only one or two gender-specific answers (e.g., 'businessman' and 'waitress').\n",
        "\n",
        "Most of the answers are occuptations that anyone could do. The difference between the two lists reflects the bias in the model. When you use these tools, you therefore need to keep in the back of your mind that the original model you are using could very easily generate sexist, racist, or homophobic content. Fine-tuning the model on your data won‚Äôt make this intrinsic bias disappear."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F15B7uHv0z7s"
      },
      "source": [
        "#  Preprocessing\n",
        "\n",
        "In this section, we'll start digging into the details of peipelines by looking at how tokenisation and data processing works.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7KhnF1xnGOcr"
      },
      "source": [
        "## Tokenisation\n",
        "\n",
        "Like other neural networks, Transformer models can‚Äôt process raw text directly, so the first step of our pipeline is to convert the text inputs into numbers that the model can make sense of. To do this we use a tokenizer, which will be responsible for:\n",
        "\n",
        "Splitting the input into words, subwords, or symbols (like punctuation) that are called tokens\n",
        "Mapping each token to an integer\n",
        "Adding additional inputs that may be useful to the model\n",
        "All this preprocessing needs to be done in exactly the same way as when the model was pretrained, so we first need to download that information from the Model Hub. To do this, we use the AutoTokenizer class and its from_pretrained() method. Using the checkpoint name of our model, it will automatically fetch the data associated with the model‚Äôs tokenizer and cache it (so it‚Äôs only downloaded the first time you run the code below).\n",
        "\n",
        "Since the default checkpoint of the sentiment-analysis pipeline is distilbert-base-uncased-finetuned-sst-2-english (you can see its model card here), we run the following:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "id": "xXapNOLF012F"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer\n",
        "\n",
        "checkpoint = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(checkpoint)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U2n9OVj003Zy"
      },
      "source": [
        "Once we have the tokenizer, we can directly pass our sentences to it and we‚Äôll get back a dictionary that‚Äôs ready to feed to our model! The only thing left to do is to convert the list of input IDs to tensors.\n",
        "\n",
        "You can use ü§ó Transformers without having to worry about which ML framework is used as a backend; it might be PyTorch or TensorFlow, or Flax for some models. However, Transformer models only accept tensors as input. If this is your first time hearing about tensors, you can think of them as NumPy arrays instead. A NumPy array can be a scalar (0D), a vector (1D), a matrix (2D), or have more dimensions. It‚Äôs effectively a tensor; other ML frameworks‚Äô tensors behave similarly, and are usually as simple to instantiate as NumPy arrays.\n",
        "\n",
        "To specify the type of tensors we want to get back (PyTorch, TensorFlow, or plain NumPy), we use the return_tensors argument:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qa6vrO8o063r",
        "outputId": "4bdac128-1ec5-445c-d512-b67fe55f3672"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'input_ids': tensor([[  101,  1045,  1005,  2310,  2042,  3403,  2005,  1037, 17662, 12172,\n",
            "          2607,  2026,  2878,  2166,  1012,   102],\n",
            "        [  101,  1045,  5223,  2023,  2061,  2172,   999,   102,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
            "        [1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0]])}\n"
          ]
        }
      ],
      "source": [
        "raw_inputs = [\n",
        "    \"I've been waiting for a HuggingFace course my whole life.\",\n",
        "    \"I hate this so much!\",\n",
        "]\n",
        "inputs = tokenizer(raw_inputs, padding=True, truncation=True, return_tensors=\"pt\")\n",
        "print(inputs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-qyVMUz308yf"
      },
      "source": [
        "Don‚Äôt worry about padding and truncation just yet; we‚Äôll explain those below. The main things to remember here are that you can pass one sentence or a list of sentences, as well as specifying the type of tensors you want to get back (if no type is passed, you will get a list of lists as a result).\n",
        "\n",
        "Loading and saving tokenizers is based on two methods: from_pretrained() and save_pretrained(). These methods will load or save the algorithm used by the tokenizer (a bit like the architecture of a model) as well as its vocabulary (a bit like the weights of a model).\n",
        "\n",
        "Loading the BERT tokenizer trained with the same checkpoint as BERT is done the same way as loading the model, except we use the BertTokenizer class:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nyTqOGAU08Xp",
        "outputId": "57b30904-498c-420d-bbd5-9e45cd2377ce"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['Using', 'a', 'Trans', '##former', 'network', 'is', 'simple']\n",
            "[7993, 170, 13809, 23763, 2443, 1110, 3014]\n",
            "Using a transformer network is simple\n"
          ]
        }
      ],
      "source": [
        "from transformers import BertTokenizer\n",
        "\n",
        "tokenizer = BertTokenizer.from_pretrained(\"bert-base-cased\")\n",
        "\n",
        "sequence = \"Using a Transformer network is simple\"\n",
        "tokens = tokenizer.tokenize(sequence)\n",
        "\n",
        "print(tokens)\n",
        "\n",
        "\n",
        "ids = tokenizer.convert_tokens_to_ids(tokens)\n",
        "\n",
        "print(ids)\n",
        "\n",
        "decoded_string = tokenizer.decode([7993, 170, 11303, 1200, 2443, 1110, 3014])\n",
        "print(decoded_string)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6F_qgJRq1a9-"
      },
      "source": [
        "The AutoTokenizer class will grab the proper tokenizer class in the library based on the checkpoint name, and can be used directly with any checkpoint:\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GhEMEnfP1bDf",
        "outputId": "324f1db5-7890-4ca4-aad2-d222300775b1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['Using', 'a', 'Trans', '##former', 'network', 'is', 'simple']\n"
          ]
        }
      ],
      "source": [
        "from transformers import AutoTokenizer\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")\n",
        "\n",
        "sequence = \"Using a Transformer network is simple\"\n",
        "tokens = tokenizer.tokenize(sequence)\n",
        "\n",
        "print(tokens)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BwPh2rgh1h24"
      },
      "source": [
        "Saving a tokenizer is straightforward:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I_nD0I4j1h7T",
        "outputId": "cc7f466f-41e9-40d0-c4c6-83a2f34bae82"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "('directory_on_my_computer/tokenizer_config.json',\n",
              " 'directory_on_my_computer/special_tokens_map.json',\n",
              " 'directory_on_my_computer/vocab.txt',\n",
              " 'directory_on_my_computer/added_tokens.json',\n",
              " 'directory_on_my_computer/tokenizer.json')"
            ]
          },
          "execution_count": 44,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "tokenizer.save_pretrained(\"directory_on_my_computer\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sCaUIiLd17SG"
      },
      "source": [
        "Above, you saw how sequences get translated into lists of numbers. Let‚Äôs convert this list of numbers to a tensor and send it to the model:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "64ys2Pkr17YU",
        "outputId": "5df6a1dc-5a38-404b-91db-f18af759a234"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Input IDs: tensor([[ 1045,  1005,  2310,  2042,  3403,  2005,  1037, 17662, 12172,  2607,\n",
            "          2026,  2878,  2166,  1012]])\n",
            "Logits: tensor([[-2.7276,  2.8789]], grad_fn=<AddmmBackward0>)\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
        "\n",
        "checkpoint = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
        "model = AutoModelForSequenceClassification.from_pretrained(checkpoint)\n",
        "\n",
        "sequence = \"I've been waiting for a HuggingFace course my whole life.\"\n",
        "\n",
        "tokens = tokenizer.tokenize(sequence)\n",
        "ids = tokenizer.convert_tokens_to_ids(tokens)\n",
        "\n",
        "input_ids = torch.tensor([ids])\n",
        "print(\"Input IDs:\", input_ids)\n",
        "\n",
        "output = model(input_ids)\n",
        "print(\"Logits:\", output.logits)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iPOGa0yl2T72"
      },
      "source": [
        "Padding the inputs\n",
        "The following list of lists cannot be converted to a tensor:\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xGv-BSPG2Wym",
        "outputId": "910cde76-b3d6-4eff-ef77-2d44b5aaa2fd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([[ 1.5694, -1.3895]], grad_fn=<AddmmBackward0>)\n",
            "tensor([[ 0.5803, -0.4125]], grad_fn=<AddmmBackward0>)\n",
            "tensor([[ 1.5694, -1.3895],\n",
            "        [ 1.3374, -1.2163]], grad_fn=<AddmmBackward0>)\n"
          ]
        }
      ],
      "source": [
        "model = AutoModelForSequenceClassification.from_pretrained(checkpoint)\n",
        "\n",
        "sequence1_ids = [[200, 200, 200]]\n",
        "sequence2_ids = [[200, 200]]\n",
        "batched_ids = [\n",
        "    [200, 200, 200],\n",
        "    [200, 200, tokenizer.pad_token_id],\n",
        "]\n",
        "\n",
        "print(model(torch.tensor(sequence1_ids)).logits)\n",
        "print(model(torch.tensor(sequence2_ids)).logits)\n",
        "print(model(torch.tensor(batched_ids)).logits)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5u-JzdZF2iAM"
      },
      "source": [
        "There‚Äôs something wrong with the logits in our batched predictions: the second row should be the same as the logits for the second sentence, but we‚Äôve got completely different values!\n",
        "\n",
        "This is because the key feature of Transformer models is attention layers that contextualize each token. These will take into account the padding tokens since they attend to all of the tokens of a sequence. To get the same result when passing individual sentences of different lengths through the model or when passing a batch with the same sentences and padding applied, we need to tell those attention layers to ignore the padding tokens. This is done by using an attention mask.\n",
        "\n",
        "Attention masks\n",
        "Attention masks are tensors with the exact same shape as the input IDs tensor, filled with 0s and 1s: 1s indicate the corresponding tokens should be attended to, and 0s indicate the corresponding tokens should not be attended to (i.e., they should be ignored by the attention layers of the model).\n",
        "\n",
        "Let‚Äôs complete the previous example with an attention mask:\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_RsJJa0e2iGl",
        "outputId": "78e0def2-8f3f-489e-b0aa-b276ea2849f9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([[ 1.5694, -1.3895],\n",
            "        [ 0.5803, -0.4125]], grad_fn=<AddmmBackward0>)\n"
          ]
        }
      ],
      "source": [
        "batched_ids = [\n",
        "    [200, 200, 200],\n",
        "    [200, 200, tokenizer.pad_token_id],\n",
        "]\n",
        "\n",
        "attention_mask = [\n",
        "    [1, 1, 1],\n",
        "    [1, 1, 0],\n",
        "]\n",
        "\n",
        "outputs = model(torch.tensor(batched_ids), attention_mask=torch.tensor(attention_mask))\n",
        "print(outputs.logits)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P0pHYG9C2z7T"
      },
      "source": [
        "The code below shows examples of a variety of ways to configure these inputs:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-iKR1GGt20AI"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer\n",
        "\n",
        "checkpoint = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
        "sequences = [\"I've been waiting for an NLP course my whole life.\", \"So have I!\"]\n",
        "\n",
        "model_inputs = tokenizer(sequences)\n",
        "\n",
        "# Will pad the sequences up to the maximum sequence length\n",
        "model_inputs_longest = tokenizer(sequences, padding=\"longest\")\n",
        "\n",
        "# Will pad the sequences up to the model max length\n",
        "# (512 for BERT or DistilBERT)\n",
        "model_inputs_model_max = tokenizer(sequences, padding=\"max_length\")\n",
        "\n",
        "# Will pad the sequences up to the specified max length\n",
        "model_inputs_custom_max = tokenizer(sequences, padding=\"max_length\", max_length=8)\n",
        "\n",
        "# Will truncate to the max available length\n",
        "model_inputs_custom_max = tokenizer(sequences, truncation=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WecxV0h83T1k"
      },
      "source": [
        "If we take a look at the input IDs returned by the tokenizer, we will see they are a tiny bit different from what we had earlier:\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O5UcVP-U3T6n",
        "outputId": "a746a80c-f675-4e07-b3fa-b125ea61c481"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[101, 1045, 1005, 2310, 2042, 3403, 2005, 2019, 17953, 2361, 2607, 2026, 2878, 2166, 1012, 102]\n",
            "[1045, 1005, 2310, 2042, 3403, 2005, 2019, 17953, 2361, 2607, 2026, 2878, 2166, 1012]\n",
            "[CLS] i've been waiting for an nlp course my whole life. [SEP]\n"
          ]
        }
      ],
      "source": [
        "sequence = \"I've been waiting for an NLP course my whole life.\"\n",
        "\n",
        "model_inputs = tokenizer(sequence)\n",
        "print(model_inputs[\"input_ids\"])\n",
        "\n",
        "tokens = tokenizer.tokenize(sequence)\n",
        "ids = tokenizer.convert_tokens_to_ids(tokens)\n",
        "print(ids)\n",
        "\n",
        "print(tokenizer.decode(model_inputs[\"input_ids\"]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1YAl_WWp3dPu"
      },
      "source": [
        "The tokenizer added the special word [CLS] at the beginning and the special word [SEP] at the end. This is because the model was pretrained with those, so to get the same results for inference we need to add them as well. Note that some models don‚Äôt add special words, or add different ones; models may also add these special words only at the beginning, or only at the end. In any case, the tokenizer knows which ones are expected and will deal with this for you.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w3wwgKMDiSdj"
      },
      "source": [
        "### Task 2\n",
        "\n",
        "Set up a BERT tokeniser that will read in the following sentences, pad the short ones to length 5 and truncate the long ones.\n",
        "\n",
        "Output should be the output of the tokenizer, showing input_ids."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CcULu9PaiQU7",
        "outputId": "d3a4ff5d-c687-4a4a-f837-1fd3861b2957"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[[101, 7592, 999, 102, 0], [101, 7592, 2088, 102, 0], [101, 7592, 2088, 999, 102], [101, 2023, 2003, 2019, 102], [101, 2023, 2003, 2019, 102], [101, 2023, 2003, 1037, 102]]\n"
          ]
        }
      ],
      "source": [
        "sentences = [\n",
        "    \"Hello!\",\n",
        "    \"Hello World\",\n",
        "    \"Hello World!\",\n",
        "    \"This is an example\",\n",
        "    \"This is an example sentence\",\n",
        "    \"This is a very long example sentence that should be truncated!\"\n",
        "]\n",
        "\n",
        "# TODO\n",
        "tokens = tokenizer(sentences, padding=\"max_length\", max_length=5, truncation=True)\n",
        "print(tokens[\"input_ids\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gGIPmJaf3rQL"
      },
      "source": [
        "# Data\n",
        "\n",
        "As you can see, we get a DatasetDict object which contains the training set, the validation set, and the test set. Each of those contains several columns (sentence1, sentence2, label, and idx) and a variable number of rows, which are the number of elements in each set (so, there are 3,668 pairs of sentences in the training set, 408 in the validation set, and 1,725 in the test set)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6CEpnGa53rXZ",
        "outputId": "d51bafd4-ed8c-4205-9b2b-5113516217af"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "DatasetDict({\n",
            "    train: Dataset({\n",
            "        features: ['sentence1', 'sentence2', 'label', 'idx'],\n",
            "        num_rows: 3668\n",
            "    })\n",
            "    validation: Dataset({\n",
            "        features: ['sentence1', 'sentence2', 'label', 'idx'],\n",
            "        num_rows: 408\n",
            "    })\n",
            "    test: Dataset({\n",
            "        features: ['sentence1', 'sentence2', 'label', 'idx'],\n",
            "        num_rows: 1725\n",
            "    })\n",
            "})\n",
            "{'sentence1': 'Amrozi accused his brother , whom he called \" the witness \" , of deliberately distorting his evidence .', 'sentence2': 'Referring to him as only \" the witness \" , Amrozi accused his brother of deliberately distorting his evidence .', 'label': 1, 'idx': 0}\n",
            "{'sentence1': Value(dtype='string', id=None), 'sentence2': Value(dtype='string', id=None), 'label': ClassLabel(names=['not_equivalent', 'equivalent'], id=None), 'idx': Value(dtype='int32', id=None)}\n"
          ]
        }
      ],
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "raw_datasets = load_dataset(\"glue\", \"mrpc\")\n",
        "\n",
        "print(raw_datasets)\n",
        "\n",
        "raw_train_dataset = raw_datasets[\"train\"]\n",
        "print(raw_train_dataset[0])\n",
        "\n",
        "# To know which integer corresponds to which label, we can inspect the features of our raw_train_dataset. This will tell us the type of each column:\n",
        "print(raw_train_dataset.features)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ezDe7cMkjRes"
      },
      "source": [
        "We can run our tokeniser on the entire dataset:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {
        "id": "_bCAQaaWjUEM"
      },
      "outputs": [],
      "source": [
        "tokenized_dataset = tokenizer(\n",
        "    raw_datasets[\"train\"][\"sentence1\"],\n",
        "    raw_datasets[\"train\"][\"sentence2\"],\n",
        "    padding=True,\n",
        "    truncation=True,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ERpNaF9Sjz1B"
      },
      "source": [
        "This works well, but it has the disadvantage of returning a dictionary (with our keys, input_ids, attention_mask, and token_type_ids, and values that are lists of lists). It will also only work if you have enough RAM to store your whole dataset during the tokenization (whereas the datasets from the ü§ó Datasets library are Apache Arrow files stored on the disk, so you only keep the samples you ask for loaded in memory).\n",
        "\n",
        "To keep the data as a dataset, we will use the Dataset.map() method. This also allows us some extra flexibility, if we need more preprocessing done than just tokenization. The map() method works by applying a function on each element of the dataset, so let‚Äôs define a function that tokenizes our inputs:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 292,
          "referenced_widgets": [
            "6d66e5e8cc624476ad3962f8a29fdc89",
            "b7bec74eb0494c0ba701b187ae33f601",
            "7676a6393af34adfb10137f827be2eb7",
            "f615aa98e8ef45ac8de1efa273d7d76b",
            "f9334a6ef48c4e65b334b4af1fa5f3bc",
            "7808b14f02884a8cba83301a7931d846",
            "e1d14e25d2cf4d93a3f6bf707ce86cf0",
            "a3e8aeff6dc94fdb9f674564c63dc8ac",
            "c43ec55a03a94265a4ea273330cbd814",
            "6e79ed3c00874fd0a6863702bbb86f8e",
            "97070d0d3bef426dbeaa3323b72704f5"
          ]
        },
        "id": "mP3TgeXQj3oN",
        "outputId": "f3a6aa3d-e4be-4229-d169-05d894bb7b25"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "6d66e5e8cc624476ad3962f8a29fdc89",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Map:   0%|          | 0/408 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": [
              "DatasetDict({\n",
              "    train: Dataset({\n",
              "        features: ['sentence1', 'sentence2', 'label', 'idx', 'input_ids', 'attention_mask'],\n",
              "        num_rows: 3668\n",
              "    })\n",
              "    validation: Dataset({\n",
              "        features: ['sentence1', 'sentence2', 'label', 'idx', 'input_ids', 'attention_mask'],\n",
              "        num_rows: 408\n",
              "    })\n",
              "    test: Dataset({\n",
              "        features: ['sentence1', 'sentence2', 'label', 'idx', 'input_ids', 'attention_mask'],\n",
              "        num_rows: 1725\n",
              "    })\n",
              "})"
            ]
          },
          "execution_count": 54,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "def tokenize_function(example):\n",
        "    return tokenizer(example[\"sentence1\"], example[\"sentence2\"], truncation=True)\n",
        "\n",
        "tokenized_datasets = raw_datasets.map(tokenize_function, batched=True)\n",
        "tokenized_datasets"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sKDGViBKj9Xi"
      },
      "source": [
        "To handle padding with a dataset being mapped in this way, we can use a Data Collator:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PeiLAHRVkCzq",
        "outputId": "3961b039-34ba-401c-e66b-ac8a57ba9dfb"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'input_ids': torch.Size([8, 67]),\n",
              " 'attention_mask': torch.Size([8, 67]),\n",
              " 'labels': torch.Size([8])}"
            ]
          },
          "execution_count": 55,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from transformers import DataCollatorWithPadding\n",
        "\n",
        "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
        "\n",
        "samples = tokenized_datasets[\"train\"][:8]\n",
        "samples = {k: v for k, v in samples.items() if k not in [\"idx\", \"sentence1\", \"sentence2\"]}\n",
        "\n",
        "batch = data_collator(samples)\n",
        "{k: v.shape for k, v in batch.items()}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "He2RXr4BkK76"
      },
      "source": [
        "That's it for now! In future weeks we will learn about fine-tuning and other capabilities of the library."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.4"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "6d66e5e8cc624476ad3962f8a29fdc89": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_b7bec74eb0494c0ba701b187ae33f601",
              "IPY_MODEL_7676a6393af34adfb10137f827be2eb7",
              "IPY_MODEL_f615aa98e8ef45ac8de1efa273d7d76b"
            ],
            "layout": "IPY_MODEL_f9334a6ef48c4e65b334b4af1fa5f3bc"
          }
        },
        "6e79ed3c00874fd0a6863702bbb86f8e": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7676a6393af34adfb10137f827be2eb7": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a3e8aeff6dc94fdb9f674564c63dc8ac",
            "max": 408,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_c43ec55a03a94265a4ea273330cbd814",
            "value": 408
          }
        },
        "7808b14f02884a8cba83301a7931d846": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "97070d0d3bef426dbeaa3323b72704f5": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "a3e8aeff6dc94fdb9f674564c63dc8ac": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b7bec74eb0494c0ba701b187ae33f601": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7808b14f02884a8cba83301a7931d846",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_e1d14e25d2cf4d93a3f6bf707ce86cf0",
            "value": "Map:‚Äá100%"
          }
        },
        "c43ec55a03a94265a4ea273330cbd814": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "e1d14e25d2cf4d93a3f6bf707ce86cf0": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "f615aa98e8ef45ac8de1efa273d7d76b": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6e79ed3c00874fd0a6863702bbb86f8e",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_97070d0d3bef426dbeaa3323b72704f5",
            "value": "‚Äá408/408‚Äá[00:00&lt;00:00,‚Äá2809.28‚Äáexamples/s]"
          }
        },
        "f9334a6ef48c4e65b334b4af1fa5f3bc": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
